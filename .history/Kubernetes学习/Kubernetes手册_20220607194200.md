#  1.  Kubernetes 入门

##  1.  Kubernetes 生产环境

###  1.  Kubernetes 容器运行时
容器运行时
-----

你需要在集群内每个节点上安装一个 容器运行时 以使 Pod 可以运行在上面。本文概述了所涉及的内容并描述了与节点设置相关的任务。

Kubernetes 1.24 要求你使用符合容器运行时接口 (CRI)的运行时。

有关详细信息，请参阅该文的 CRI 版本支持。 本页简要介绍在 Kubernetes 中几个常见的容器运行时的用法。

*   ​`containerd` ​
*   ​`CRI-O` ​
*   ​`Docker Engine` ​
*   ​`Mirantis Container Runtime`​

> Note:  
> 提示：v1.24 之前的 Kubernetes 版本包括与 Docker Engine 的直接集成，使用名为 dockershim 的组件。 这种特殊的直接整合不再是 Kubernetes 的一部分 （这次删除被作为 v1.20 发行版本的一部分宣布）。   
> 如果你正在运行 v1.24 以外的 Kubernetes 版本，检查该版本的文档。

安装和配置先决条件
---------

以下步骤将通用设置应用于 Linux 上的 Kubernetes 节点。

如果你确定不需要某个特定设置，则可以跳过它。

#### 转发 IPv4 并让 iptables 看到桥接流量

通过运行 ​`lsmod | grep br_netfilter`​ 来验证 ​`br_netfilter`​ 模块是否已加载。

若要显式加载此模块，请运行 ​`sudo modprobe br_netfilter`​。

为了让 Linux 节点的 iptables 能够正确查看桥接流量，请确认 ​`sysctl` ​配置中的 ​`net.bridge.bridge-nf-call-iptables`​ 设置为 1。 例如：

`cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF  sudo modprobe overlay sudo modprobe br_netfilter  # 设置所需的 sysctl 参数，参数在重新启动后保持不变 cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables  = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward                 = 1 EOF  # 应用 sysctl 参数而不重新启动 sudo sysctl --system`

Cgroup 驱动程序 
------------

在 Linux 上，控制组（CGroup）用于限制分配给进程的资源。

当某个 Linux 系统发行版使用 [systemd](https://www.freedesktop.org/wiki/Software/systemd/) 作为其初始化系统时，初始化进程会生成并使用一个 root 控制组（​`cgroup`​），并充当 cgroup 管理器。 Systemd 与 cgroup 集成紧密，并将为每个 systemd 单元分配一个 cgroup。 你也可以配置容器运行时和 kubelet 使用 ​`cgroupfs`​。 连同 systemd 一起使用 ​`cgroupfs` ​意味着将有两个不同的 cgroup 管理器。

单个 cgroup 管理器将简化分配资源的视图，并且默认情况下将对可用资源和使用 中的资源具有更一致的视图。 当有两个管理器共存于一个系统中时，最终将对这些资源产生两种视图。 在此领域人们已经报告过一些案例，某些节点配置让 kubelet 和 docker 使用 ​`cgroupfs`​，而节点上运行的其余进程则使用 systemd; 这类节点在资源压力下 会变得不稳定。

更改设置，令容器运行时和 kubelet 使用 ​`systemd` ​作为 cgroup 驱动，以此使系统更为稳定。 对于 Docker, 设置 ​`native.cgroupdriver=systemd`​ 选项。

注意：更改已加入集群的节点的 cgroup 驱动是一项敏感的操作。 如果 kubelet 已经使用某 cgroup 驱动的语义创建了 pod，更改运行时以使用 别的 cgroup 驱动，当为现有 Pods 重新创建 PodSandbox 时会产生错误。 重启 kubelet 也可能无法解决此类问题。 如果你有切实可行的自动化方案，使用其他已更新配置的节点来替换该节点， 或者使用自动化方案来重新安装。

#### Cgroup v2

Cgroup v2 是 cgroup Linux API 的下一个版本。与 cgroup v1 不同的是， Cgroup v2 只有一个层次结构，而不是每个控制器有一个不同的层次结构。

新版本对 cgroup v1 进行了多项改进，其中一些改进是：

*   更简洁、更易于使用的 API
*   可将安全子树委派给容器
*   更新的功能，如压力失速信息（Pressure Stall Information）

尽管内核支持混合配置，即其中一些控制器由 cgroup v1 管理，另一些由 cgroup v2 管理， Kubernetes 仅支持使用同一 cgroup 版本来管理所有控制器。

如果 systemd 默认不使用 cgroup v2，你可以通过在内核命令行中添加 ​`systemd.unified_cgroup_hierarchy=1`​ 来配置系统去使用它。

`# 此示例适用于使用 DNF 包管理器的 Linux 操作系统 # 你的系统可能使用不同的方法来设置 Linux 内核使用的命令行。 sudo dnf install -y grubby && \   sudo grubby \   --update-kernel=ALL \   --args="systemd.unified_cgroup_hierarchy=1"`

如果更改内核的命令行，则必须重新启动节点才能使更改生效。

切换到 cgroup v2 时，用户体验不应有任何明显差异， 除非用户直接在节点上或在容器内访问 cgroup 文件系统。 为了使用它，CRI 运行时也必须支持 cgroup v2。

#### 将 kubeadm 托管的集群迁移到 systemd 驱动

如果你希望将现有的由 kubeadm 管理的集群迁移到 systemd cgroup 驱动程序， 请按照配置 cgroup 驱动程序操作。

CRI 版本支持
--------

你的容器运行时必须至少支持容器运行时接口的 v1alpha2。

Kubernetes 1.24 默认使用 v1 的 CRI API。如果容器运行时不支持 v1 API， 则 kubelet 会回退到使用（已弃用的）v1alpha2 API。

容器运行时 
------

#### containerd

本节概述了使用 containerd 作为 CRI 运行时的必要步骤。

使用以下命令在系统上安装 Containerd：

按照[开始使用 containerd](https://github.com/containerd/containerd/blob/main/docs/getting-started.md) 的说明进行操作。 创建有效的配置文件 ​`config.toml`​ 后返回此步骤。

*   Linux

你可以在路径 ​`/etc/containerd/config.toml`​ 下找到此文件。

*   Windows

你可以在路径 \`​`C:\Program Files\containerd\config.toml`​\` 下找到此文件。

在 Linux 上，containerd 的默认 CRI 套接字是 ​`/run/containerd/containerd.sock`​。 在 Windows 上，默认 CRI 端点是 ​`npipe://./pipe/containerd-containerd`​。

#### 配置 systemd cgroup 驱动程序

结合 ​`runc` ​使用 ​`systemd` ​cgroup 驱动，在 ​`/etc/containerd/config.toml`​ 中设置

`[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]   ...   [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]     SystemdCgroup = true`

如果你应用此更改，请确保重新启动 containerd：

`sudo systemctl restart containerd`

当使用 kubeadm 时，请手动配置 kubelet 的 cgroup 驱动.

#### CRI-O

本节包含安装 CRI-O 作为容器运行时的必要步骤。

要安装 CRI-O，请按照 [CRI-O 安装说明](https://github.com/cri-o/cri-o/blob/main/install.md target=)执行操作。

#### cgroup 驱动程序

CRI-O 默认使用 systemd cgroup 驱动程序，这对你来说可能工作得很好。要切换到 ​`cgroupfs` ​cgroup 驱动程序， 请编辑 ​`/etc/crio/crio.conf`​ 或在 ​`/etc/crio/crio.conf.d/02-cgroup-manager.conf`​ 中放置一个插入式配置 ，例如：

`[crio.runtime] conmon_cgroup = "pod" cgroup_manager = "cgroupfs"`

你还应该注意到 ​`conmon_cgroup` ​被更改，当使用 CRI-O 和 ​`cgroupfs` ​时，必须将其设置为值 ​`pod`​。 通常需要保持 kubelet 的 cgroup 驱动配置（通常通过 kubeadm 完成）和 CRI-O 同步。

对于 CRI-O，CRI 套接字默认为 ​`/var/run/crio/crio.sock`​。

#### Docker Engine

> Note: 以下操作假设你使用 [cri-dockerd](https://github.com/Mirantis/cri-dockerd) 适配器来将 Docker Engine 与 Kubernetes 集成。

1.  在你的每个节点上，遵循[安装 Docker 引擎](https://docs.docker.com/engine/install/ target=)指南为你的 Linux 发行版安装 Docker。
2.  按照源代码仓库中的说明安装 [cri-dockerd](https://github.com/Mirantis/cri-dockerd)。

对于 ​`cri-dockerd`​，默认情况下，CRI 套接字是 ​`/run/cri-dockerd.sock`​。

#### Mirantis 容器运行时

[Mirantis Container Runtime](https://docs.mirantis.com/mcr/20.10/overview.html) (MCR) 是一种商用容器运行时，以前称为 Docker 企业版。 你可以使用 MCR 中包含的开源 ​[`cri-dockerd`​](https://github.com/Mirantis/cri-dockerd) 组件将 Mirantis Container Runtime 与 Kubernetes 一起使用。

要了解有关如何安装 Mirantis Container Runtime 的更多信息，请访问 [MCR 部署指南](https://docs.mirantis.com/mcr/20.10/install.html)。

检查名为 ​`cri-docker.socket`​ 的 systemd 单元以找出 CRI 套接字的路径。

##  2.  Kubernetes 使用部署工具安装Kubernetes

###  1.  Kubernetes 安装kubeadm
在开始之前
-----

*   一台兼容的 Linux 主机。Kubernetes 项目为基于 Debian 和 Red Hat 的 Linux 发行版以及一些不提供包管理器的发行版提供通用的指令
*   每台机器 2 GB 或更多的 RAM （如果少于这个数字将会影响你应用的运行内存)
*   2 CPU 核或更多
*   集群中的所有机器的网络彼此均能相互连接(公网和内网都可以)
*   节点之中不可以有重复的主机名、MAC 地址或 product\_uuid
*   开启机器上的某些端口
*   禁用交换分区。为了保证 kubelet 正常工作，你 必须 禁用交换分区

确保每个节点上 MAC 地址和 product\_uuid 的唯一性
----------------------------------

*   你可以使用命令 ​`ip link`​ 或 ​`ifconfig -a`​ 来获取网络接口的 MAC 地址
*   可以使用 ​`sudo cat /sys/class/dmi/id/product_uuid`​ 命令对 product\_uuid 校验

一般来讲，硬件设备会拥有唯一的地址，但是有些虚拟机的地址可能会重复。 Kubernetes 使用这些值来唯一确定集群中的节点。 如果这些值在每个节点上不唯一，可能会导致安装 [失败](https://github.com/kubernetes/kubeadm/issues/31)。

检查网络适配器
-------

如果你有一个以上的网络适配器，同时你的 Kubernetes 组件通过默认路由不可达，我们建议你预先添加 IP 路由规则，这样 Kubernetes 集群就可以通过对应的适配器完成连接。

允许 iptables 检查桥接流量
------------------

确保 ​`br_netfilter`​ 模块被加载。这一操作可以通过运行 ​`lsmod | grep br_netfilter`​ 来完成。若要显式加载该模块，可执行 ​`sudo modprobe br_netfilter`​。

为了让你的 Linux 节点上的 iptables 能够正确地查看桥接流量，你需要确保在你的 ​`sysctl` ​配置中将 ​`net.bridge.bridge-nf-call-iptables`​ 设置为 1。例如：

`cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF  cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system`

检查所需端口
------

启用必要的端口后才能使 Kubernetes 的各组件相互通信。可以使用 netcat 之类的工具来检查端口是否启用，例如：

`nc 127.0.0.1 6443`

你使用的 Pod 网络插件 (详见后续章节) 也可能需要开启某些特定端口。由于各个 Pod 网络插件的功能都有所不同， 请参阅他们各自文档中对端口的要求。

安装容器运行时
-------

为了在 Pod 中运行容器，Kubernetes 使用 容器运行时（Container Runtime）。

默认情况下，Kubernetes 使用 容器运行时接口（Container Runtime Interface，CRI） 来与你所选择的容器运行时交互。

如果你不指定运行时，kubeadm 会自动尝试通过扫描已知的端点列表来检测已安装的容器运行时。

如果检测到有多个或者没有容器运行时，kubeadm 将抛出一个错误并要求你指定一个想要使用的运行时。

###  2.  Kubernetes 对kubeadm进行故障排查
对 kubeadm 进行故障排查
----------------

与任何程序一样，你可能会在安装或者运行 kubeadm 时遇到错误。 本文列举了一些常见的故障场景，并提供可帮助你理解和解决这些问题的步骤。

如果你的问题未在下面列出，请执行以下步骤：

*   如果你认为问题是 kubeadm 的错误：

*   转到 [github.com/kubernetes/kubeadm](https://github.com/kubernetes/kubeadm/issues) 并搜索存在的问题。
*   如果没有问题，请 [打开](https://github.com/kubernetes/kubeadm/issues/new) 并遵循问题模板。

*   如果你对 kubeadm 的工作方式有疑问，可以在 [Slack](https://slack.k8s.io/) 上的 ​`#kubeadm`​ 频道提问， 或者在 [StackOverflow](https://stackoverflow.com/questions/tagged/kubernetes) 上提问。 请加入相关标签，例如 ​`#kubernetes`​ 和 ​`#kubeadm`​，这样其他人可以帮助你。

由于缺少 RBAC，无法将 v1.18 Node 加入 v1.17 集群
------------------------------------

自从 v1.18 后，如果集群中已存在同名 Node，kubeadm 将禁止 Node 加入集群。 这需要为 bootstrap-token 用户添加 RBAC 才能 GET Node 对象。

但这会导致一个问题，v1.18 的 ​`kubeadm join`​ 无法加入由 kubeadm v1.17 创建的集群。

要解决此问题，你有两种选择：

使用 kubeadm v1.18 在控制平面节点上执行 ​`kubeadm init phase bootstrap-token`​。 请注意，这也会启用 bootstrap-token 的其余权限。

或者，也可以使用 ​`kubectl apply -f ...`​ 手动应用以下 RBAC：

`apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata:   name: kubeadm:get-nodes rules: - apiGroups:   - ""   resources:   - nodes   verbs:   - get --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata:   name: kubeadm:get-nodes roleRef:   apiGroup: rbac.authorization.k8s.io   kind: ClusterRole   name: kubeadm:get-nodes subjects: - apiGroup: rbac.authorization.k8s.io   kind: Group   name: system:bootstrappers:kubeadm:default-node-token`

在安装过程中没有找到 ebtables 或者其他类似的可执行文件
--------------------------------

如果在运行 ​`kubeadm init`​ 命令时，遇到以下的警告

`[preflight] WARNING: ebtables not found in system path [preflight] WARNING: ethtool not found in system path`

那么或许在你的节点上缺失 ​`ebtables`​、​`ethtool` ​或者类似的可执行文件。 你可以使用以下命令安装它们：

*   对于 Ubuntu/Debian 用户，运行 ​`apt install ebtables ethtool`​ 命令。
*   对于 CentOS/Fedora 用户，运行 ​`yum install ebtables ethtool`​ 命令。

在安装过程中，kubeadm 一直等待控制平面就绪
-------------------------

如果你注意到 ​`kubeadm init`​ 在打印以下行后挂起：

`[apiclient] Created API client, waiting for the control plane to become ready`

这可能是由许多问题引起的。最常见的是：

*   网络连接问题。在继续之前，请检查你的计算机是否具有全部联通的网络连接。
*   容器运行时的 cgroup 驱动不同于 kubelet 使用的 cgroup 驱动。
*   控制平面上的 Docker 容器持续进入崩溃状态或（因其他原因）挂起。你可以运行 docker ps 命令来检查以及 docker logs 命令来检视每个容器的运行日志。 

当删除托管容器时 kubeadm 阻塞 
--------------------

如果容器运行时停止并且未删除 Kubernetes 所管理的容器，可能发生以下情况：

`sudo kubeadm reset`

`[preflight] Running pre-flight checks [reset] Stopping the kubelet service [reset] Unmounting mounted directories in "/var/lib/kubelet" [reset] Removing kubernetes-managed containers (block)`

一个可行的解决方案是重新启动 Docker 服务，然后重新运行 ​`kubeadm reset`​： 你也可以使用 ​`crictl` ​来调试容器运行时的状态。

Pods 处于 RunContainerError、CrashLoopBackOff 或者 Error 状态
------------------------------------------------------

在 ​`kubeadm init`​ 命令运行后，系统中不应该有 pods 处于这类状态。

*   在 ​`kubeadm init`​ 命令执行完后，如果有 pods 处于这些状态之一，请在 kubeadm 仓库提起一个 issue。​`coredns` ​(或者 ​`kube-dns`​) 应该处于 ​`Pending` ​状态， 直到你部署了网络插件为止。
*   如果在部署完网络插件之后，有 Pods 处于 ​`RunContainerError`​、​`CrashLoopBackOff` ​或 ​`Error` ​状态之一，并且 ​`coredns` ​（或者 ​`kube-dns`​）仍处于 ​`Pending` ​状态， 那很可能是你安装的网络插件由于某种原因无法工作。你或许需要授予它更多的 RBAC 特权或使用较新的版本。请在 Pod Network 提供商的问题跟踪器中提交问题， 然后在此处分类问题。
*   如果你安装的 Docker 版本早于 1.12.1，请在使用 ​`systemd` ​来启动 ​`dockerd` ​和重启 ​`docker` ​时， 删除 ​`MountFlags=slave`​ 选项。 你可以在 ​`/usr/lib/systemd/system/docker.service`​ 中看到 MountFlags。 MountFlags 可能会干扰 Kubernetes 挂载的卷，并使 Pods 处于 ​`CrashLoopBackOff` ​状态。 当 Kubernetes 不能找到 ​`var/run/secrets/kubernetes.io/serviceaccount`​ 文件时会发生错误。

coredns 停滞在 Pending 状态
----------------------

这一行为是 预期之中 的，因为系统就是这么设计的。 kubeadm 的网络供应商是中立的，因此管理员应该选择 安装 pod 的网络插件。 你必须完成 Pod 的网络配置，然后才能完全部署 CoreDNS。 在网络被配置好之前，DNS 组件会一直处于 ​`Pending` ​状态。

HostPort 服务无法工作
---------------

此 ​`HostPort` ​和 ​`HostIP` ​功能是否可用取决于你的 Pod 网络配置。请联系 Pod 网络插件的作者， 以确认 ​`HostPort` ​和 ​`HostIP` ​功能是否可用。

已验证 Calico、Canal 和 Flannel CNI 驱动程序支持 HostPort。

有关更多信息，请参考 [CNI portmap 文档](https://github.com/containernetworking/plugins/blob/main/plugins/meta/portmap/README.md).

如果你的网络提供商不支持 portmap CNI 插件，你或许需要使用 NodePort 服务的功能 或者使用 ​`HostNetwork=true`​。

无法通过其服务 IP 访问 Pod
-----------------

*   许多网络附加组件尚未启用 hairpin 模式 该模式允许 Pod 通过其服务 IP 进行访问。这是与 [CNI](https://github.com/containernetworking/cni/issues/476) 有关的问题。 请与网络附加组件提供商联系，以获取他们所提供的 hairpin 模式的最新状态。
*   如果你正在使用 VirtualBox (直接使用或者通过 Vagrant 使用)，你需要 确保 ​`hostname -i`​ 返回一个可路由的 IP 地址。默认情况下，第一个接口连接不能路由的仅主机网络。 解决方法是修改 ​`/etc/hosts`​，请参考示例 [Vagrantfile](https://github.com/errordeveloper/kubernetes-ansible-vagrant/blob/22dd39dfc06111235620e6c4404a96ae146f26fd/Vagrantfile target=)。

TLS 证书错误
--------

以下错误指出证书可能不匹配。

`# kubectl get pods Unable to connect to the server: x509: certificate signed by unknown authority (possibly because of "crypto/rsa: verification error" while trying to verify candidate authority certificate "kubernetes")`

*   验证 ​`$HOME/.kube/config`​ 文件是否包含有效证书，并 在必要时重新生成证书。在 kubeconfig 文件中的证书是 base64 编码的。 该 ​`base64 --decode`​ 命令可以用来解码证书，​`openssl x509 -text -noout`​ 命令 可以用于查看证书信息。
*   使用如下方法取消设置 ​`KUBECONFIG` ​环境变量的值：

`unset KUBECONFIG`

或者将其设置为默认的 ​`KUBECONFIG` ​位置：

`export KUBECONFIG=/etc/kubernetes/admin.conf`

*   另一个方法是覆盖 ​`kubeconfig` ​的现有用户 "管理员"：

`mv  $HOME/.kube $HOME/.kube.bak mkdir $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config`

Kubelet 客户端证书轮换失败 
------------------

默认情况下，kubeadm 使用 ​`/etc/kubernetes/kubelet.conf`​ 中指定的 ​`/var/lib/kubelet/pki/kubelet-client-current.pem`​ 符号链接 来配置 kubelet 自动轮换客户端证书。如果此轮换过程失败，你可能会在 kube-apiserver 日志中看到 诸如 ​`x509: certificate has expired or is not yet valid`​ 之类的错误。要解决此问题，你必须执行以下步骤：

1.  从故障节点备份和删除 ​`/etc/kubernetes/kubelet.conf`​ 和 ​`/var/lib/kubelet/pki/kubelet-client*`​。
2.  在集群中具有 ​`/etc/kubernetes/pki/ca.key`​ 的、正常工作的控制平面节点上 执行 ​`kubeadm kubeconfig user --org system:nodes --client-name system:node:$NODE > kubelet.conf`​。 ​`$NODE`​ 必须设置为集群中现有故障节点的名称。 手动修改生成的 ​`kubelet.conf`​ 以调整集群名称和服务器端点， 或传递 ​`kubeconfig user --config`​（此命令接受 ​`InitConfiguration`​）。 如果你的集群没有 ​`ca.key`​，你必须在外部对 ​`kubelet.conf`​ 中的嵌入式证书进行签名。
3.  将得到的 ​`kubelet.conf`​ 文件复制到故障节点上，作为 ​`/etc/kubernetes/kubelet.conf`​。
4.  在故障节点上重启 kubelet（​`systemctl restart kubelet`​），等待 ​`/var/lib/kubelet/pki/kubelet-client-current.pem`​ 重新创建。
5.  手动编辑 ​`kubelet.conf`​ 指向轮换的 kubelet 客户端证书，方法是将 ​`client-certificate-data`​ 和 ​`client-key-data`​ 替换为：

`client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem client-key: /var/lib/kubelet/pki/kubelet-client-current.pem`

7.  重新启动 kubelet。
8.  确保节点状况变为 ​`Ready`​。

在 Vagrant 中使用 flannel 作为 pod 网络时的默认 NIC
---------------------------------------

以下错误可能表明 Pod 网络中出现问题：

`Error from server (NotFound): the server could not find the requested resource`

*   如果你正在 Vagrant 中使用 flannel 作为 pod 网络，则必须指定 flannel 的默认接口名称。

Vagrant 通常为所有 VM 分配两个接口。第一个为所有主机分配了 IP 地址 ​`10.0.2.15`​，用于获得 NATed 的外部流量。

这可能会导致 flannel 出现问题，它默认为主机上的第一个接口。这导致所有主机认为它们具有 相同的公共 IP 地址。为防止这种情况，传递 ​`--iface eth1`​ 标志给 flannel 以便选择第二个接口。

容器使用的非公共 IP
-----------

在某些情况下 ​`kubectl logs`​ 和 ​`kubectl run`​ 命令或许会返回以下错误，即便除此之外集群一切功能正常：

`Error from server: Get https://10.19.0.41:10250/containerLogs/default/mysql-ddc65b868-glc5m/mysql: dial tcp 10.19.0.41:10250: getsockopt: no route to host`

*   这或许是由于 Kubernetes 使用的 IP 无法与看似相同的子网上的其他 IP 进行通信的缘故， 可能是由机器提供商的政策所导致的。
*   DigitalOcean 既分配一个共有 IP 给 ​`eth0`​，也分配一个私有 IP 在内部用作其浮动 IP 功能的锚点， 然而 ​`kubelet` ​将选择后者作为节点的 ​`InternalIP` ​而不是公共 IP

使用 ​`ip addr show`​ 命令代替 ​`ifconfig` ​命令去检查这种情况，因为 ​`ifconfig` ​命令 不会显示有问题的别名 IP 地址。或者指定的 DigitalOcean 的 API 端口允许从 droplet 中 查询 anchor IP：

`curl http://169.254.169.254/metadata/v1/interfaces/public/0/anchor_ipv4/address`

解决方法是通知 ​`kubelet` ​使用哪个 ​`--node-ip`​。当使用 DigitalOcean 时，可以是公网IP（分配给 ​`eth0` ​的）， 或者是私网IP（分配给 ​`eth1` ​的）。私网 IP 是可选的。 kubadm ​`NodeRegistrationOptions` ​结构 的 ​`KubeletExtraArgs` ​部分被用来处理这种情况。

然后重启 ​`kubelet`​：

`systemctl daemon-reload systemctl restart kubelet`

coredns pods 有 CrashLoopBackOff 或者 Error 状态
-------------------------------------------

如果有些节点运行的是旧版本的 Docker，同时启用了 SELinux，你或许会遇到 ​`coredns` ​pods 无法启动的情况。 要解决此问题，你可以尝试以下选项之一：

*   升级到 Docker 的较新版本
*   [禁用 SELinux](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/security-enhanced_linux/sect-security-enhanced_linux-enabling_and_disabling_selinux-disabling_selinux)
*   修改 ​`coredns` ​部署以设置 ​`allowPrivilegeEscalation` ​为 ​`true`​：

`kubectl -n kube-system get deployment coredns -o yaml | \   sed 's/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g' | \   kubectl apply -f -`

CoreDNS 处于 ​`CrashLoopBackOff` ​时的另一个原因是当 Kubernetes 中部署的 CoreDNS Pod 检测 到环路时。[有许多解决方法](https://github.com/coredns/coredns/tree/master/plugin/loop target=) 可以避免在每次 CoreDNS 监测到循环并退出时，Kubernetes 尝试重启 CoreDNS Pod 的情况。

> Warning: 禁用 SELinux 或设置 ​`allowPrivilegeEscalation` ​为 ​`true` ​可能会损害集群的安全性。

etcd pods 持续重启
--------------

如果你遇到以下错误：

`rpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container process caused "process_linux.go:110: decoding init error from pipe caused \"read parent: connection reset by peer\""`

如果你使用 Docker 1.13.1.84 运行 CentOS 7 就会出现这种问题。 此版本的 Docker 会阻止 kubelet 在 etcd 容器中执行。

为解决此问题，请选择以下选项之一：

*   回滚到早期版本的 Docker，例如 1.13.1-75

`yum downgrade docker-1.13.1-75.git8633870.el7.centos.x86_64 docker-client-1.13.1-75.git8633870.el7.centos.x86_64 docker-common-1.13.1-75.git8633870.el7.centos.x86_64`

*   安装较新的推荐版本之一，例如 18.06:

`sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum install docker-ce-18.06.1.ce-3.el7.x86_64`

无法将以逗号分隔的值列表传递给 --component-extra-args 标志内的参数
---------------------------------------------

​`kubeadm init`​ 标志例如 ​`--component-extra-args`​ 允许你将自定义参数传递给像 kube-apiserver 这样的控制平面组件。然而，由于解析 (​`mapStringString`​) 的基础类型值，此机制将受到限制。

如果你决定传递一个支持多个逗号分隔值（例如 ​`--apiserver-extra-args "enable-admission-plugins=LimitRanger,NamespaceExists"`​）参数， 将出现 ​`flag: malformed pair, expect string=string`​ 错误。 发生这种问题是因为参数列表 ​`--apiserver-extra-args`​ 预期的是 ​`key=value`​ 形式， 而这里的 ​`NamespacesExists`​ 被误认为是缺少取值的键名。

一种解决方法是尝试分离 ​`key=value`​ 对，像这样： ​`--apiserver-extra-args "enable-admission-plugins=LimitRanger,enable-admission-plugins=NamespaceExists"`​ 但这将导致键 ​`enable-admission-plugins`​ 仅有值 ​`NamespaceExists`​。

已知的解决方法是使用 kubeadm 配置文件。

在节点被云控制管理器初始化之前，kube-proxy 就被调度了
--------------------------------

在云环境场景中，可能出现在云控制管理器完成节点地址初始化之前，kube-proxy 就被调度到新节点了。 这会导致 kube-proxy 无法正确获取节点的 IP 地址，并对管理负载平衡器的代理功能产生连锁反应。

在 kube-proxy Pod 中可以看到以下错误：

`server.go:610] Failed to retrieve node IP: host IP unknown; known addresses: [] proxier.go:340] invalid nodeIP, initializing kube-proxy with 127.0.0.1 as nodeIP`

一种已知的解决方案是修补 kube-proxy DaemonSet，以允许在控制平面节点上调度它， 而不管它们的条件如何，将其与其他节点保持隔离，直到它们的初始保护条件消除：

`kubectl -n kube-system patch ds kube-proxy -p='{ "spec": { "template": { "spec": { "tolerations": [ { "key": "CriticalAddonsOnly", "operator": "Exists" }, { "effect": "NoSchedule", "key": "node-role.kubernetes.io/master" }, { "effect": "NoSchedule", "key": "node-role.kubernetes.io/control-plane" } ] } } } }'`

此问题的跟踪[在这里](https://github.com/kubernetes/kubeadm/issues/1027)。

节点上的 /usr 被以只读方式挂载
------------------

在类似 Fedora CoreOS 或者 Flatcar Container Linux 这类 Linux 发行版本中， 目录 ​`/usr`​ 是以只读文件系统的形式挂载的。 在支持 [FlexVolume](https://github.com/kubernetes/community/blob/ab55d85/contributors/devel/sig-storage/flexvolume.md)时， 类似 kubelet 和 kube-controller-manager 这类 Kubernetes 组件使用默认路径 ​`/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`​， 而 FlexVolume 的目录 必须是可写入的，该功能特性才能正常工作。 （注意：FlexVolume 在 Kubernetes v1.23 版本中已被弃用）

为了解决这个问题，你可以使用 kubeadm 的配置文件 来配置 FlexVolume 的目录。

在（使用 ​`kubeadm init`​ 创建的）主控制节点上，使用 ​`--config`​ 参数传入如下文件：

`apiVersion: kubeadm.k8s.io/v1beta3 kind: InitConfiguration nodeRegistration:   kubeletExtraArgs:     volume-plugin-dir: "/opt/libexec/kubernetes/kubelet-plugins/volume/exec/" --- apiVersion: kubeadm.k8s.io/v1beta3 kind: ClusterConfiguration controllerManager:   extraArgs:     flex-volume-plugin-dir: "/opt/libexec/kubernetes/kubelet-plugins/volume/exec/"`

在加入到集群中的节点上，使用下面的文件：

`apiVersion: kubeadm.k8s.io/v1beta3 kind: JoinConfiguration nodeRegistration:   kubeletExtraArgs:     volume-plugin-dir: "/opt/libexec/kubernetes/kubelet-plugins/volume/exec/"`

或者，你要可以更改 ​`/etc/fstab`​ 使得 ​`/usr`​ 目录能够以可写入的方式挂载，不过 请注意这样做本质上是在更改 Linux 发行版的某种设计原则。

kubeadm upgrade plan 输出错误信息 context deadline exceeded
-----------------------------------------------------

在使用 ​`kubeadm` ​来升级某运行外部 etcd 的 Kubernetes 集群时可能显示这一错误信息。 这并不是一个非常严重的一个缺陷，之所以出现此错误信息，原因是老的 kubeadm 版本会对外部 etcd 集群执行版本检查。你可以继续执行 ​`kubeadm upgrade apply ...`​。

这一问题已经在 1.19 版本中得到修复。

kubeadm reset 会卸载 /var/lib/kubelet
----------------------------------

如果已经挂载了 ​`/var/lib/kubelet`​ 目录，执行 ​`kubeadm reset`​ 操作的时候 会将其卸载。

要解决这一问题，可以在执行了 ​`kubeadm reset`​ 操作之后重新挂载 ​`/var/lib/kubelet`​ 目录。

这是一个在 1.15 中引入的故障，已经在 1.20 版本中修复。

无法在 kubeadm 集群中安全地使用 metrics-server 
------------------------------------

在 kubeadm 集群中可以通过为 [metrics-server](https://github.com/kubernetes-sigs/metrics-server) 设置 ​`--kubelet-insecure-tls`​ 来以不安全的形式使用该服务。 建议不要在生产环境集群中这样使用。

如果你需要在 metrics-server 和 kubelet 之间使用 TLS，会有一个问题， kubeadm 为 kubelet 部署的是自签名的服务证书。这可能会导致 metrics-server 端报告下面的错误信息：

`x509: certificate signed by unknown authority x509: certificate is valid for IP-foo not IP-bar`

另请参阅 [How to run the metrics-server securely](https://github.com/kubernetes-sigs/metrics-server/blob/master/FAQ.md target=)。

###  3.  Kubernetes 使用kubeadm创建集群
使用 kubeadm 创建集群
---------------

使用 ​`kubeadm`​，你能创建一个符合最佳实践的最小化 Kubernetes 集群。 事实上，你可以使用 ​`kubeadm` ​配置一个通过 Kubernetes 一致性测试的集群。 ​`kubeadm` ​还支持其他集群生命周期功能， 例如启动引导令牌和集群升级。

kubeadm 工具很棒，如果你需要：

*   一个尝试 Kubernetes 的简单方法。
*   一个现有用户可以自动设置集群并测试其应用程序的途径。
*   其他具有更大范围的生态系统和/或安装工具中的构建模块。

你可以在各种机器上安装和使用 ​`kubeadm`​：笔记本电脑， 一组云服务器，Raspberry Pi 等。无论是部署到云还是本地， 你都可以将 ​`kubeadm` ​集成到预配置系统中，例如 Ansible 或 Terraform。

在开始之前
-----

要遵循本指南，你需要：

*   一台或多台运行兼容 deb/rpm 的 Linux 操作系统的计算机；例如：Ubuntu 或 CentOS。
*   每台机器 2 GB 以上的内存，内存不足时应用会受限制。
*   用作控制平面节点的计算机上至少有2个 CPU。
*   集群中所有计算机之间具有完全的网络连接。你可以使用公共网络或专用网络。

你还需要使用可以在新集群中部署特定 Kubernetes 版本对应的 ​`kubeadm`​。

Kubernetes 版本及版本偏差策略适用于 ​`kubeadm` ​以及整个 Kubernetes。 查阅该策略以了解支持哪些版本的 Kubernetes 和 ​`kubeadm`​。 该页面是为 Kubernetes v1.24 编写的。

​`kubeadm` ​工具的整体功能状态为一般可用性（GA）。一些子功能仍在积极开发中。 随着工具的发展，创建集群的实现可能会略有变化，但总体实现应相当稳定。

> Note: 根据定义，在 ​`kubeadm alpha`​ 下的所有命令均在 alpha 级别上受支持。

目标
--

*   安装单个控制平面的 Kubernetes 集群
*   在集群上安装 Pod 网络，以便你的 Pod 可以相互连通

操作指南
----

#### 主机准备

在所有主机上安装 容器运行时 和 kubeadm。

> Note:  
> 如果你已经安装了kubeadm，执行 ​`apt-get update && apt-get upgrade`​ 或 ​`yum update`​ 以获取 kubeadm 的最新版本。  
> 升级时，kubelet 每隔几秒钟重新启动一次， 在 crashloop 状态中等待 kubeadm 发布指令。crashloop 状态是正常现象。 初始化控制平面后，kubelet 将正常运行。

#### 准备所需的容器镜像 

这个步骤是可选的，只适用于你希望 ​`kubeadm init`​ 和 ​`kubeadm join`​ 不去下载存放在 ​`k8s.gcr.io`​ 上的默认的容器镜像的情况。

当你在离线的节点上创建一个集群的时候，Kubeadm 有一些命令可以帮助你预拉取所需的镜像。

Kubeadm 允许你给所需要的镜像指定一个自定义的镜像仓库。

#### 初始化控制平面节点

控制平面节点是运行控制平面组件的机器， 包括 etcd （集群数据库） 和 API Server （命令行工具 kubectl 与之通信）。

1.  （推荐）如果计划将单个控制平面 kubeadm 集群升级成高可用， 你应该指定 ​`--control-plane-endpoint`​ 为所有控制平面节点设置共享端点。 端点可以是负载均衡器的 DNS 名称或 IP 地址。
2.  选择一个 Pod 网络插件，并验证是否需要为 ​`kubeadm init`​ 传递参数。 根据你选择的第三方网络插件，你可能需要设置 ​`--pod-network-cidr`​ 的值。 
3.  （可选）​`kubeadm` ​试图通过使用已知的端点列表来检测容器运行时。 使用不同的容器运行时或在预配置的节点上安装了多个容器运行时，请为 ​`kubeadm init`​ 指定 ​`--cri-socket`​ 参数。
4.  （可选）除非另有说明，否则 ​`kubeadm` ​使用与默认网关关联的网络接口来设置此控制平面节点 API server 的广播地址。 要使用其他网络接口，请为 ​`kubeadm init`​ 设置 ​`--apiserver-advertise-address=<ip-address>`​ 参数。 要部署使用 IPv6 地址的 Kubernetes 集群， 必须指定一个 IPv6 地址，例如 ​`--apiserver-advertise-address=fd00::101`​

要初始化控制平面节点，请运行：

`kubeadm init <args>`

#### 关于 apiserver-advertise-address 和 ControlPlaneEndpoint 的注意事项 

​`--apiserver-advertise-address`​ 可用于为控制平面节点的 API server 设置广播地址， ​`--control-plane-endpoint`​ 可用于为所有控制平面节点设置共享端点。

​`--control-plane-endpoint`​ 允许 IP 地址和可以映射到 IP 地址的 DNS 名称。 请与你的网络管理员联系，以评估有关此类映射的可能解决方案。

这是一个示例映射：

`192.168.0.102 cluster-endpoint`

其中 ​`192.168.0.102`​ 是此节点的 IP 地址，​`cluster-endpoint`​ 是映射到该 IP 的自定义 DNS 名称。 这将允许你将 ​`--control-plane-endpoint=cluster-endpoint`​ 传递给 ​`kubeadm init`​，并将相同的 DNS 名称传递给 ​`kubeadm join`​。 稍后你可以修改 ​`cluster-endpoint`​ 以指向高可用性方案中的负载均衡器的地址。

kubeadm 不支持将没有 ​`--control-plane-endpoint`​ 参数的单个控制平面集群转换为高可用性集群。

#### 更多信息

要再次运行 ​`kubeadm init`​，你必须首先卸载集群。

如果将具有不同架构的节点加入集群， 请确保已部署的 DaemonSet 对这种体系结构具有容器镜像支持。

​`kubeadm init`​ 首先运行一系列预检查以确保机器 准备运行 Kubernetes。这些预检查会显示警告并在错误时退出。然后 ​`kubeadm init`​ 下载并安装集群控制平面组件。这可能会需要几分钟。 完成之后你应该看到：

`Your Kubernetes control-plane has initialized successfully!  To start using your cluster, you need to run the following as a regular user:    mkdir -p $HOME/.kube   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config   sudo chown $(id -u):$(id -g) $HOME/.kube/config  You should now deploy a Pod network to the cluster. Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:   /docs/concepts/cluster-administration/addons/  You can now join any number of machines by running the following on each node as root:    kubeadm join <control-plane-host>:<control-plane-port> --token <token> --discovery-token-ca-cert-hash sha256:<hash>`

要使非 root 用户可以运行 kubectl，请运行以下命令， 它们也是 ​`kubeadm init`​ 输出的一部分：

`mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config`

或者，如果你是 ​`root` ​用户，则可以运行：

`export KUBECONFIG=/etc/kubernetes/admin.conf`

> Warning:  
> kubeadm 对 ​`admin.conf`​ 中的证书进行签名时，将其配置为 ​`Subject: O = system:masters, CN = kubernetes-admin`​。 ​`system:masters`​ 是一个例外的、超级用户组，可以绕过鉴权层（例如 RBAC）。 不要将 ​`admin.conf`​ 文件与任何人共享，应该使用 ​`kubeadm kubeconfig user`​ 命令为其他用户生成 kubeconfig 文件，完成对他们的定制授权。

记录 ​`kubeadm init`​ 输出的 ​`kubeadm join`​ 命令。 你需要此命令将节点加入集群。

令牌用于控制平面节点和加入节点之间的相互身份验证。 这里包含的令牌是密钥。确保它的安全， 因为拥有此令牌的任何人都可以将经过身份验证的节点添加到你的集群中。 可以使用 ​`kubeadm token`​ 命令列出，创建和删除这些令牌。

#### 安装 Pod 网络附加组件 

> Caution:  
> 本节包含有关网络设置和部署顺序的重要信息。 在继续之前，请仔细阅读所有建议。  
> 你必须部署一个基于 Pod 网络插件的 容器网络接口 (CNI)，以便你的 Pod 可以相互通信。 在安装网络之前，集群 DNS (CoreDNS) 将不会启动。  
> 
> *   注意你的 Pod 网络不得与任何主机网络重叠： 如果有重叠，你很可能会遇到问题。 （如果你发现网络插件的首选 Pod 网络与某些主机网络之间存在冲突， 则应考虑使用一个合适的 CIDR 块来代替， 然后在执行 ​`kubeadm init`​ 时使用 ​`--pod-network-cidr`​ 参数并在你的网络插件的 YAML 中替换它）。
> *   默认情况下，​`kubeadm` ​将集群设置为使用和强制使用 RBAC（基于角色的访问控制）。 确保你的 Pod 网络插件支持 RBAC，以及用于部署它的 manifests 也是如此。
> *   如果要为集群使用 IPv6（双协议栈或仅单协议栈 IPv6 网络）， 请确保你的 Pod 网络插件支持 IPv6。 IPv6 支持已在 CNI [v0.6.0](https://github.com/containernetworking/cni/releases/tag/v0.6.0) 版本中添加。

> Note: kubeadm 应该是与 CNI 无关的，对 CNI 驱动进行验证目前不在我们的端到端测试范畴之内。 如果你发现与 CNI 插件相关的问题，应在其各自的问题跟踪器中记录而不是在 kubeadm 或 kubernetes 问题跟踪器中记录。

一些外部项目为 Kubernetes 提供使用 CNI 的 Pod 网络，其中一些还支持网络策略。

你可以使用以下命令在控制平面节点或具有 kubeconfig 凭据的节点上安装 Pod 网络附加组件：

`kubectl apply -f <add-on.yaml>`

每个集群只能安装一个 Pod 网络。

安装 Pod 网络后，你可以通过在 ​`kubectl get pods --all-namespaces`​ 输出中检查 CoreDNS Pod 是否 ​`Running` ​来确认其是否正常运行。 一旦 CoreDNS Pod 启用并运行，你就可以继续加入节点。

如果你的网络无法正常工作或 CoreDNS 不在“运行中”状态，请查看 ​`kubeadm` ​的 故障排除指南。

#### 托管节点标签 

默认情况下，kubeadm 启用 NodeRestriction 准入控制器来限制 kubelets 在节点注册时可以应用哪些标签。准入控制器文档描述 kubelet ​`--node-labels`​ 选项允许使用哪些标签。 其中 ​`node-role.kubernetes.io/control-plane`​ 标签就是这样一个受限制的标签， kubeadm 在节点创建后使用特权客户端手动应用此标签。 你可以使用一个有特权的 kubeconfig，比如由 kubeadm 管理的 ​`/etc/kubernetes/admin.conf`​， 通过执行 ​`kubectl label`​ 来手动完成操作。

#### 控制平面节点隔离

默认情况下，出于安全原因，你的集群不会在控制平面节点上调度 Pod。 如果你希望能够在控制平面节点上调度 Pod，例如单机 Kubernetes 集群，请运行:

`kubectl taint nodes --all node-role.kubernetes.io/control-plane- node-role.kubernetes.io/master-`

输出看起来像：

`node "test-01" untainted`

这将从任何拥有 ​`node-role.kubernetes.io/control-plane`​ 和 ​`node-role.kubernetes.io/master`​ 污点的节点上移除该污点。

包括控制平面节点，这意味着调度程序将能够在任何地方调度 Pods。

> Note: ​`node-role.kubernetes.io/master`​ 污点已被废弃，kubeadm 将在 1.25 版本中停止使用它。

#### 加入节点

节点是你的工作负载（容器和 Pod 等）运行的地方。要将新节点添加到集群，请对每台计算机执行以下操作：

*   SSH 到机器
*   成为 root （例如 ​`sudo su -`​）
*   运行 ​`kubeadm init`​ 输出的命令，例如：

`kubeadm join --token <token> <control-plane-host>:<control-plane-port> --discovery-token-ca-cert-hash sha256:<hash>`

如果没有令牌，可以通过在控制平面节点上运行以下命令来获取令牌：

`kubeadm token list`

输出类似于以下内容：

`TOKEN                    TTL  EXPIRES              USAGES           DESCRIPTION            EXTRA GROUPS 8ewj1p.9r9hcjoqgajrj4gi  23h  2018-06-12T02:51:28Z authentication,  The default bootstrap  system:                                                    signing          token generated by     bootstrappers:                                                                     'kubeadm init'.        kubeadm:                                                                                            default-node-token`

默认情况下，令牌会在 24 小时后过期。如果要在当前令牌过期后将节点加入集群， 则可以通过在控制平面节点上运行以下命令来创建新令牌：

`kubeadm token create`

输出类似于以下内容：

`5didvk.d09sbcov8ph2amjw`

如果你没有 ​`--discovery-token-ca-cert-hash`​ 的值，则可以通过在控制平面节点上执行以下命令链来获取它：

`openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | \    openssl dgst -sha256 -hex | sed 's/^.* //'`

输出类似于以下内容：

`8cb2de97839780a412b93877f8507ad6c94f73add17d5d7058e91741c9d5ec78`

> Note: 要为 ​`<control-plane-host>:<control-plane-port>`​ 指定 IPv6 元组，必须将 IPv6 地址括在方括号中，例如：​`[fd00::101]:2073`​

输出应类似于：

`[preflight] Running pre-flight checks  ... (log output of join workflow) ...  Node join complete: * Certificate signing request sent to control-plane and response   received. * Kubelet informed of new secure connection details.  Run 'kubectl get nodes' on control-plane to see this machine join.`

几秒钟后，当你在控制平面节点上执行 ​`kubectl get nodes`​，你会注意到该节点出现在输出中。

> Note: 由于集群节点通常是按顺序初始化的，CoreDNS Pods 很可能都运行在第一个控制面节点上。 为了提供更高的可用性，请在加入至少一个新节点后 使用 ​`kubectl -n kube-system rollout restart deployment coredns`​ 命令，重新平衡 CoreDNS Pods。

#### （可选）从控制平面节点以外的计算机控制集群

为了使 kubectl 在其他计算机（例如笔记本电脑）上与你的集群通信， 你需要将管理员 kubeconfig 文件从控制平面节点复制到工作站，如下所示：

`scp root@<control-plane-host>:/etc/kubernetes/admin.conf . kubectl --kubeconfig ./admin.conf get nodes`

> Note:  
> 上面的示例假定为 root 用户启用了 SSH 访问。如果不是这种情况， 你可以使用 ​`scp`​ 将 ​`admin.conf`​ 文件复制给其他允许访问的用户。  
> admin.conf 文件为用户提供了对集群的超级用户特权。 该文件应谨慎使用。对于普通用户，建议生成一个你为其授予特权的唯一证书。 你可以使用 ​`kubeadm alpha kubeconfig user --client-name <CN>`​ 命令执行此操作。 该命令会将 KubeConfig 文件打印到 STDOUT，你应该将其保存到文件并分发给用户。 之后，使用 ​`kubectl create (cluster)rolebinding`​ 授予特权。

#### （可选）将 API 服务器代理到本地主机

如果要从集群外部连接到 API 服务器，则可以使用 ​`kubectl proxy`​：

`scp root@<control-plane-host>:/etc/kubernetes/admin.conf . kubectl --kubeconfig ./admin.conf proxy`

你现在可以在本地访问 API 服务器 ​`http://localhost:8001/api/v1`​。

清理
--

如果你在集群中使用了一次性服务器进行测试，则可以关闭这些服务器，而无需进一步清理。你可以使用 ​`kubectl config delete-cluster`​ 删除对集群的本地引用。

但是，如果要更干净地取消配置集群， 则应首先[清空节点](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands target=)并确保该节点为空， 然后取消配置该节点。

#### 删除节点

使用适当的凭证与控制平面节点通信，运行：

`kubectl drain <node name> --delete-emptydir-data --force --ignore-daemonsets`

在删除节点之前，请重置 ​`kubeadm` ​安装的状态：

`kubeadm reset`

重置过程不会重置或清除 iptables 规则或 IPVS 表。如果你希望重置 iptables，则必须手动进行：

`iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X`

如果要重置 IPVS 表，则必须运行以下命令：

`ipvsadm -C`

现在删除节点：

`kubectl delete node <node name>`

如果你想重新开始，只需运行 ​`kubeadm init`​ 或 ​`kubeadm join`​ 并加上适当的参数。

#### 清理控制平面

你可以在控制平面主机上使用 ​`kubeadm reset`​ 来触发尽力而为的清理。

版本偏差策略
------

虽然 kubeadm 允许所管理的组件有一定程度的版本偏差， 但是建议你将 kubeadm 的版本与控制平面组件、kube-proxy 和 kubelet 的版本相匹配。

#### kubeadm 中的 Kubernetes 版本偏差

kubeadm 可以与 Kubernetes 组件一起使用，这些组件的版本与 kubeadm 相同，或者比它大一个版本。 Kubernetes 版本可以通过使用 ​`--kubeadm init`​ 的 ​`--kubernetes-version`​ 标志或使用 ​`--config`​ 时的 ​`ClusterConfiguration.kubernetesVersion`​ 字段指定给 kubeadm。 这个选项将控制 kube-apiserver、kube-controller-manager、kube-scheduler 和 kube-proxy 的版本。

例如：

*   kubeadm 的版本为 1.24。
*   ​`kubernetesVersion` ​必须为 1.24 或者 1.23。

#### kubeadm 中 kubelet 的版本偏差

与 Kubernetes 版本类似，kubeadm 可以使用与 kubeadm 相同版本的 kubelet， 或者比 kubeadm 老一个版本的 kubelet。

例如：

*   kubeadm 的版本为 1.24
*   主机上的 kubelet 版本必须为 1.24 或者 1.23

#### kubeadm 支持的 kubeadm 的版本偏差

kubeadm 命令在现有节点或由 kubeadm 管理的整个集群上的操作有一定限制。

如果新的节点加入到集群中，用于 ​`kubeadm join`​ 的 kubeadm 二进制文件必须与用 ​`kubeadm init`​ 创建集群或用 ​`kubeadm upgrade`​ 升级同一节点时所用的 kubeadm 版本一致。 类似的规则适用于除了 ​`kubeadm upgrade`​ 以外的其他 kubeadm 命令。

​`kubeadm join`​ 的例子：

*   使用 ​`kubeadm init`​ 创建集群时使用版本为 1.24 的 kubeadm。
*   加入的节点必须使用版本为 1.24 的 kubeadm 二进制文件。

对于正在升级的节点，所使用的的 kubeadm 必须与管理该节点的 kubeadm 具有相同的 MINOR 版本或比后者新一个 MINOR 版本。

​`kubeadm upgrade`​ 的例子:

*   用于创建或升级节点的 kubeadm 版本为 1.23。
*   用于升级节点的 kubeadm 版本必须为 1.23 或 1.24。

局限性
---

#### 集群弹性

此处创建的集群具有单个控制平面节点，运行单个 etcd 数据库。 这意味着如果控制平面节点发生故障，你的集群可能会丢失数据并且可能需要从头开始重新创建。

解决方法：

*   定期[备份 etcd](https://etcd.io/)。 kubeadm 配置的 etcd 数据目录位于控制平面节点上的 ​`/var/lib/etcd`​ 中。
*   使用多个控制平面节点。你可以阅读 可选的高可用性拓扑选择集群拓扑提供的 高可用性。

#### 平台兼容性

kubeadm deb/rpm 软件包和二进制文件是为 amd64、arm (32-bit)、arm64、ppc64le 和 s390x 构建的遵循[多平台提案](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/multi-platform.md)。

从 v1.12 开始还支持用于控制平面和附加组件的多平台容器镜像。

只有一些网络提供商为所有平台提供解决方案。请查阅上方的网络提供商清单或每个提供商的文档以确定提供商是否支持你选择的平台。

> Note:要为<control-plane-host>:<control-plane-port>指定IPv6元组，必须将IPv6地址括在方括号中，例如：\[fd00::101\]:2073

###  4.  Kubernetes 使用kubeadm API定制组件
使用 kubeadm API 定制组件
-------------------

本页面介绍了如何自定义 kubeadm 部署的组件。 你可以使用 ​`ClusterConfiguration` ​结构中定义的参数，或者在每个节点上应用补丁来定制控制平面组件。 你可以使用 ​`KubeletConfiguration` ​和 ​`KubeProxyConfiguration` ​结构分别定制 kubelet 和 kube-proxy 组件。

所有这些选项都可以通过 kubeadm 配置 API 实现。

> Note:  
> kubeadm 目前不支持对 CoreDNS 部署进行定制。 你必须手动更新 ​`kube-system/coredns`​ ConfigMap 并在更新后重新创建 CoreDNS Pods。 或者，你可以跳过默认的 CoreDNS 部署并部署你自己的 CoreDNS 变种。

使用 ClusterConfiguration 中的标志自定义控制平面 
------------------------------------

kubeadm ​`ClusterConfiguration` ​对象为用户提供了一种方法， 用以覆盖传递给控制平面组件（如 APIServer、ControllerManager、Scheduler 和 Etcd）的默认参数。 各组件配置使用如下字段定义：

*   ​`apiServer` ​
*   ​`controllerManager` ​
*   ​`scheduler` ​
*   ​`etcd`​

这些结构包含一个通用的 ​`extraArgs` ​字段，该字段由 ​`key: value`​ 组成。 要覆盖控制平面组件的参数：

1.  将适当的字段 ​`extraArgs` ​添加到配置中。
2.  向字段 ​`extraArgs` ​添加要覆盖的参数值。
3.  用 ​`--config <YOUR CONFIG YAML>`​ 运行 ​`kubeadm init`​。

> Note:  
> 你可以通过运行 ​`kubeadm config print init-defaults`​ 并将输出保存到你所选的文件中， 以默认值形式生成 ​`ClusterConfiguration` ​对象。

> Note:  
> ​`ClusterConfiguration` ​对象目前在 kubeadm 集群中是全局的。 这意味着你添加的任何标志都将应用于同一组件在不同节点上的所有实例。 要在不同节点上为每个组件应用单独的配置，你可以使用补丁。

> Note:  
> 当前不支持重复的参数（keys）或多次传递相同的参数 ​`--foo`​。 要解决此问题，你必须使用补丁。

#### APIServer 参数 

使用示例：

`apiVersion: kubeadm.k8s.io/v1beta3 kind: ClusterConfiguration kubernetesVersion: v1.16.0 apiServer:   extraArgs:     anonymous-auth: "false"     enable-admission-plugins: AlwaysPullImages,DefaultStorageClass     audit-log-path: /home/johndoe/audit.log`

#### ControllerManager 参数 

使用示例：

`apiVersion: kubeadm.k8s.io/v1beta3 kind: ClusterConfiguration kubernetesVersion: v1.16.0 controllerManager:   extraArgs:     cluster-signing-key-file: /home/johndoe/keys/ca.key     deployment-controller-sync-period: "50"`

Scheduler 参数 
-------------

使用示例：

`apiVersion: kubeadm.k8s.io/v1beta3 kind: ClusterConfiguration kubernetesVersion: v1.16.0 scheduler:   extraArgs:     config: /etc/kubernetes/scheduler-config.yaml   extraVolumes:     - name: schedulerconfig       hostPath: /home/johndoe/schedconfig.yaml       mountPath: /etc/kubernetes/scheduler-config.yaml       readOnly: true       pathType: "File"`

#### Etcd 参数 

使用示例：

`apiVersion: kubeadm.k8s.io/v1beta3 kind: ClusterConfiguration etcd:   local:     extraArgs:       election-timeout: 1000`

使用补丁定制控制平面 
-----------

FEATURE STATE: Kubernetes v1.22 \[beta\]

Kubeadm 允许将包含补丁文件的目录传递给各个节点上的 ​`InitConfiguration` ​和 ​`JoinConfiguration`​。 这些补丁可被用作控制平面组件清单写入磁盘之前的最后一个自定义步骤。

可以使用 ​`--config <你的 YAML 格式控制文件>`​ 将配置文件传递给 ​`kubeadm init`​：

`apiVersion: kubeadm.k8s.io/v1beta3 kind: InitConfiguration patches:   directory: /home/user/somedir`

> Note:  
> 对于 ​`kubeadm init`​，你可以传递一个包含 ​`ClusterConfiguration` ​和 ​`InitConfiguration` ​的文件，以 ​`---`​ 分隔。

你可以使用 ​`--config <你的 YAML 格式配置文件>`​ 将配置文件传递给 ​`kubeadm join`​：

`apiVersion: kubeadm.k8s.io/v1beta3 kind: JoinConfiguration patches:   directory: /home/user/somedir`

补丁目录必须包含名为 ​`target[suffix][+patchtype].extension`​ 的文件。 例如，​`kube-apiserver0+merge.yaml`​ 或只是 ​`etcd.json`​。

*   ​`target` ​可以是 ​`kube-apiserver`​、​`kube-controller-manager`​、​`kube-scheduler`​ 和 ​`etcd` ​之一。
*   ​`patchtype` ​可以是 ​`strategy`​、​`merge` ​或 ​`json` ​之一，并且这些必须匹配 kubectl 支持 的补丁格式。 默认补丁类型是 ​`strategic` ​的。
*   ​`extension` ​必须是 ​`json` ​或 ​`yaml`​。
*   ​`suffix` ​是一个可选字符串，可用于确定首先按字母数字应用哪些补丁。

> Note:  
> 如果你使用 ​`kubeadm upgrade`​ 升级 kubeadm 节点，你必须再次提供相同的补丁，以便在升级后保留自定义配置。 为此，你可以使用 ​`--patches`​ 参数，该参数必须指向同一目录。 ​`kubeadm upgrade`​ 目前不支持用于相同目的的 API 结构配置。

自定义 kubelet 
------------

要自定义 kubelet，你可以在同一配置文件中的 ​`ClusterConfiguration` ​或 ​`InitConfiguration` ​之外添加一个 ​`KubeletConfiguration`​，用 ​`---`​ 分隔。 然后可以将此文件传递给 ​`kubeadm init`​。

> Note:  
> kubeadm 将相同的 ​`KubeletConfiguration` ​配置应用于集群中的所有节点。 要应用节点特定设置，你可以使用 ​`kubelet` ​参数进行覆盖，方法是将它们传递到 ​`InitConfiguration` ​和 ​`JoinConfiguration` ​支持的 ​`nodeRegistration.kubeletExtraArgs`​ 字段中。

自定义 kube-proxy 
---------------

要自定义 kube-proxy，你可以在 ​`ClusterConfiguration` ​或 ​`InitConfiguration` ​之外添加一个 由 ​`---`​ 分隔的 ​`KubeProxyConfiguration`​， 传递给 ​`kubeadm init`​。

> Note:  
> kubeadm 将 kube-proxy 部署为 DaemonSet， 这意味着 ​`KubeProxyConfiguration` ​将应用于集群中的所有 kube-proxy 实例。

###  5.  Kubernetes 高可用拓扑选项
高可用拓扑选项
-------

本页面介绍了配置高可用（HA）Kubernetes 集群拓扑的两个选项。

你可以设置 HA 集群：

*   使用堆叠（stacked）控制平面节点，其中 etcd 节点与控制平面节点共存
*   使用外部 etcd 节点，其中 etcd 在与控制平面不同的节点上运行

在设置 HA 集群之前，你应该仔细考虑每种拓扑的优缺点。

> Note:  
> kubeadm 静态引导 etcd 集群。 阅读 etcd [集群指南](https://github.com/etcd-io/etcd/blob/release-3.4/Documentation/op-guide/clustering.md target=)以获得更多详细信息。

堆叠（Stacked）etcd 拓扑 
-------------------

堆叠（Stacked）HA 集群是一种这样的[拓扑](https://en.wikipedia.org/wiki/Network_topology)， 其中 etcd 分布式数据存储集群堆叠在 kubeadm 管理的控制平面节点上，作为控制平面的一个组件运行。

每个控制平面节点运行 ​`kube-apiserver`​、​`kube-scheduler`​ 和 ​`kube-controller-manager`​ 实例。

​`kube-apiserver`​ 使用负载均衡器暴露给工作节点。

每个控制平面节点创建一个本地 etcd 成员（member），这个 etcd 成员只与该节点的 ​`kube-apiserver`​ 通信。 这同样适用于本地 ​`kube-controller-manager`​ 和 ​`kube-scheduler`​ 实例。

这种拓扑将控制平面和 etcd 成员耦合在同一节点上。相对使用外部 etcd 集群， 设置起来更简单，而且更易于副本管理。

然而，堆叠集群存在耦合失败的风险。如果一个节点发生故障，则 etcd 成员和控制平面实例都将丢失， 并且冗余会受到影响。你可以通过添加更多控制平面节点来降低此风险。

因此，你应该为 HA 集群运行至少三个堆叠的控制平面节点。

这是 kubeadm 中的默认拓扑。当使用 ​`kubeadm init`​ 和 ​`kubeadm join --control-plane`​ 时， 在控制平面节点上会自动创建本地 etcd 成员。

![](https://atts.w3cschool.cn/attachments/image/20220531/1653965800244371.svg)  

外部 etcd 拓扑 
-----------

具有外部 etcd 的 HA 集群是一种这样的[拓扑](https://zh.wikipedia.org/wiki/%E7%BD%91%E7%BB%9C%E6%8B%93%E6%89%91)， 其中 etcd 分布式数据存储集群在独立于控制平面节点的其他节点上运行。

就像堆叠的 etcd 拓扑一样，外部 etcd 拓扑中的每个控制平面节点都运行 ​`kube-apiserver`​，​`kube-scheduler`​ 和 ​`kube-controller-manager`​ 实例。 同样，​`kube-apiserver`​ 使用负载均衡器暴露给工作节点。但是 etcd 成员在不同的主机上运行， 每个 etcd 主机与每个控制平面节点的 ​`kube-apiserver`​ 通信。

这种拓扑结构解耦了控制平面和 etcd 成员。因此它提供了一种 HA 设置， 其中失去控制平面实例或者 etcd 成员的影响较小，并且不会像堆叠的 HA 拓扑那样影响集群冗余。

但此拓扑需要两倍于堆叠 HA 拓扑的主机数量。

具有此拓扑的 HA 集群至少需要三个用于控制平面节点的主机和三个用于 etcd 节点的主机。

![](https://atts.w3cschool.cn/attachments/image/20220531/1653965875455303.svg)

###  6.  Kubernetes 利用kubeadm创建高可用集群
利用 kubeadm 创建高可用集群
------------------

本文讲述了使用 kubeadm 设置一个高可用的 Kubernetes 集群的两种不同方式：

*   使用具有堆叠的控制平面节点。这种方法所需基础设施较少。etcd 成员和控制平面节点位于同一位置。
*   使用外部集群。这种方法所需基础设施较多。控制平面的节点和 etcd 成员是分开的。

如果你在安装 HA 集群时遇到问题，请在 kubeadm [问题跟踪](https://github.com/kubernetes/kubeadm/issues/new)里向我们提供反馈。

> Caution: 这篇文档没有讲述在云提供商上运行集群的问题。在云环境中，此处记录的方法不适用于类型为 LoadBalancer 的服务对象，或者具有动态的 PersistentVolumes。

在开始之前
-----

根据集群控制平面所选择的拓扑结构不同，准备工作也有所差异：

*   堆叠（Stacked） etcd 拓扑

需要准备：

*   配置满足 kubeadm 的最低要求 的三台机器作为控制面节点。奇数台控制平面节点有利于机器故障或者网络分区时进行重新选主。

*   机器已经安装好容器运行时，并正常运行

*   配置满足 kubeadm 的最低要求 的三台机器作为工作节点

*   机器已经安装好容器运行时，并正常运行

*   在集群中，确保所有计算机之间存在全网络连接（公网或私网）
*   在所有机器上具有 sudo 权限

*   可以使用其他工具；本教程以 ​`sudo` ​举例

*   从某台设备通过 SSH 访问系统中所有节点的能力
*   所有机器上已经安装 ​`kubeadm` ​和 ​`kubelet`​

*   外部 etcd 拓扑

需要准备：

*   配置满足 kubeadm 的最低要求 的三台机器作为控制面节点。奇数台控制平面节点有利于机器故障或者网络分区时进行重新选主。

*   机器已经安装好容器运行时，并正常运行

*   配置满足 kubeadm 的最低要求 的三台机器作为工作节点

*   机器已经安装好容器运行时，并正常运行

*   在集群中，确保所有计算机之间存在全网络连接（公网或私网）
*   在所有机器上具有 sudo 权限

*   可以使用其他工具；本教程以 ​`sudo` ​举例

*   从某台设备通过 SSH 访问系统中所有节点的能力
*   所有机器上已经安装 ​`kubeadm` ​和 ​`kubelet` ​

还需要准备：

*   给 etcd 集群使用的另外三台及以上机器。为了分布式一致性算法达到更好的投票效果，集群必须由奇数个节点组成。

*   机器上已经安装 ​`kubeadm` ​和 ​`kubelet`​。
*   机器上同样需要安装好容器运行时，并能正常运行。

#### 容器镜像

每台主机需要能够从 Kubernetes 容器镜像仓库（ ​`k8s.gcr.io`​ ）读取和拉取镜像。 想要在无法拉取 Kubernetes 仓库镜像的机器上部署高可用集群也是可行的。通过其他的手段保证主机上已经有对应的容器镜像即可。

#### 命令行 

一旦集群创建成功，需要在 PC 上安装 kubectl 用于管理 Kubernetes。为了方便故障排查，也可以在每个控制平面节点上安装 ​`kubectl`​。

这两种方法的第一步
---------

#### 为 kube-apiserver 创建负载均衡器

> Note: 使用负载均衡器需要许多配置。你的集群搭建可能需要不同的配置。 下面的例子只是其中的一方面配置。

1.  创建一个名为 kube-apiserver 的负载均衡器解析 DNS。

*   在云环境中，应该将控制平面节点放置在 TCP 转发负载平衡后面。 该负载均衡器将流量分配给目标列表中所有运行状况良好的控制平面节点。 API 服务器的健康检查是在 kube-apiserver 的监听端口（默认值 ​`:6443`​） 上进行的一个 TCP 检查。
*   不建议在云环境中直接使用 IP 地址。
*   负载均衡器必须能够在 API 服务器端口上与所有控制平面节点通信。 它还必须允许其监听端口的入站流量。
*   确保负载均衡器的地址始终匹配 kubeadm 的 ​`ControlPlaneEndpoint` ​地址。
*   阅读[软件负载平衡选项指南](https://github.com/kubernetes/kubeadm/blob/main/docs/ha-considerations.md target=) 以获取更多详细信息。

3.  添加第一个控制平面节点到负载均衡器并测试连接：

`nc -v LOAD_BALANCER_IP PORT`

由于 apiserver 尚未运行，预期会出现一个连接拒绝错误。 然而超时意味着负载均衡器不能和控制平面节点通信。 如果发生超时，请重新配置负载均衡器与控制平面节点进行通信。

6.  将其余控制平面节点添加到负载均衡器目标组。

使用堆控制平面和 etcd 节点
----------------

#### 控制平面节点的第一步

1.  初始化控制平面：

`sudo kubeadm init --control-plane-endpoint "LOAD_BALANCER_DNS:LOAD_BALANCER_PORT" --upload-certs`

*   你可以使用 ​`--kubernetes-version`​ 标志来设置要使用的 Kubernetes 版本。 建议将 kubeadm、kebelet、kubectl 和 Kubernetes 的版本匹配。
*   这个 ​`--control-plane-endpoint`​ 标志应该被设置成负载均衡器的地址或 DNS 和端口。
*   这个 ​`--upload-certs`​ 标志用来将在所有控制平面实例之间的共享证书上传到集群。

> Note: 标志 ​`kubeadm init`​、​`--config`​ 和 ​`--certificate-key`​ 不能混合使用， 因此如果你要使用 kubeadm 配置，你必须在相应的配置结构 （位于 ​`InitConfiguration` ​和 ​`JoinConfiguration: controlPlane`​）添加 ​`certificateKey` ​字段。

> Note: 一些 CNI 网络插件如 Calico 需要 CIDR 例如 ​`192.168.0.0/16`​ 和一些像 Weave 没有。通过传递 ​`--pod-network-cidr`​ 标志添加 pod CIDR，或者你可以使用 kubeadm 配置文件，在 ​`ClusterConfiguration` ​的 ​`networking` ​对象下设置 ​`podSubnet` ​字段。

*   输出类似于：

`... You can now join any number of control-plane node by running the following command on each as a root: kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07  Please note that the certificate-key gives access to cluster sensitive data, keep it secret! As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use kubeadm init phase upload-certs to reload certs afterward.  Then you can join any number of worker nodes by running the following on each as root:   kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866`

*   将此输出复制到文本文件。 稍后你将需要它来将控制平面节点和工作节点加入集群。
*   当使用 ​`--upload-certs`​ 调用 ​`kubeadm init`​ 时，主控制平面的证书被加密并上传到 ​`kubeadm-certs`​ Secret 中。
*   要重新上传证书并生成新的解密密钥，请在已加入集群节点的控制平面上使用以下命令：

`sudo kubeadm init phase upload-certs --upload-certs`

*   你还可以在 ​`init` ​期间指定自定义的 ​`--certificate-key`​，以后可以由 ​`join` ​使用。 要生成这样的密钥，可以使用以下命令：

`kubeadm certs certificate-key`

> Note: ​`kubeadm-certs`​ Secret 和解密密钥会在两个小时后失效。

> Caution: 正如命令输出中所述，证书密钥可访问集群敏感数据。请妥善保管！

11.  应用你所选择的 CNI 插件：安装 CNI 驱动。如果适用，请确保配置与 kubeadm 配置文件中指定的 Pod CIDR 相对应。

> Note: 在进行下一步之前，必须选择并部署合适的网络插件。 否则集群不会正常运行。

13.  输入以下内容，并查看控制平面组件的 Pods 启动：

`kubectl get pod -n kube-system -w`

#### 其余控制平面节点的步骤 

> Note: 从 kubeadm 1.15 版本开始，你可以并行加入多个控制平面节点。 在此版本之前，你必须在第一个节点初始化后才能依序的增加新的控制平面节点。

对于每个其他控制平面节点，你应该：

*   执行先前由第一个节点上的 ​`kubeadm init`​ 输出提供给你的 join 命令。 它看起来应该像这样：

`sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07`

*   这个 ​`--control-plane`​ 标志通知 ​`kubeadm join`​ 创建一个新的控制平面。
*   ​`--certificate-key ...`​ 将导致从集群中的 ​`kubeadm-certs`​ Secret 下载控制平面证书并使用给定的密钥进行解密。

外部 etcd 节点
----------

使用外部 etcd 节点设置集群类似于用于堆叠 etcd 的过程， 不同之处在于你应该首先设置 etcd，并在 kubeadm 配置文件中传递 etcd 信息。

#### 设置 ectd 集群

1.  按照下文 去设置 etcd 集群。
2.  根据本章-手动证书分发 的描述配置 SSH。
3.  将以下文件从集群中的任何 etcd 节点复制到第一个控制平面节点：

`export CONTROL_PLANE="ubuntu@10.0.0.7" scp /etc/kubernetes/pki/etcd/ca.crt "${CONTROL_PLANE}": scp /etc/kubernetes/pki/apiserver-etcd-client.crt "${CONTROL_PLANE}": scp /etc/kubernetes/pki/apiserver-etcd-client.key "${CONTROL_PLANE}":`

*   用第一台控制平面机的 ​`user@host`​ 替换 ​`CONTROL_PLANE` ​的值。

#### 设置第一个控制平面节点 

*   用以下内容创建一个名为 ​`kubeadm-config.yaml`​ 的文件：

`--- apiVersion: kubeadm.k8s.io/v1beta3 kind: ClusterConfiguration kubernetesVersion: stable controlPlaneEndpoint: "LOAD_BALANCER_DNS:LOAD_BALANCER_PORT" # change this (see below) etcd:   external:     endpoints:       - https://ETCD_0_IP:2379 # change ETCD_0_IP appropriately       - https://ETCD_1_IP:2379 # change ETCD_1_IP appropriately       - https://ETCD_2_IP:2379 # change ETCD_2_IP appropriately     caFile: /etc/kubernetes/pki/etcd/ca.crt     certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt     keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key`

> Note: 这里的堆叠（stacked）etcd 和外部 etcd 之前的区别在于设置外部 etcd 需要一个 ​`etcd` ​的 ​`external` ​对象下带有 etcd 端点的配置文件。 如果是内部 etcd，是自动管理的。

*   在你的集群中，将配置模板中的以下变量替换为适当值：

*   ​`LOAD_BALANCER_DNS`​
*   ​`LOAD_BALANCER_PORT`​
*   ​`ETCD_0_IP`​
*   ​`ETCD_1_IP`​
*   ​`ETCD_2_IP`​

以下的步骤与设置内置 etcd 的集群是相似的：

1.  在节点上运行 ​`sudo kubeadm init --config kubeadm-config.yaml --upload-certs`​ 命令。
2.  记下输出的 join 命令，这些命令将在以后使用。
3.  应用你选择的 CNI 插件。

> Note: 在进行下一步之前，必须选择并部署合适的网络插件。 否则集群不会正常运行。

#### 其他控制平面节点的步骤

步骤与设置内置 etcd 相同：

*   确保第一个控制平面节点已完全初始化。
*   使用保存到文本文件的 join 命令将每个控制平面节点连接在一起。 建议一次加入一个控制平面节点。
*   不要忘记默认情况下，​`--certificate-key`​ 中的解密秘钥会在两个小时后过期。

列举控制平面之后的常见任务
-------------

#### 安装工作节点

你可以使用之前存储的 ​`kubeadm init`​ 命令的输出将工作节点加入集群中：

`sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866`

手动证书分发 
-------

如果你选择不将 ​`kubeadm init`​ 与 ​`--upload-certs`​ 命令一起使用， 则意味着你将必须手动将证书从主控制平面节点复制到 将要加入的控制平面节点上。

有许多方法可以实现这种操作。在下面的例子中我们使用 ​`ssh` ​和 ​`scp`​：

如果要在单独的一台计算机控制所有节点，则需要 SSH。

1.  在你的主设备上启用 ssh-agent，要求该设备能访问系统中的所有其他节点：

`eval $(ssh-agent)`

3.  将 SSH 身份添加到会话中：

`ssh-add ~/.ssh/path_to_private_key`

5.  检查节点间的 SSH 以确保连接是正常运行的

*   SSH 到任何节点时，请确保添加 ​`-A`​ 标志：

`ssh -A 10.0.0.7`

*   当在任何节点上使用 sudo 时，请确保保持环境变量设置，以便 SSH 转发能够正常工作：

`sudo -E -s`

7.  在所有节点上配置 SSH 之后，你应该在运行过 ​`kubeadm init`​ 命令的第一个 控制平面节点上运行以下脚本。 该脚本会将证书从第一个控制平面节点复制到另一个控制平面节点：

在以下示例中，用其他控制平面节点的 IP 地址替换 ​`CONTROL_PLANE_IPS`​。

`USER=ubuntu # 可定制 CONTROL_PLANE_IPS="10.0.0.7 10.0.0.8" for host in ${CONTROL_PLANE_IPS}; do     scp /etc/kubernetes/pki/ca.crt "${USER}"@$host:     scp /etc/kubernetes/pki/ca.key "${USER}"@$host:     scp /etc/kubernetes/pki/sa.key "${USER}"@$host:     scp /etc/kubernetes/pki/sa.pub "${USER}"@$host:     scp /etc/kubernetes/pki/front-proxy-ca.crt "${USER}"@$host:     scp /etc/kubernetes/pki/front-proxy-ca.key "${USER}"@$host:     scp /etc/kubernetes/pki/etcd/ca.crt "${USER}"@$host:etcd-ca.crt     scp /etc/kubernetes/pki/etcd/ca.key "${USER}"@$host:etcd-ca.key done`

> Caution: 只需要复制上面列表中的证书。kubeadm 将负责生成其余证书以及加入控制平面实例所需的 SAN。 如果你错误地复制了所有证书，由于缺少所需的 SAN，创建其他节点可能会失败。

11.  然后，在每个即将加入集群的控制平面节点上，你必须先运行以下脚本，然后 再运行 ​`kubeadm join`​。 该脚本会将先前复制的证书从主目录移动到 ​`/etc/kubernetes/pki`​：

`USER=ubuntu # 可定制 mkdir -p /etc/kubernetes/pki/etcd mv /home/${USER}/ca.crt /etc/kubernetes/pki/ mv /home/${USER}/ca.key /etc/kubernetes/pki/ mv /home/${USER}/sa.pub /etc/kubernetes/pki/ mv /home/${USER}/sa.key /etc/kubernetes/pki/ mv /home/${USER}/front-proxy-ca.crt /etc/kubernetes/pki/ mv /home/${USER}/front-proxy-ca.key /etc/kubernetes/pki/ mv /home/${USER}/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt mv /home/${USER}/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key`

###  7.  Kubernetes 使用kubeadm创建一个高可用etcd集群
使用 kubeadm 创建一个高可用 etcd 集群 
---------------------------

> Note:  
> 在本指南中，使用 kubeadm 作为外部 etcd 节点管理工具，请注意 kubeadm 不计划支持此类节点的证书更换或升级。 对于长期规划是使用 [etcdadm](https://github.com/kubernetes-sigs/etcdadm) 增强工具来管理这些方面。

默认情况下，kubeadm 在每个控制平面节点上运行一个本地 etcd 实例。也可以使用外部的 etcd 集群，并在不同的主机上提供 etcd 实例。

这个任务将指导你创建一个由三个成员组成的高可用外部 etcd 集群，该集群在创建过程中可被 kubeadm 使用。

在开始之前
-----

*   三个可以通过 2379 和 2380 端口相互通信的主机。本文档使用这些作为默认端口。不过，它们可以通过 kubeadm 的配置文件进行自定义。
*   每个主机必须安装 systemd 和 bash 兼容的 shell。
*   每台主机必须安装有容器运行时、kubelet 和 kubeadm。
*   一些可以用来在主机间复制文件的基础设施。例如 ssh 和 scp 就可以满足需求。

建立集群
----

一般来说，是在一个节点上生成所有证书并且只分发这些必要的文件到其它节点上。

> Note:  
> kubeadm 包含生成下述证书所需的所有必要的密码学工具；在这个例子中，不需要其他加密工具。

> Note: 下面的例子使用 IPv4 地址，但是你也可以使用 IPv6 地址配置 kubeadm、kubelet 和 etcd。一些 Kubernetes 选项支持双协议栈，但是 etcd 不支持。 

####  将 kubelet 配置为 etcd 的服务管理器。

> Note: 你必须在要运行 etcd 的所有主机上执行此操作。

由于 etcd 是首先创建的，因此你必须通过创建具有更高优先级的新文件来覆盖 kubeadm 提供的 kubelet 单元文件。

`cat << EOF > /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf [Service] ExecStart= # 将下面的 "systemd" 替换为你的容器运行时所使用的 cgroup 驱动。 # kubelet 的默认值为 "cgroupfs"。 # 如果需要的话，将 "--container-runtime-endpoint " 的值替换为一个不同的容器运行时。 ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --cgroup-driver=systemd Restart=always EOF  systemctl daemon-reload systemctl restart kubelet`

检查 kubelet 的状态以确保其处于运行状态：

`systemctl status kubelet`

####  为 kubeadm 创建配置文件。

使用以下脚本为每个将要运行 etcd 成员的主机生成一个 kubeadm 配置文件。

`# 使用你的主机 IP 替换 HOST0、HOST1 和 HOST2 的 IP 地址 export HOST0=10.0.0.6 export HOST1=10.0.0.7 export HOST2=10.0.0.8   # 使用你的主机名更新 NAME0, NAME1 和 NAME2  export NAME0="infra0"  export NAME1="infra1"  export NAME2="infra2"  # 创建临时目录来存储将被分发到其它主机上的文件 mkdir -p /tmp/${HOST0}/ /tmp/${HOST1}/ /tmp/${HOST2}/   HOSTS=(${HOST0} ${HOST1} ${HOST2})  NAMES=(${NAME0} ${NAME1} ${NAME2})   for i in "${!HOSTS[@]}"; do  HOST=${HOSTS[$i]}  NAME=${NAMES[$i]}  cat << EOF > /tmp/${HOST}/kubeadmcfg.yaml  ---  apiVersion: "kubeadm.k8s.io/v1beta3"  kind: InitConfiguration  nodeRegistration:      name: ${NAME}  localAPIEndpoint:      advertiseAddress: ${HOST}  ---  apiVersion: "kubeadm.k8s.io/v1beta3"  kind: ClusterConfiguration  etcd:      local:          serverCertSANs:          - "${HOST}"          peerCertSANs:          - "${HOST}"          extraArgs:              initial-cluster: ${NAMES[0]}=https://${HOSTS[0]}:2380,${NAMES[1]}=https://${HOSTS[1]}:2380,${NAMES[2]}=https://${HOSTS[2]}:2380              initial-cluster-state: new              name: ${NAME}              listen-peer-urls: https://${HOST}:2380              listen-client-urls: https://${HOST}:2379              advertise-client-urls: https://${HOST}:2379              initial-advertise-peer-urls: https://${HOST}:2380  EOF  done`

####  生成证书颁发机构

如果你已经拥有 CA，那么唯一的操作是复制 CA 的 ​`crt` ​和 ​`key` ​文件到 ​`etc/kubernetes/pki/etcd/ca.crt`​ 和 ​`/etc/kubernetes/pki/etcd/ca.key`​。 复制完这些文件后继续下一步，“为每个成员创建证书”。

如果你还没有 CA，则在 ​`$HOST0`​（你为 kubeadm 生成配置文件的位置）上运行此命令。

`kubeadm init phase certs etcd-ca`

这一操作创建如下两个文件

*   ​`/etc/kubernetes/pki/etcd/ca.crt` ​
*   ​`/etc/kubernetes/pki/etcd/ca.key`​

####  为每个成员创建证书

`kubeadm init phase certs etcd-server --config=/tmp/${HOST2}/kubeadmcfg.yaml kubeadm init phase certs etcd-peer --config=/tmp/${HOST2}/kubeadmcfg.yaml kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST2}/kubeadmcfg.yaml kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST2}/kubeadmcfg.yaml cp -R /etc/kubernetes/pki /tmp/${HOST2}/ # 清理不可重复使用的证书 find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete  kubeadm init phase certs etcd-server --config=/tmp/${HOST1}/kubeadmcfg.yaml kubeadm init phase certs etcd-peer --config=/tmp/${HOST1}/kubeadmcfg.yaml kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST1}/kubeadmcfg.yaml kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST1}/kubeadmcfg.yaml cp -R /etc/kubernetes/pki /tmp/${HOST1}/ find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete  kubeadm init phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yaml kubeadm init phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yaml kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yaml kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml # 不需要移动 certs 因为它们是给 HOST0 使用的  # 清理不应从此主机复制的证书 find /tmp/${HOST2} -name ca.key -type f -delete find /tmp/${HOST1} -name ca.key -type f -delete`

####  复制证书和 kubeadm 配置

证书已生成，现在必须将它们移动到对应的主机。

`USER=ubuntu HOST=${HOST1} scp -r /tmp/${HOST}/* ${USER}@${HOST}: ssh ${USER}@${HOST} USER@HOST $ sudo -Es root@HOST $ chown -R root:root pki root@HOST $ mv pki /etc/kubernetes/`

####  确保已经所有预期的文件都存在

​`$HOST0`​ 所需文件的完整列表如下：

`/tmp/${HOST0} └── kubeadmcfg.yaml --- /etc/kubernetes/pki ├── apiserver-etcd-client.crt ├── apiserver-etcd-client.key └── etcd     ├── ca.crt     ├── ca.key     ├── healthcheck-client.crt     ├── healthcheck-client.key     ├── peer.crt     ├── peer.key     ├── server.crt     └── server.key`

在 ​`$HOST1`​ 上：

`$HOME └── kubeadmcfg.yaml --- /etc/kubernetes/pki ├── apiserver-etcd-client.crt ├── apiserver-etcd-client.key └── etcd     ├── ca.crt     ├── healthcheck-client.crt     ├── healthcheck-client.key     ├── peer.crt     ├── peer.key     ├── server.crt     └── server.key`

在 ​`$HOST2`​ 上：

`$HOME └── kubeadmcfg.yaml --- /etc/kubernetes/pki ├── apiserver-etcd-client.crt ├── apiserver-etcd-client.key └── etcd     ├── ca.crt     ├── healthcheck-client.crt     ├── healthcheck-client.key     ├── peer.crt     ├── peer.key     ├── server.crt     └── server.key`

####  创建静态 Pod 清单

既然证书和配置已经就绪，是时候去创建清单了。 在每台主机上运行 ​`kubeadm` ​命令来生成 etcd 使用的静态清单。

 `root@HOST0 $ kubeadm init phase etcd local --config=/tmp/${HOST0}/kubeadmcfg.yaml  root@HOST1 $ kubeadm init phase etcd local --config=$HOME/kubeadmcfg.yaml  root@HOST2 $ kubeadm init phase etcd local --config=$HOME/kubeadmcfg.yaml`

####  可选：检查集群运行状况

`docker run --rm -it \ --net host \ -v /etc/kubernetes:/etc/kubernetes k8s.gcr.io/etcd:${ETCD_TAG} etcdctl \ --cert /etc/kubernetes/pki/etcd/peer.crt \ --key /etc/kubernetes/pki/etcd/peer.key \ --cacert /etc/kubernetes/pki/etcd/ca.crt \ --endpoints https://${HOST0}:2379 endpoint health --cluster ... https://[HOST0 IP]:2379 is healthy: successfully committed proposal: took = 16.283339ms https://[HOST1 IP]:2379 is healthy: successfully committed proposal: took = 19.44402ms https://[HOST2 IP]:2379 is healthy: successfully committed proposal: took = 35.926451ms`

*   将 ​`${ETCD_TAG}`​ 设置为你的 etcd 镜像的版本标签，例如 ​`3.4.3-0`​。 要查看 kubeadm 使用的 etcd 镜像和标签，请执行 ​`kubeadm config images list --kubernetes-version ${K8S_VERSION}`​， 例如，其中的 ​`${K8S_VERSION}`​ 可以是 ​`v1.17.0`​。
*   将 ​`${HOST0}`​ 设置为要测试的主机的 IP 地址。

###  8.  Kubernetes 使用kubeadm配置集群中的每个kubelet
使用 kubeadm 配置集群中的每个 kubelet
---------------------------

> Note: Dockershim 自 1.24 版起已从 Kubernetes 项目中删除。

FEATURE STATE: Kubernetes v1.11 \[stable\]

kubeadm CLI 工具的生命周期与 kubelet 解耦；kubelet 是一个守护程序，在 Kubernetes 集群中的每个节点上运行。 当 Kubernetes 初始化或升级时，kubeadm CLI 工具由用户执行，而 kubelet 始终在后台运行。

由于kubelet是守护程序，因此需要通过某种初始化系统或服务管理器进行维护。 当使用 DEB 或 RPM 安装 kubelet 时，配置系统去管理 kubelet。 你可以改用其他服务管理器，但需要手动地配置。

集群中涉及的所有 kubelet 的一些配置细节都必须相同， 而其他配置方面则需要基于每个 kubelet 进行设置，以适应给定机器的不同特性（例如操作系统、存储和网络）。 你可以手动地管理 kubelet 的配置，但是 kubeadm 现在提供一种 ​`KubeletConfiguration` ​API 类型 用于集中管理 kubelet 的配置。

Kubelet 配置模式 
-------------

以下各节讲述了通过使用 kubeadm 简化 kubelet 配置模式，而不是在每个节点上手动地管理 kubelet 配置。

#### 将集群级配置传播到每个 kubelet 中 

你可以通过 ​`kubeadm init`​ 和 ​`kubeadm join`​ 命令为 kubelet 提供默认值。 有趣的示例包括使用其他容器运行时或通过服务器设置不同的默认子网。

如果你想使用子网 ​`10.96.0.0/12`​ 作为服务的默认网段，你可以给 kubeadm 传递 ​`--service-cidr`​ 参数：

`kubeadm init --service-cidr 10.96.0.0/12`

现在，可以从该子网分配服务的虚拟 IP。 你还需要通过 kubelet 使用 ​`--cluster-dns`​ 标志设置 DNS 地址。 在集群中的每个管理器和节点上的 kubelet 的设置需要相同。 kubelet 提供了一个版本化的结构化 API 对象，该对象可以配置 kubelet 中的大多数参数，并将此配置推送到集群中正在运行的每个 kubelet 上。 此对象被称为 ​`KubeletConfiguration`​。 ​`KubeletConfiguration` ​允许用户指定标志，例如用骆峰值代表集群的 DNS IP 地址，如下所示：

`apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration clusterDNS: - 10.96.0.10`

#### 提供特定于某实例的配置细节 

由于硬件、操作系统、网络或者其他主机特定参数的差异。某些主机需要特定的 kubelet 配置。 以下列表提供了一些示例。

*   由 kubelet 配置标志 ​`--resolv-conf`​ 指定的 DNS 解析文件的路径在操作系统之间可能有所不同， 它取决于你是否使用 ​`systemd-resolved`​。 如果此路径错误，则在其 kubelet 配置错误的节点上 DNS 解析也将失败。
*   除非你使用云驱动，否则默认情况下 Node API 对象的 ​`.metadata.name`​ 会被设置为计算机的主机名。 如果你需要指定一个与机器的主机名不同的节点名称，你可以使用 ​`--hostname-override`​ 标志覆盖默认值。
*   当前，kubelet 无法自动检测容器运行时使用的 cgroup 驱动程序， 但是值 ​`--cgroup-driver`​ 必须与容器运行时使用的 cgroup 驱动程序匹配，以确保 kubelet 的健康运行状况。
*   要指定容器运行时，你必须用 ​`--container-runtime-endpoint=<path>`​ 标志来指定端点。

你可以在服务管理器（例如 systemd）中设定某个 kubelet 的配置来指定这些参数。

使用 kubeadm 配置 kubelet 
----------------------

如果自定义的 ​`KubeletConfiguration` ​API 对象使用像 ​`kubeadm ... --config some-config-file.yaml`​ 这样的配置文件进行传递，则可以配置 kubeadm 启动的 kubelet。

通过调用 ​`kubeadm config print init-defaults --component-configs KubeletConfiguration`​， 你可以看到此结构中的所有默认值。

#### 使用 kubeadm init 时的工作流程 

当调用 ​`kubeadm init`​ 时，kubelet 的配置会被写入磁盘 ​`/var/lib/kubelet/config.yaml`​， 并上传到集群 ​`kube-system`​ 命名空间的 ​`kubelet-config`​ ConfigMap。 kubelet 配置信息也被写入 ​`/etc/kubernetes/kubelet.conf`​，其中包含集群内所有 kubelet 的基线配置。 此配置文件指向允许 kubelet 与 API 服务器通信的客户端证书。 这解决了将集群级配置传播到每个 kubelet 的需求。

针对为特定实例提供配置细节的第二种模式， kubeadm 的解决方法是将环境文件写入 ​`/var/lib/kubelet/kubeadm-flags.env`​，其中包含了一个标志列表， 当 kubelet 启动时，该标志列表会传递给 kubelet 标志在文件中的显示方式如下：

`KUBELET_KUBEADM_ARGS="--flag1=value1 --flag2=value2 ..."`

除了启动 kubelet 时所使用的标志外，该文件还包含动态参数，例如 cgroup 驱动程序以及是否使用其他容器运行时套接字（​`--cri-socket`​）。

将这两个文件编组到磁盘后，如果使用 systemd，则 kubeadm 尝试运行以下两个命令：

`systemctl daemon-reload && systemctl restart kubelet`

如果重新加载和重新启动成功，则正常的 ​`kubeadm init`​ 工作流程将继续。

#### 使用 kubeadm join 时的工作流程 

当运行 ​`kubeadm join`​ 时，kubeadm 使用 Bootstrap Token 证书执行 TLS 引导，该引导会获取一份证书， 该证书需要下载 ​`kubelet-config`​ ConfigMap 并把它写入 ​`/var/lib/kubelet/config.yaml`​ 中。 动态环境文件的生成方式恰好与 ​`kubeadm init`​ 完全相同。

接下来，​`kubeadm` ​运行以下两个命令将新配置加载到 kubelet 中：

`systemctl daemon-reload && systemctl restart kubelet`

在 kubelet 加载新配置后，kubeadm 将写入 ​`/etc/kubernetes/bootstrap-kubelet.conf`​ KubeConfig 文件中， 该文件包含 CA 证书和引导程序令牌。 kubelet 使用这些证书执行 TLS 引导程序并获取唯一的凭据，该凭据被存储在 ​`/etc/kubernetes/kubelet.conf`​ 中。

当 ​`/etc/kubernetes/kubelet.conf`​ 文件被写入后，kubelet 就完成了 TLS 引导过程。 Kubeadm 在完成 TLS 引导过程后将删除 ​`/etc/kubernetes/bootstrap-kubelet.conf`​ 文件。

kubelet 的 systemd drop-in 文件 
-----------------------------

​`kubeadm` ​中附带了有关系统如何运行 kubelet 的 systemd 配置文件。 请注意 kubeadm CLI 命令不会修改此文件。

通过 ​`kubeadm` ​[DEB 包](https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf) 或者 [RPM 包](https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/rpm/kubeadm/10-kubeadm.conf) 安装的配置文件被写入 ​`/etc/systemd/system/kubelet.service.d/10-kubeadm.conf`​ 并由 systemd 使用。 它对原来的 [RPM 版本 ​`kubelet.service`](https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/rpm/kubelet/kubelet.service)​ 或者 [DEB 版本 ​`kubelet.service`](https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service)​ 作了增强：

`[Service] Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf" Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml" # 这是 "kubeadm init" 和 "kubeadm join" 运行时生成的文件，动态地填充 KUBELET_KUBEADM_ARGS 变量 EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env # 这是一个文件，用户在不得已下可以将其用作替代 kubelet args。 # 用户最好使用 .NodeRegistration.KubeletExtraArgs 对象在配置文件中替代。 # KUBELET_EXTRA_ARGS 应该从此文件中获取。 EnvironmentFile=-/etc/default/kubelet ExecStart= ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS`

此文件指定由 kubeadm 为 kubelet 管理的所有文件的默认位置。

*   用于 TLS 引导程序的 KubeConfig 文件为 ​`/etc/kubernetes/bootstrap-kubelet.conf`​， 但仅当 ​`/etc/kubernetes/kubelet.conf`​ 不存在时才能使用。
*   具有唯一 kubelet 标识的 KubeConfig 文件为 ​`/etc/kubernetes/kubelet.conf`​。
*   包含 kubelet 的组件配置的文件为 ​`/var/lib/kubelet/config.yaml`​。
*   包含的动态环境的文件 ​`KUBELET_KUBEADM_ARGS`​ 是来源于 ​`/var/lib/kubelet/kubeadm-flags.env`​。
*   包含用户指定标志替代的文件 ​`KUBELET_EXTRA_ARGS` ​是来源于 ​`/etc/default/kubelet`​（对于 DEB），或者 ​`/etc/sysconfig/kubelet`​（对于 RPM）。 ​`KUBELET_EXTRA_ARGS` ​在标志链中排在最后，并且在设置冲突时具有最高优先级。

Kubernetes 可执行文件和软件包内容 
-----------------------

Kubernetes 版本对应的 DEB 和 RPM 软件包是：

软件包名称

描述

`kubeadm`

给 kubelet 安装 `/usr/bin/kubeadm` CLI 工具和 kubelet 的 systemd drop-in 文件。

`kubelet`

安装 `/usr/bin/kubelet` 可执行文件。

`kubectl`

安装 `/usr/bin/kubectl` 可执行文件。

`cri-tools`

从 [cri-tools git 仓库](https://github.com/kubernetes-sigs/cri-tools)中安装 `/usr/bin/crictl` 可执行文件。

`kubernetes-cni`

从 [plugins git 仓库](https://github.com/containernetworking/plugins)中安装 `/opt/cni/bin` 可执行文件。

###  9.  Kubernetes 使用kubeadm支持双协议栈
使用 kubeadm 支持双协议栈
-----------------

FEATURE STATE: Kubernetes v1.23 \[stable\]

你的集群包含双协议栈组网支持， 这意味着集群网络允许你在两种地址族间任选其一。在集群中，控制面可以为同一个 Pod 或者 Service 同时赋予 IPv4 和 IPv6 地址。

在开始之前
-----

你需要已经遵从安装 kubeadm 中所给的步骤安装了 kubeadm 工具。

针对你要作为节点使用的每台服务器， 确保其允许 IPv6 转发。在 Linux 节点上，你可以通过以 root 用户在每台服务器上运行 ​`sysctl -w net.ipv6.conf.all.forwarding=1`​ 来完成设置。

你需要一个可以使用的 IPv4 和 IPv6 地址范围。集群操作人员通常为 IPv4 使用 私有地址范围。对于 IPv6，集群操作人员通常会基于分配给该操作人员的地址范围， 从 ​`2000::/3`​ 中选择一个全局的单播地址块。你不需要将集群的 IP 地址范围路由 到公众互联网。

> Note:  
> 如果你在使用 ​`kubeadm upgrade`​ 命令升级现有的集群，​`kubeadm` ​不允许更改 Pod 的 IP 地址范围（“集群 CIDR”），也不允许更改集群的服务地址范围（“Service CIDR”）。

#### 创建双协议栈集群 

要使用 ​`kubeadm init`​ 创建一个双协议栈集群，你可以传递与下面的例子类似的命令行参数：

`# 这里的地址范围仅作示例使用 kubeadm init --pod-network-cidr=10.244.0.0/16,2001:db8:42:0::/56 --service-cidr=10.96.0.0/16,2001:db8:42:1::/112`

为了更便于理解，参看下面的名为 ​`kubeadm-config.yaml`​ 的 kubeadm 配置文件， 该文件用于双协议栈控制面的主控制节点。

`--- apiVersion: kubeadm.k8s.io/v1beta3 kind: ClusterConfiguration networking:   podSubnet: 10.244.0.0/16,2001:db8:42:0::/56   serviceSubnet: 10.96.0.0/16,2001:db8:42:1::/112 --- apiVersion: kubeadm.k8s.io/v1beta3 kind: InitConfiguration localAPIEndpoint:   advertiseAddress: "10.100.0.1"   bindPort: 6443 nodeRegistration:   kubeletExtraArgs:     node-ip: 10.100.0.2,fd00:1:2:3::2`

InitConfiguration 中的 ​`advertiseAddress` ​给出 API 服务器将公告自身要监听的 IP 地址。​`advertiseAddress` ​的取值与 ​`kubeadm init`​ 的标志 ​`--apiserver-advertise-address`​ 的取值相同。

运行 kubeadm 来实例化双协议栈控制面节点：

`kubeadm init --config=kubeadm-config.yaml`

kube-controller-manager 标志 ​`--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6`​ 是使用默认值来设置的。

> Note:  
> 标志 ​`--apiserver-advertise-address`​ 不支持双协议栈。

#### 向双协议栈集群添加节点 

在添加节点之前，请确保该节点具有 IPv6 可路由的网络接口并且启用了 IPv6 转发。

下面的名为 ​`kubeadm-config.yaml`​ 的 kubeadm 配置文件 示例用于向集群中添加工作节点。

`apiVersion: kubeadm.k8s.io/v1beta3 kind: JoinConfiguration discovery:   bootstrapToken:     apiServerEndpoint: 10.100.0.1:6443     token: "clvldh.vjjwg16ucnhp94qr"     caCertHashes:     - "sha256:a4863cde706cfc580a439f842cc65d5ef112b7b2be31628513a9881cf0d9fe0e"     # 请更改上面的认证信息，使之与你的集群中实际使用的令牌和 CA 证书匹配 nodeRegistration:   kubeletExtraArgs:     node-ip: 10.100.0.3,fd00:1:2:3::3`

下面的名为 ​`kubeadm-config.yaml`​ 的 kubeadm 配置文件 示例用于向集群中添加另一个控制面节点。

`apiVersion: kubeadm.k8s.io/v1beta3 kind: JoinConfiguration controlPlane:   localAPIEndpoint:     advertiseAddress: "10.100.0.2"     bindPort: 6443 discovery:   bootstrapToken:     apiServerEndpoint: 10.100.0.1:6443     token: "clvldh.vjjwg16ucnhp94qr"     caCertHashes:     - "sha256:a4863cde706cfc580a439f842cc65d5ef112b7b2be31628513a9881cf0d9fe0e"     # 请更改上面的认证信息，使之与你的集群中实际使用的令牌和 CA 证书匹配 nodeRegistration:   kubeletExtraArgs:     node-ip: 10.100.0.4,fd00:1:2:3::4`

JoinConfiguration.controlPlane 中的 ​`advertiseAddress` ​设定 API 服务器将公告自身要监听的 IP 地址。​`advertiseAddress` ​的取值与 ​`kubeadm join`​ 的标志 ​`--apiserver-advertise-address`​ 的取值相同。

`kubeadm join --config=kubeadm-config.yaml`

#### 创建单协议栈集群 

> Note:  
> 双协议栈支持并不意味着你需要使用双协议栈来寻址。 你可以部署一个启用了双协议栈联网特性的单协议栈集群。

为了更便于理解，参看下面的名为 ​`kubeadm-config.yaml`​ 的 kubeadm 配置文件示例， 该文件用于单协议栈控制面节点。

`apiVersion: kubeadm.k8s.io/v1beta3 kind: ClusterConfiguration networking:   podSubnet: 10.244.0.0/16   serviceSubnet: 10.96.0.0/16`

###  10.  Kubernetes 使用Kops安装Kubernetes
使用 Kops 安装 Kubernetes
---------------------

本篇快速入门介绍了如何在 AWS 上轻松安装 Kubernetes 集群。 本篇使用了一个名为 ​`[kops](https://github.com/kubernetes/kops)`​ 的工具。

kops 是一个自动化的制备系统：

*   全自动安装流程
*   使用 DNS 识别集群
*   自我修复：一切都在自动扩缩组中运行
*   支持多种操作系统（如 Debian、Ubuntu 16.04、CentOS、RHEL、Amazon Linux 和 CoreOS） - 参考 [images.md](https://github.com/kubernetes/kops/blob/master/docs/operations/images.md)
*   支持高可用
*   可以直接提供或者生成 terraform 清单 - 参考 [terraform.md](https://github.com/kubernetes/kops/blob/master/docs/terraform.md)

在开始之前
-----

*   你必须安装 kubectl。
*   你必须安装 [kops](https://github.com/kubernetes/kops target=) 到 64 位的（AMD64 和 Intel 64）设备架构上。
*   你必须拥有一个 [AWS 账户](https://docs.aws.amazon.com/polly/latest/dg/setting-up.html)， 生成 [IAM 秘钥](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html target=) 并 [配置](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html target=) 该秘钥。IAM 用户需要[足够的权限许可](https://github.com/kubernetes/kops/blob/master/docs/getting_started/aws.md target=)。

创建集群
----

#### (1/5) 安装 kops

##### 安装 

从[下载页面](https://github.com/kubernetes/kops/releases)下载 kops （从源代码构建也很方便）：

*   macOS

使用下面的命令下载最新发布版本：

`curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-darwin-amd64`

要下载特定版本，使用特定的 kops 版本替换下面命令中的部分：

`$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)`

例如，要下载 kops v1.20.0，输入：

`curl -LO https://github.com/kubernetes/kops/releases/download/v1.20.0/kops-darwin-amd64`

令 kops 二进制文件可执行：

`chmod +x kops-darwin-amd64`

将 kops 二进制文件移到你的 PATH 下：

`sudo mv kops-darwin-amd64 /usr/local/bin/kops`

你也可以使用 [Homebrew](https://brew.sh/) 安装 kops：

`brew update && brew install kops`

*   Linux

使用命令下载最新发布版本：

`curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64`

要下载 kops 的特定版本，用特定的 kops 版本替换下面命令中的部分：

`$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)`

例如，要下载 kops v1.20 版本，输入：

`curl -LO https://github.com/kubernetes/kops/releases/download/v1.20.0/kops-linux-amd64`

令 kops 二进制文件可执行：

`chmod +x kops-linux-amd64`

将 kops 二进制文件移到 PATH 下：

`sudo mv kops-linux-amd64 /usr/local/bin/kops`

你也可以使用 [Homebrew](https://docs.brew.sh/Homebrew-on-Linux) 来安装 kops：

`brew update && brew install kops`

#### (2/5) 为你的集群创建一个 route53 域名

kops 在集群内部和外部都使用 DNS 进行发现操作，这样你可以从客户端访问 kubernetes API 服务器。

kops 对集群名称有明显的要求：它应该是有效的 DNS 名称。这样一来，你就不会再使集群混乱， 可以与同事明确共享集群，并且无需依赖记住 IP 地址即可访问集群。

你可以，或许应该使用子域名来划分集群。作为示例，我们将使用域名 ​`useast1.dev.example.com`​。 这样，API 服务器端点域名将为 ​`api.useast1.dev.example.com`​。

Route53 托管区域可以服务子域名。你的托管区域可能是 ​`useast1.dev.example.com`​，还有 ​`dev.example.com`​ 甚至 ​`example.com`​。 kops 可以与以上任何一种配合使用，因此通常你出于组织原因选择不同的托管区域。 例如，允许你在 ​`dev.example.com`​ 下创建记录，但不能在 ​`example.com`​ 下创建记录。

假设你使用 ​`dev.example.com`​ 作为托管区域。你可以使用 [正常流程](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/CreatingNewSubdomain.html) 或者使用诸如 ​`aws route53 create-hosted-zone --name dev.example.com --caller-reference 1`​ 之类的命令来创建该托管区域。

然后，你必须在父域名中设置你的 DNS 记录，以便该域名中的记录可以被解析。 在这里，你将在 ​`example.com`​ 中为 dev 创建 DNS 记录。 如果它是根域名，则可以在域名注册机构配置 DNS 记录。 例如，你需要在购买 ​`example.com`​ 的地方配置 ​`example.com`​。

检查你的 route53 域已经被正确设置（这是导致问题的最常见原因！）。 如果你安装了 dig 工具，则可以通过运行以下步骤再次检查集群是否配置正确：

`dig DNS dev.example.com`

你应该看到 Route53 分配了你的托管区域的 4 条 DNS 记录。

#### (3/5) 创建一个 S3 存储桶来存储集群状态

kops 使你即使在安装后也可以管理集群。为此，它必须跟踪已创建的集群及其配置、所使用的密钥等。 此信息存储在 S3 存储桶中。S3 权限用于控制对存储桶的访问。

多个集群可以使用同一 S3 存储桶，并且你可以在管理同一集群的同事之间共享一个 S3 存储桶 - 这比传递 kubecfg 文件容易得多。 但是有权访问 S3 存储桶的任何人都将拥有对所有集群的管理访问权限， 因此你不想在运营团队之外共享它。

因此，通常每个运维团队都有一个 S3 存储桶（而且名称通常对应于上面托管区域的名称！）

在我们的示例中，我们选择 ​`dev.example.com`​ 作为托管区域，因此我们选择 ​`clusters.dev.example.com`​ 作为 S3 存储桶名称。

*   导出 ​`AWS_PROFILE`​ 文件（如果你需要选择一个配置文件用来使 AWS CLI 正常工作）
*   使用 ​`aws s3 mb s3://clusters.dev.example.com`​ 创建 S3 存储桶
*   你可以进行 ​`export KOPS_STATE_STORE=s3://clusters.dev.example.com`​ 操作， 然后 kops 将默认使用此位置。 我们建议将其放入你的 bash profile 文件或类似文件中。

#### (4/5) 建立你的集群配置

运行 ​`kops create cluster`​ 以创建你的集群配置：

​`kops create cluster --zones=us-east-1c useast1.dev.example.com` ​

kops 将为你的集群创建配置。请注意，它\_仅\_创建配置，实际上并没有创建云资源 - 你将在下一步中使用 kops update cluster 进行配置。 这使你有机会查看配置或进行更改。

它打印出可用于进一步探索的命令：

*   使用以下命令列出集群：​`kops get cluster` ​
*   使用以下命令编辑该集群：​`kops edit cluster useast1.dev.example.com` ​
*   使用以下命令编辑你的节点实例组：​`kops edit ig --name = useast1.dev.example.com nodes` ​
*   使用以下命令编辑你的主实例组：​`kops edit ig --name = useast1.dev.example.com master-us-east-1c`​

如果这是你第一次使用 kops，请花几分钟尝试一下！ 实例组是一组实例，将被注册为 kubernetes 节点。 在 AWS 上，这是通过 auto-scaling-groups 实现的。你可以有多个实例组。 例如，如果你想要的是混合实例和按需实例的节点，或者 GPU 和非 GPU 实例。

#### (5/5) 在 AWS 中创建集群

运行 "kops update cluster" 以在 AWS 中创建集群：

`kops update cluster useast1.dev.example.com --yes`

这需要几秒钟的时间才能运行，但实际上集群可能需要几分钟才能准备就绪。 每当更改集群配置时，都会使用 ​`kops update cluster`​ 工具。 它将对配置进行的更改应用于你的集群 - 根据需要重新配置 AWS 或者 kubernetes。

例如，在你运行 ​`kops edit ig nodes`​ 之后，然后运行 ​`kops update cluster --yes`​ 应用你的配置，有时你还必须运行 ​`kops rolling-update cluster`​ 立即回滚更新配置。

如果没有 ​`--yes` ​参数，​`kops update cluster`​ 操作将向你显示其操作的预览效果。这对于生产集群很方便！

清理
--

*   删除集群：​`kops delete cluster useast1.dev.example.com --yes`​

###  11.  Kubernetes 使用Kubespray安装Kubernetes
使用 Kubespray 安装 Kubernetes
--------------------------

此快速入门有助于使用 [Kubespray](https://github.com/kubernetes-sigs/kubespray) 安装在 GCE、Azure、OpenStack、AWS、vSphere、Packet（裸机）、Oracle Cloud Infrastructure（实验性）或 Baremetal 上托管的 Kubernetes 集群。

Kubespray 是一个由 [Ansible](https://docs.ansible.com/) playbooks、 [清单（inventory）](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/ansible.md)、 制备工具和通用 OS/Kubernetes 集群配置管理任务的领域知识组成的。 Kubespray 提供：

*   高可用性集群
*   可组合属性
*   支持大多数流行的 Linux 发行版

*   Ubuntu 16.04、18.04、20.04
*   CentOS / RHEL / Oracle Linux 7、8
*   Debian Buster、Jessie、Stretch、Wheezy
*   Fedora 31、32
*   Fedora CoreOS
*   openSUSE Leap 15
*   Kinvolk 的 Flatcar Container Linux

*   持续集成测试

要选择最适合你的用例的工具，请阅读 ​`kubeadm` ​和 ​`kops` ​之间的 [这份比较](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/comparisons.md)

创建集群
----

#### （1/5）满足下层设施要求

按以下[要求](https://github.com/kubernetes-sigs/kubespray target=)来配置服务器：

*   在将运行 Ansible 命令的计算机上安装 Ansible v2.9 和 python-netaddr
*   运行 Ansible Playbook 需要 Jinja 2.11（或更高版本）
*   目标服务器必须有权访问 Internet 才能拉取 Docker 镜像。否则， 需要其他配置（[请参见离线环境](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/offline-environment.md)）
*   目标服务器配置为允许 IPv4 转发
*   你的 SSH 密钥必须复制到部署集群的所有服务器中
*   防火墙不是由 kubespray 管理的。你需要根据需求设置适当的规则策略。为了避免部署过程中出现问题，可以禁用防火墙。
*   如果从非 root 用户帐户运行 kubespray，则应在目标服务器中配置正确的特权升级方法 并指定 ​`ansible_become` ​标志或命令参数 ​`--become`​ 或 ​`-b`​

Kubespray 提供以下实用程序来帮助你设置环境：

*   为以下云驱动提供的 [Terraform](https://www.terraform.io/) 脚本：
*   [AWS](https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/aws)
*   [OpenStack](http://sitebeskuethree/contrigetbernform/contribeskubernform/contribeskupernform/https/sitebesku/master/)
*   Packet

#### （2/5）编写清单文件

设置服务器后，请创建一个 [Ansible 的清单文件](https://docs.ansible.com/ansible/latest/network/getting_started/first_inventory.html)。 你可以手动执行此操作，也可以通过动态清单脚本执行此操作。有关更多信息，请参阅 “[建立你自己的清单](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md target=)”。

#### （3/5）规划集群部署

Kubespray 能够自定义部署的许多方面：

*   选择部署模式： kubeadm 或非 kubeadm
*   CNI（网络）插件
*   DNS 配置
*   控制平面的选择：本机/可执行文件或容器化
*   组件版本
*   Calico 路由反射器
*   组件运行时选项

*   Docker
*   containerd
*   CRI-O

*   证书生成方式

可以修改[变量文件](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html) 以进行 Kubespray 定制。 如果你刚刚开始使用 Kubespray，请考虑使用 Kubespray 默认设置来部署你的集群 并探索 Kubernetes 。

#### （4/5）部署集群

接下来，部署你的集群：

使用 [ansible-playbook](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md target=) 进行集群部署。

`ansible-playbook -i your/inventory/inventory.ini cluster.yml -b -v \   --private-key=~/.ssh/private_key`

大型部署（超过 100 个节点）可能需要 [特定的调整](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/large-deployments.md)， 以获得最佳效果。

#### （5/5）验证部署

Kubespray 提供了一种使用 [Netchecker](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/netcheck.md) 验证 Pod 间连接和 DNS 解析的方法。 Netchecker 确保 netchecker-agents Pods 可以解析 DNS 请求， 并在默认命名空间内对每个请求执行 ping 操作。 这些 Pod 模仿其他工作负载类似的行为，并用作集群运行状况指示器。

集群操作
----

Kubespray 提供了其他 Playbooks 来管理集群： scale 和 upgrade。

#### 扩展集群

你可以通过运行 scale playbook 向集群中添加工作节点。有关更多信息， 请参见 “[添加节点](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md target=)”。 你可以通过运行 remove-node playbook 来从集群中删除工作节点。有关更多信息， 请参见 “[删除节点](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md target=)”。

#### 升级集群

你可以通过运行 upgrade-cluster Playbook 来升级集群。有关更多信息，请参见 “[升级](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/upgrades.md)”。

清理
--

你可以通过 [reset](https://github.com/kubernetes-sigs/kubespray/blob/master/reset.yml) Playbook 重置节点并清除所有与 Kubespray 一起安装的组件。

> Caution: 运行 reset playbook 时，请确保不要意外地将生产集群作为目标！  

反馈
--

*   Slack 频道：[#kubespray](https://kubernetes.slack.com/?redir=%2Fmessages%2Fkubespray%2F) （你可以在[此处](https://slack.k8s.io/)获得邀请）
*   [GitHub 问题](https://github.com/kubernetes-sigs/kubespray/issues)

##  12.  Kubernetes 对Windows的支持
Kubernetes 对 Windows 的支持
------------------------

在很多组织中，其服务和应用的很大比例是 Windows 应用。 [Windows 容器](https://docs.microsoft.com/zh-cn/virtualization/windowscontainers/)提供了一种对进程和包依赖关系 进行封装的现代方式，这使得用户更容易采用 DevOps 实践，令 Windows 应用同样遵从 云原生模式。 Kubernetes 已经成为事实上的标准容器编排器，Kubernetes 1.14 发行版本中包含了将 Windows 容器调度到 Kubernetes 集群中 Windows 节点上的生产级支持，从而使得巨大 的 Windows 应用生态圈能够充分利用 Kubernetes 的能力。 对于同时投入基于 Windows 应用和 Linux 应用的组织而言，他们不必寻找不同的编排系统 来管理其工作负载，其跨部署的运维效率得以大幅提升，而不必关心所用操作系统。

kubernetes 中的 Windows 容器 
-------------------------

若要在 Kubernetes 中启用对 Windows 容器的编排，可以在现有的 Linux 集群中 包含 Windows 节点。在 Kubernetes 上调度 Pods 中的 Windows 容器与调用基于 Linux 的容器类似。

为了运行 Windows 容器，你的 Kubernetes 集群必须包含多个操作系统，控制面 节点运行 Linux，工作节点则可以根据负载需要运行 Windows 或 Linux。 Windows Server 2019 是唯一被支持的 Windows 操作系统，在 Windows 上启用 [Kubernetes 节点](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/architecture.md target=) 支持（包括 kubelet, [容器运行时](https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/containerd)、 以及 kube-proxy）。关于 Windows 发行版渠道的详细讨论，可参见  [Microsoft 文档](https://docs.microsoft.com/en-us/windows-server/get-started/servicing-channels-comparison)。

> Note: Kubernetes 控制面，包括主控组件， 继续在 Linux 上运行。 目前没有支持完全是 Windows 节点的 Kubernetes 集群的计划。

> Note: 在本文中，当我们讨论 Windows 容器时，我们所指的是具有进程隔离能力的 Windows 容器。具有 [Hyper-V 隔离能力](https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/hyperv-container) 的 Windows 容器计划在将来发行版本中推出。

支持的功能与局限性 
----------

### 支持的功能 

#### Windows 操作系统版本支持 

参考下面的表格，了解 Kubernetes 中支持的 Windows 操作系统。 同一个异构的 Kubernetes 集群中可以同时包含 Windows 和 Linux 工作节点。 Windows 容器仅能调度到 Windows 节点，Linux 容器则只能调度到 Linux 节点。

Kubernetes 版本

Windows Server LTSC 版本

Windows Server SAC 版本

_Kubernetes v1.20_

Windows Server 2019

Windows Server ver 1909, Windows Server ver 2004

_Kubernetes v1.21_

Windows Server 2019

Windows Server ver 2004, Windows Server ver 20H2

_Kubernetes v1.22_

Windows Server 2019

Windows Server ver 2004, Windows Server ver 20H2

关于不同的 Windows Server 版本的服务渠道，包括其支持模式等相关信息可以在 [Windows Server servicing channels](https://docs.microsoft.com/en-us/windows-server/get-started/servicing-channels-comparison) 找到。

我们并不指望所有 Windows 客户都为其应用频繁地更新操作系统。 对应用的更新是向集群中引入新代码的根本原因。 对于想要更新运行于 Kubernetes 之上的容器中操作系统的客户，我们会在添加对新 操作系统版本的支持时提供指南和分步的操作指令。 该指南会包含与集群节点一起来升级用户应用的建议升级步骤。 Windows 节点遵从 Kubernetes 版本偏差策略（节点到控制面的 版本控制），与 Linux 节点的现行策略相同。

Windows Server 主机操作系统会受 [Windows Server](https://www.microsoft.com/en-us/windows-server/pricing) 授权策略控制。Windows 容器镜像则遵从 [Windows 容器的补充授权条款](https://docs.microsoft.com/en-us/virtualization/windowscontainers/images-eula) 约定。

带进程隔离的 Windows 容器受一些严格的兼容性规则约束， [其中宿主 OS 版本必须与容器基准镜像的 OS 版本相同](https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/version-compatibility?tabs=windows-server-2022%2Cwindows-11-21H2)。 一旦我们在 Kubernetes 中支持带 Hyper-V 隔离的 Windows 容器， 这一约束和兼容性规则也会发生改变。

#### Pause 镜像 

Kubernetes 维护着一个多体系结构镜像，其中包括对 Windows 的支持。 对于 Kubernetes v1.22，推荐的 pause 镜像是 ​`k8s.gcr.io/pause:3.5`​。 [源代码](https://github.com/kubernetes/kubernetes/tree/master/build/pause)可在 GitHub 上找到。

Microsoft 维护了一个支持 Linux 和 Windows amd64 的多体系结构镜像： ​`mcr.microsoft.com/oss/kubernetes/pause:3.5`​。 此镜像与 Kubernetes 维护的镜像是从同一来源构建，但所有 Windows 二进制文件 均由 Microsoft [签名](https://docs.microsoft.com/en-us/windows-hardware/drivers/install/authenticode)。 当生产环境需要被签名的二进制文件时，建议使用 Microsoft 维护的镜像。

#### 计算 

从 API 和 kubectl 的角度，Windows 容器的表现在很大程度上与基于 Linux 的容器 是相同的。不过也有一些与关键功能相关的差别值得注意，这些差别列举于 局限性小节中。

关键性的 Kubernetes 元素在 Windows 下与其在 Linux 下工作方式相同。我们在本节中 讨论一些关键性的负载支撑组件及其在 Windows 中的映射。

*   Pods

Pod 是 Kubernetes 中最基本的构造模块，是 Kubernetes 对象模型中你可以创建或部署的 最小、最简单元。你不可以在同一 Pod 中部署 Windows 和 Linux 容器。 Pod 中的所有容器都会被调度到同一节点（Node），而每个节点代表的是一种特定的平台 和体系结构。Windows 容器支持 Pod 的以下能力、属性和事件：

*   在带进程隔离和卷共享支持的 Pod 中运行一个或多个容器
*   Pod 状态字段
*   就绪态（Readiness）和活跃性（Liveness）探针
*   postStart 和 preStop 容器生命周期事件
*   ConfigMap、Secrets：用作环境变量或卷
*   emptyDir 卷
*   从宿主系统挂载命名管道
*   资源限制

*   控制器（Controllers）

Kubernetes 控制器处理 Pod 的期望状态。Windows 容器支持以下负载控制器：

*   ReplicaSet
*   ReplicationController
*   Deployment
*   StatefulSet
*   DaemonSet
*   Job
*   CronJob

*   服务（Services）

Kubernetes Service 是一种抽象对象，用来定义 Pod 的一个逻辑集合及用来访问这些 Pod 的策略。Service 有时也称作微服务（Micro-service）。你可以使用服务来实现 跨操作系统的连接。在 Windows 系统中，服务可以使用下面的类型、属性和能力：

*   Service 环境变量
*   NodePort
*   ClusterIP
*   LoadBalancer
*   ExternalName
*   无头（Headless）服务

Pods、控制器和服务是在 Kubernetes 上管理 Windows 负载的关键元素。 不过，在一个动态的云原生环境中，这些元素本身还不足以用来正确管理 Windows 负载的生命周期。我们为此添加了如下功能特性：

*   Pod 和容器的度量（Metrics）
*   对水平 Pod 自动扩展的支持
*   对 kubectl exec 命令的支持
*   资源配额
*   调度器抢占

#### 容器运行时 

##### Docker EE

FEATURE STATE: Kubernetes v1.14 \[stable\]

Docker EE-basic 19.03+ 是建议所有 Windows Server 版本采用的容器运行时。 该容器运行时能够与 kubelet 中的 dockershim 代码协同工作。

##### CRI-ContainerD

FEATURE STATE: Kubernetes v1.20 \[stable\]

ContainerD 1.4.0+ 也可作为 Windows Kubernetes 节点上的容器运行时。

#### 持久性存储 

使用 Kubernetes 卷，对数据持久性和 Pod 卷 共享有需求的复杂应用也可以部署到 Kubernetes 上。 管理与特定存储后端或协议相关的持久卷时，相关的操作包括：对卷的配备（Provisioning）、 去配（De-provisioning）和调整大小，将卷挂接到 Kubernetes 节点或从节点上解除挂接， 将卷挂载到需要持久数据的 Pod 中的某容器或从容器上卸载。 负责实现为特定存储后端或协议实现卷管理动作的代码以 Kubernetes 卷 插件的形式发布。 Windows 支持以下大类的 Kubernetes 卷插件：

##### 树内卷插件 

与树内卷插件（In-Tree Volume Plugin）相关的代码都作为核心 Kubernetes 代码基 的一部分发布。树内卷插件的部署不需要安装额外的脚本，也不需要额外部署独立的 容器化插件组件。这些插件可以处理：对应存储后端上存储卷的配备、去配和尺寸更改， 将卷挂接到 Kubernetes 或从其上解挂，以及将卷挂载到 Pod 中各个容器上或从其上 卸载。以下树内插件支持 Windows 节点：

*   ​`awsElasticBlockStore` ​
*   ​`azureDisk` ​
*   ​`azureFile` ​
*   ​`gcePersistentDisk` ​
*   ​`vsphereVolume`​

##### FlexVolume 插件 

与 FlexVolume 插件相关的代码是作为 树外（Out-of-tree）脚本或可执行文件来发布的，因此需要在宿主系统上直接部署。 FlexVolume 插件处理将卷挂接到 Kubernetes 节点或从其上解挂、将卷挂载到 Pod 中 各个容器上或从其上卸载等操作。对于与 FlexVolume 插件相关联的持久卷的配备和 去配操作，可以通过外部的配置程序来处理。这类配置程序通常与 FlexVolume 插件 相分离。下面的 FlexVolume  [插件](https://github.com/Microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows) 可以以 PowerShell 脚本的形式部署到宿主系统上，支持 Windows 节点：

*   [SMB](https://github.com/microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows/plugins/microsoft.com~smb.cmd)
*   [iSCSI](https://github.com/microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows/plugins/microsoft.com~iscsi.cmd)

##### CSI 插件 

FEATURE STATE: Kubernetes v1.22 \[stable\]

与 CSI 插件相关联的代码作为 树外脚本和可执行文件来发布且通常发布为容器镜像形式，并使用 DaemonSet 和 StatefulSet 这类标准的 Kubernetes 构造体来部署。 CSI 插件处理 Kubernetes 中的很多卷管理操作：对卷的配备、去配和调整大小， 将卷挂接到 Kubernetes 节点或从节点上解除挂接，将卷挂载到需要持久数据的 Pod 中的某容器或从容器上卸载，使用快照和克隆来备份或恢复持久数据。

来支持；csi-proxy 是一个社区管理的、独立的可执行文件，需要预安装在每个 Windows 节点之上。请参考你要部署的 CSI 插件的部署指南以进一步了解其细节。

CSI 插件与执行本地存储操作的 CSI 节点插件通信。 在 Windows 节点上，CSI 节点插件通常调用处理本地存储操作的 csi-proxy 公开的 API, [csi-proxy](https://github.com/kubernetes-csi/csi-proxy) 由社区管理。

有关安装的更多详细信息，请参阅你要部署的 Windows CSI 插件的环境部署指南。 你也可以参考以下[安装步骤](https://github.com/kubernetes-csi/csi-proxy target=) 。

#### 联网 

Windows 容器的联网是通过 CNI 插件 来暴露出来的。Windows 容器的联网行为与虚拟机的联网行为类似。 每个容器有一块虚拟的网络适配器（vNIC）连接到 Hyper-V 的虚拟交换机（vSwitch）。 宿主的联网服务（Host Networking Service，HNS）和宿主计算服务（Host Compute Service，HCS）协同工作，创建容器并将容器的虚拟网卡连接到网络上。 HCS 负责管理容器，HNS 则负责管理网络资源，例如：

*   虚拟网络（包括创建 vSwitch）
*   端点（Endpoint）/ vNIC
*   名字空间（Namespace）
*   策略（报文封装、负载均衡规则、访问控制列表、网络地址转译规则等等）

支持的服务规约类型如下：

*   NodePort
*   ClusterIP
*   LoadBalancer
*   ExternalName

##### 网络模式 

Windows 支持五种不同的网络驱动/模式：二层桥接（L2bridge）、二层隧道（L2tunnel）、 覆盖网络（Overlay）、透明网络（Transparent）和网络地址转译（NAT）。 在一个包含 Windows 和 Linux 工作节点的异构集群中，你需要选择一种对 Windows 和 Linux 兼容的联网方案。下面是 Windows 上支持的一些树外插件及何时使用某种 CNI 插件的建议：

网络驱动

描述

容器报文更改

网络插件

网络插件特点

L2bridge

容器挂接到外部 vSwitch 上。容器挂接到下层网络之上，但由于容器的 MAC 地址在入站和出站时被重写，物理网络不需要这些地址。

MAC 地址被重写为宿主系统的 MAC 地址，IP 地址也可能依据 HNS OutboundNAT 策略重写为宿主的 IP 地址。

[win-bridge](https://github.com/containernetworking/plugins/tree/master/plugins/main/windows/win-bridge)、 [Azure-CNI](https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md)、 Flannel 宿主网关（host-gateway）使用 win-bridge

win-bridge 使用二层桥接（L2bridge）网络模式，将容器连接到下层宿主系统上， 从而提供最佳性能。需要用户定义的路由（User-Defined Routes，UDR）才能 实现节点间的连接。

L2Tunnel

这是二层桥接的一种特殊情形，但仅被用于 Azure 上。 所有报文都被发送到虚拟化环境中的宿主机上并根据 SDN 策略进行处理。

MAC 地址被改写，IP 地址在下层网络上可见。

[Azure-CNI](https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md)

Azure-CNI 使得容器能够与 Azure vNET 集成，并允许容器利用 \[Azure 虚拟网络\](https://azure.microsoft.com/en-us/services/virtual-network/) 所提供的功能特性集合。例如，可以安全地连接到 Azure 服务上或者使用 Azure NSG。 你可以参考 \[azure-cni\](https://docs.microsoft.com/en-us/azure/aks/concepts-network#azure-cni-advanced-networking) 所提供的一些示例。

覆盖网络（Kubernetes 中为 Windows 提供的覆盖网络支持处于 \*alpha\* 阶段）

每个容器会获得一个连接到外部 vSwitch 的虚拟网卡（vNIC）。 每个覆盖网络都有自己的、通过定制 IP 前缀来定义的 IP 子网。 覆盖网络驱动使用 VxLAN 封装。

封装于外层包头内。

[Win-overlay](https://github.com/containernetworking/plugins/tree/master/plugins/main/windows/win-overlay)、 Flannel VXLAN（使用 win-overlay）

当（比如出于安全原因）期望虚拟容器网络与下层宿主网络隔离时， 应该使用 win-overlay。如果你的数据中心可用 IP 地址受限， 覆盖网络允许你在不同的网络中复用 IP 地址（每个覆盖网络有不同的 VNID 标签）。 这一选项要求在 Windows Server 2009 上安装 \[KB4489899\](https://support.microsoft.com/help/4489899) 补丁。

透明网络（\[ovn-kubernetes\](https://github.com/openvswitch/ovn-kubernetes) 的特殊用例）

需要一个外部 vSwitch。容器挂接到某外部 vSwitch 上，该 vSwitch 通过逻辑网络（逻辑交换机和路由器）允许 Pod 间通信。

报文或者通过 \[GENEVE\](https://datatracker.ietf.org/doc/draft-gross-geneve/) 来封装， 或者通过 \[STT\](https://datatracker.ietf.org/doc/draft-davie-stt/) 隧道来封装， 以便能够到达不在同一宿主系统上的每个 Pod。  
报文通过 OVN 网络控制器所提供的隧道元数据信息来判定是转发还是丢弃。  
北-南向通信通过 NAT 网络地址转译来实现。

[ovn-kubernetes](https://github.com/openvswitch/ovn-kubernetes)

\[通过 Ansible 来部署\](https://github.com/openvswitch/ovn-kubernetes/tree/master/contrib)。 所发布的 ACL 可以通过 Kubernetes 策略来应用实施。支持 IPAM 。 负载均衡能力不依赖 kube-proxy。 网络地址转译（NAT）也不需要 iptables 或 netsh。

NAT（**未在 Kubernetes 中使用**）

容器获得一个连接到某内部 vSwitch 的 vNIC 接口。 DNS/DHCP 服务通过名为 \[WinNAT\](https://blogs.technet.microsoft.com/virtualization/2016/05/25/windows-nat-winnat-capabilities-and-limitations/) 的内部组件来提供。

MAC 地址和 IP 地址都被重写为宿主系统的 MAC 地址和 IP 地址。

[nat](https://github.com/Microsoft/windows-container-networking/tree/master/plugins/nat)

列在此表中仅出于完整性考虑

如前所述，[Flannel](https://github.com/flannel-io/flannel) CNI meta 插件 在 Windows 上也是 被支持 的，方法是通过 [VXLAN 网络后端](https://github.com/flannel-io/flannel/blob/master/Documentation/backends.md target=) （alpha 阶段 ：委托给 win-overlay）和 [主机-网关（host-gateway）网络后端](https://github.com/flannel-io/flannel/blob/master/Documentation/backends.md target=) （稳定版本；委托给 win-bridge 实现）。 此插件支持将操作委托给所引用的 CNI 插件（win-overlay、win-bridge）之一， 从而能够与 Windows 上的 Flannel 守护进程（Flanneld）一同工作，自动为节点 分配子网租期，创建 HNS 网络。 该插件读入其自身的配置文件（cni.conf），并将其与 FlannelD 所生成的 subnet.env 文件中的环境变量整合，之后将其操作委托给所引用的 CNI 插件之一以完成网络发现， 并将包含节点所被分配的子网信息的正确配置发送给 IPAM 插件（例如 host-local）。

对于节点、Pod 和服务对象，可针对 TCP/UDP 流量支持以下网络数据流：

*   Pod -> Pod （IP 寻址）
*   Pod -> Pod （名字寻址）
*   Pod -> 服务（集群 IP）
*   Pod -> 服务（部分限定域名，仅适用于名称中不包含“.”的情形）
*   Pod -> 服务（全限定域名）
*   Pod -> 集群外部（IP 寻址）
*   Pod -> 集群外部（DNS 寻址）
*   节点 -> Pod
*   Pod -> 节点

##### IP 地址管理（IPAM） 

Windows 上支持以下 IPAM 选项：

*   [host-local](https://github.com/containernetworking/plugins/tree/main/plugins/ipam/host-local)
*   HNS IPAM (Inbox 平台 IPAM，未指定 IPAM 时的默认设置）
*   [Azure-vnet-ipam](https://github.com/Azure/azure-container-networking/blob/master/docs/ipam.md)（仅适用于 azure-cni ）

##### 负载均衡与服务 

在 Windows 系统上，你可以使用以下配置来设定服务和负载均衡行为：

功能特性

描述

所支持的 Kubernetes 版本

所支持的 Windows OS 版本

如何启用

会话亲和性

确保来自特定客户的连接每次都被交给同一 Pod。

v1.20+

\[Windows Server vNext Insider Preview Build 19551\](https://blogs.windows.com/windowsexperience/2020/01/28/announcing-windows-server-vnext-insider-preview-build-19551/) 或更高版本

将 `service.spec.sessionAffinitys` 设置为 "ClientIP"

直接服务器返回（DSR）

这是一种负载均衡模式，IP 地址的修正和负载均衡地址转译（LBNAT） 直接在容器的 vSwitch 端口上处理；服务流量到达时，其源端 IP 地址 设置为来源 Pod 的 IP。

v1.20+

Windows Server 2019

为 kube-proxy 设置标志：\`--feature-gates="WinDSR=true" --enable-dsr=true\`

保留目标地址

对服务流量略过 DNAT 步骤，这样就可以在到达后端 Pod 的报文中保留目标服务的 虚拟 IP 地址。还要禁止节点之间的转发。

v1.20+

Windows Server 1903 或更高版本

在服务注解中设置 \`"preserve-destination": "true"\` 并启用 kube-proxy 中的 DSR 标志。

IPv4/IPv6 双栈网络

在集群内外同时支持原生的 IPv4-到-IPv4 和 IPv6-到-IPv6 通信。

v1.19+

Windows Server 2004 或更高版本

参见 \[IPv4/IPv6 双栈网络\](#ipv4ipv6-dual-stack)

保留客户端 IP

确保入站流量的源 IP 地址被保留。同样要禁止节点之间的转发。

v1.20+

Windows Server 2019 或更高版本

将 `service.spec.externalTrafficPolicy` 设置为 "Local"， 并在 kube-proxy 上启用 DSR。

#### IPv4/IPv6 双栈支持 

你可以通过使用 ​`IPv6DualStack` ​特性门控 来为 ​`l2bridge` ​网络启用 IPv4/IPv6 双栈联网支持。

对 Windows 而言，在 Kubernetes 中使用 IPv6 需要 Windows Server 2004 （内核版本 10.0.19041.610）或更高版本。

目前 Windows 上的覆盖网络（VXLAN）还不支持双协议栈联网。

### 局限性 

在 Kubernetes 架构和节点阵列中仅支持将 Windows 作为工作节点使用。 这意味着 Kubernetes 集群必须总是包含 Linux 主控节点，零个或者多个 Linux 工作节点以及零个或者多个 Windows 工作节点。

#### 资源处理

Linux 上使用 Linux 控制组（CGroups）作为 Pod 的边界，以实现资源控制。 容器都创建于这一边界之内，从而实现网络、进程和文件系统的隔离。 控制组 CGroups API 可用来收集 CPU、I/O 和内存的统计信息。 与此相比，Windows 为每个容器创建一个带有系统名字空间过滤设置的 Job 对象， 以容纳容器中的所有进程并提供其与宿主系统间的逻辑隔离。 没有现成的名字空间过滤设置是无法运行 Windows 容器的。 这也意味着，系统特权无法在宿主环境中评估，因而 Windows 上也就不存在特权容器。 归咎于独立存在的安全账号管理器（Security Account Manager，SAM），容器也不能 获得宿主系统上的任何身份标识。

#### 资源预留 

##### 内存预留 

Windows 不像 Linux 一样有一个内存耗尽（Out-of-memory）进程杀手（Process Killer）机制。Windows 总是将用户态的内存分配视为虚拟请求，页面文件（Pagefile） 是必需的。这一差异的直接结果是 Windows 不会像 Linux 那样出现内存耗尽的状况， 系统会将进程内存页面写入磁盘而不会因内存耗尽而终止进程。 当内存被过量使用且所有物理内存都被用光时，系统的换页行为会导致性能下降。

使用 kubelet 参数 ​`--kubelet-reserve`​ 与/或 ​`-system-reserve`​ 可以统计 节点上的内存用量（各容器之外），进而可能将内存用量限制在一个合理的范围，。 这样做会减少节点可分配内存。

在你部署工作负载时，对容器使用资源限制（必须仅设置 limits 或者让 limits 等于 requests 值）。这也会从 NodeAllocatable 中耗掉部分内存量，从而避免在节点 负荷已满时调度器继续向节点添加 Pods。

避免过量分配的最佳实践是为 kubelet 配置至少 2 GB 的系统预留内存，以供 Windows、Docker 和 Kubernetes 进程使用。

##### CPU 预留 

为了统计 Windows、Docker 和其他 Kubernetes 宿主进程的 CPU 用量，建议 预留一定比例的 CPU，以便对事件作出相应。此值需要根据 Windows 节点上 CPU 核的个数来调整，要确定此百分比值，用户需要为其所有节点确定 Pod 密度的上线，并监控系统服务的 CPU 用量，从而选择一个符合其负载需求的值。

使用 kubelet 参数 ​`--kubelet-reserve`​ 与/或 ​`-system-reserve`​ 可以统计 节点上的 CPU 用量（各容器之外），进而可能将 CPU 用量限制在一个合理的范围，。 这样做会减少节点可分配 CPU。

##### 功能特性限制 

*   终止宽限期（Termination Grace Period）：未实现
*   单文件映射：将用 CRI-ContainerD 来实现
*   终止消息（Termination message）：将用 CRI-ContainerD 来实现
*   特权容器：Windows 容器当前不支持
*   巨页（Huge Pages）：Windows 容器当前不支持
*   现有的节点问题探测器（Node Problem Detector）仅适用于 Linux，且要求使用特权容器。 一般而言，我们不设想此探测器能用于 Windows 节点，因为 Windows 不支持特权容器。
*   并非支持共享名字空间的所有功能特性（参见 API 节以了解详细信息）

#### 与 Linux 相比参数行为的差别

以下 kubelet 参数的行为在 Windows 节点上有些不同，描述如下：

*   ​`--kubelet-reserve`​、​`--system-reserve`​ 和 ​`--eviction-hard`​ 标志 会更新节点可分配资源量
*   未实现通过使用 ​`--enforce-node-allocable`​ 来完成的 Pod 驱逐
*   未实现通过使用 ​`--eviction-hard`​ 和 ​`--eviction-soft`​ 来完成的 Pod 驱逐
*   ​`MemoryPressure` ​状况未实现
*   ​`kubelet` ​不会采取措施来执行基于 OOM 的驱逐动作
*   Windows 节点上运行的 kubelet 没有内存约束。 ​`--kubelet-reserve`​ 和 ​`--system-reserve`​ 不会为 kubelet 或宿主系统上运行 的进程设限。这意味着 kubelet 或宿主系统上的进程可能导致内存资源紧张， 而这一情况既不受节点可分配量影响，也不会被调度器感知。
*   在 Windows 节点上存在一个额外的参数用来设置 kubelet 进程的优先级，称作 ​`--windows-priorityclass`​。此参数允许 kubelet 进程获得与 Windows 宿主上 其他进程相比更多的 CPU 时间片。 关于可用参数值及其含义的进一步信息可参考 [Windows Priority Classes](https://docs.microsoft.com/en-us/windows/win32/procthread/scheduling-priorities target=)。 为了让 kubelet 总能够获得足够的 CPU 周期，建议将此参数设置为 ​`ABOVE_NORMAL_PRIORITY_CLASS` ​或更高。

#### 存储 

Windows 上包含一个分层的文件系统来挂载容器的分层，并会基于 NTFS 来创建一个 拷贝文件系统。容器中的所有文件路径都仅在该容器的上下文内完成解析。

*   Docker 卷挂载仅可针对容器中的目录进行，不可针对独立的文件。 这一限制不适用于 CRI-containerD。
*   卷挂载无法将文件或目录投射回宿主文件系统。
*   不支持只读文件系统，因为 Windows 注册表和 SAM 数据库总是需要写访问权限。 不过，Windows 支持只读的卷。
*   不支持卷的用户掩码和访问许可，因为宿主与容器之间并不共享 SAM，二者之间不存在 映射关系。所有访问许可都是在容器上下文中解析的。

因此，Windows 节点上不支持以下存储功能特性：

*   卷的子路径挂载；只能在 Windows 容器上挂载整个卷。
*   为 Secret 执行子路径挂载；
*   宿主挂载投射；
*   默认访问模式 defaultMode（因为该特性依赖 UID/GID）；
*   只读的根文件系统；映射的卷仍然支持 ​`readOnly`​；
*   块设备映射；
*   将内存作为存储介质；
*   类似 UUID/GUID、每用户不同的 Linux 文件系统访问许可等文件系统特性；
*   基于 NFS 的存储和卷支持；
*   扩充已挂载卷（resizefs）。

#### 联网 

Windows 容器联网与 Linux 联网有着非常重要的差别。 [Microsoft documentation for Windows Container Networking](https://docs.microsoft.com/en-us/virtualization/windowscontainers/container-networking/architecture) 中包含额外的细节和背景信息。

Windows 宿主联网服务和虚拟交换机实现了名字空间隔离，可以根据需要为 Pod 或容器 创建虚拟的网络接口（NICs）。不过，很多类似 DNS、路由、度量值之类的配置数据都 保存在 Windows 注册表数据库中而不是像 Linux 一样保存在 ​`/etc/...`​ 文件中。 Windows 为容器提供的注册表与宿主系统的注册表是分离的，因此类似于将 /etc/resolv.conf 文件从宿主系统映射到容器中的做法不会产生与 Linux 系统相同的效果。 这些信息必须在容器内部使用 Windows API 来配置。 因此，CNI 实现需要调用 HNS，而不是依赖文件映射来将网络细节传递到 Pod 或容器中。

Windows 节点不支持以下联网功能：

*   Windows Pod 不能使用宿主网络模式；
*   从节点本地访问 NodePort 会失败（但从其他节点或外部客户端可访问）
*   Windows Server 的未来版本中会支持从节点访问服务的 VIP；
*   每个服务最多支持 64 个后端 Pod 或独立的目标 IP 地址；
*   kube-proxy 的覆盖网络支持是 Beta 特性。此外，它要求在 Windows Server 2019 上安装 [KB4482887](https://support.microsoft.com/en-us/topic/march-1-2019-kb4482887-os-build-17763-348-f7a9f207-0627-1fb9-cca7-29bb7b26027f) 补丁；
*   非 DSR（保留目标地址）模式下的本地流量策略；
*   连接到覆盖网络的 Windows 容器不支持使用 IPv6 协议栈通信。 要使得这一网络驱动支持 IPv6 地址需要在 Windows 平台上开展大量的工作， 还需要在 Kubernetes 侧修改 kubelet、kube-proxy 以及 CNI 插件。
*   通过 win-overlay、win-bridge 和 Azure-CNI 插件使用 ICMP 协议向集群外通信。 尤其是，Windows 数据面 （[VFP](https://www.microsoft.com/en-us/research/project/azure-virtual-filtering-platform/)） 不支持转换 ICMP 报文。这意味着：

*   指向同一网络内目标地址的 ICMP 报文（例如 Pod 之间的 ping 通信）是可以工作的， 没有局限性；
*   TCP/UDP 报文可以正常工作，没有局限性；
*   指向远程网络的 ICMP 报文（例如，从 Pod 中 ping 外部互联网的通信）无法被转换， 因此也无法被路由回到其源点；
*   由于 TCP/UDP 包仍可被转换，用户可以将 ​`ping <目标>`​ 操作替换为 ​`curl <目标>`​ 以便能够调试与外部世界的网络连接。

Kubernetes v1.15 中添加了以下功能特性：

*   ​`kubectl port-forward` ​

##### CNI 插件 

*   Windows 参考网络插件 win-bridge 和 win-overlay 当前未实现 [CNI spec](https://github.com/containernetworking/cni/blob/master/SPEC.md) v0.4.0， 原因是缺少检查（CHECK）用的实现。
    
*   Windows 上的 Flannel VXLAN CNI 有以下局限性：

1.  其设计上不支持从节点到 Pod 的连接。 只有在 Flannel v0.12.0 或更高版本后才有可能访问本地 Pods。
2.  我们被限制只能使用 VNI 4096 和 UDP 端口 4789。 VNI 的限制正在被解决，会在将来的版本中消失（开源的 Flannel 更改）。 参见官方的 [Flannel VXLAN](https://github.com/flannel-io/flannel/blob/master/Documentation/backends.md target=) 后端文档以了解关于这些参数的详细信息。

##### DNS

*   不支持 DNS 的 ClusterFirstWithHostNet 配置。Windows 将所有包含 “.” 的名字 视为全限定域名（FQDN），因而不会对其执行部分限定域名（PQDN）解析。
*   在 Linux 上，你可以有一个 DNS 后缀列表供解析部分限定域名时使用。 在 Windows 上，我们只有一个 DNS 后缀，即与该 Pod 名字空间相关联的 DNS 后缀（例如 ​`mydns.svc.cluster.local`​）。 Windows 可以解析全限定域名、或者恰好可用该后缀来解析的服务名称。 例如，在 default 名字空间中生成的 Pod 会获得 DNS 后缀 ​`default.svc.cluster.local`​。在 Windows Pod 中，你可以解析 ​`kubernetes.default.svc.cluster.local`​ 和 ​`kubernetes`​，但无法解析二者 之间的形式，如 ​`kubernetes.default`​ 或 ​`kubernetes.default.svc`​。
*   在 Windows 上，可以使用的 DNS 解析程序有很多。由于这些解析程序彼此之间 会有轻微的行为差别，建议使用 ​`Resolve-DNSName`​ 工具来完成名字查询解析。

##### IPv6

Windows 上的 Kubernetes 不支持单协议栈的“只用 IPv6”联网选项。 不过，系统支持在 IPv4/IPv6 双协议栈的 Pod 和节点上运行单协议家族的服务。

##### 会话亲和性 

不支持使用 ​`service.spec.sessionAffinityConfig.clientIP.timeoutSeconds`​ 来为 Windows 服务设置最大会话粘滞时间。

##### 安全性 

Secret 以明文形式写入节点的卷中（而不是像 Linux 那样写入内存或 tmpfs 中）。 这意味着客户必须做以下两件事：

1.  使用文件访问控制列表来保护 Secret 文件所在的位置
2.  使用 [BitLocker](https://docs.microsoft.com/en-us/windows/security/information-protection/bitlocker/bitlocker-how-to-deploy-on-windows-server) 来执行卷层面的加密

用户可以为 Windows Pods 或 Container 设置 ​`RunAsUserName` ​以便以非节点默认用户来执行容器中的进程。这大致等价于设置 ​`RunAsUser`​。

不支持特定于 Linux 的 Pod 安全上下文特权，例如 SELinux、AppArmor、Seccomp、 权能字（POSIX 权能字）等等。

此外，如前所述，Windows 不支持特权容器。

#### API

对 Windows 而言，大多数 Kubernetes API 的工作方式没有变化。 一些不易察觉的差别通常体现在 OS 和容器运行时上的不同。 在某些场合，负载 API （如 Pod 或 Container）的某些属性在设计时假定其 在 Linux 上实现，因此会无法在 Windows 上运行。

在较高层面，不同的 OS 概念有：

*   身份标识 - Linux 使用证书类型来表示用户 ID（UID）和组 ID（GID）。用户和组名 没有特定标准，它们是 ​`/etc/groups`​ 或 ​`/etc/passwd`​ 中的别名表项，会映射回 UID+GID。Windows 使用一个更大的二进制安全标识符（SID），保存在 Windows 安全访问管理器（Security Access Manager，SAM）数据库中。此数据库并不在宿主系统 与容器间，或者任意两个容器之间共享。
*   文件许可 - Windows 使用基于 SID 的访问控制列表，而不是基于 UID+GID 的访问权限位掩码。
*   文件路径 - Windows 上的习惯是使用 ​`\`​ 而非 ​`/`​。Go 语言的 IO 库同时接受这两种文件路径分隔符。不过，当你在指定要在容器内解析的路径或命令行时， 可能需要使用 ​`\`​。
*   信号（Signal） - Windows 交互式应用以不同方式来处理终止事件，并可实现以下方式之一或组合：

*   UI 线程处理包含 ​`WM_CLOSE` ​在内的良定的消息
*   控制台应用使用控制处理程序来处理 Ctrl-C 或 Ctrl-Break
*   服务会注册服务控制处理程序，接受 ​`SERVICE_CONTROL_STOP` ​控制代码

退出代码遵从相同的习惯，0 表示成功，非 0 值表示失败。 特定的错误代码在 Windows 和 Linux 上可能会不同。不过，从 Kubernetes 组件 （kubelet、kube-proxy）所返回的退出代码是没有变化的。

*   ​`v1.Container.ResourceRequirements.limits.cpu`​ 和 ​`v1.Container.ResourceRequirements.limits.memory`​ - Windows 不对 CPU 分配设置硬性的限制。与之相反，Windows 使用一个份额（share）系统。 基于毫核（millicores）的现有字段值会被缩放为相对的份额值，供 Windows 调度器使用。

*   Windows 容器运行时中没有实现巨页支持，因此相关特性不可用。 巨页支持需要判定用户的特权 而这一特性无法在容器级别配置。

*   ​`v1.Container.ResourceRequirements.requests.cpu`​ 和 ​`v1.Container.ResourceRequirements.requests.memory`​ - 请求 值会从节点可分配资源中扣除，从而可用来避免节点上的资源过量分配。 但是，它们无法用来在一个已经过量分配的节点上提供资源保障。 如果操作员希望彻底避免过量分配，作为最佳实践，他们就需要为所有容器设置资源请求值。
*   ​`v1.Container.SecurityContext.allowPrivilegeEscalation`​ - 在 Windows 上无法实现，对应的权能无一可在 Windows 上生效。
*   ​`v1.Container.SecurityContext.Capabilities`​ - Windows 上未实现 POSIX 权能机制
*   ​`v1.Container.SecurityContext.privileged`​ - Windows 不支持特权容器
*   ​`v1.Container.SecurityContext.procMount`​ - Windows 不包含 ​`/proc`​ 文件系统
*   ​`v1.Container.SecurityContext.readOnlyRootFilesystem`​ - 在 Windows 上无法实现， 要在容器内使用注册表或运行系统进程就必需写访问权限。
*   ​`v1.Container.SecurityContext.runAsGroup`​ - 在 Windows 上无法实现，没有 GID 支持
*   ​`v1.Container.SecurityContext.runAsNonRoot`​ - Windows 上没有 root 用户。 与之最接近的等价用户是 ContainerAdministrator，而该身份标识在节点上并不存在。
*   ​`v1.Container.SecurityContext.runAsUser`​ - 在 Windows 上无法实现， 因为没有作为整数支持的 GID。
*   ​`v1.Container.SecurityContext.seLinuxOptions`​ - 在 Windows 上无法实现， 因为没有 SELinux
*   ​`V1.Container.terminationMessagePath`​ - 因为 Windows 不支持单个文件的映射，这一功能 在 Windows 上也受限。默认值 ​`/dev/termination-log`​ 在 Windows 上也无法使用因为 对应路径在 Windows 上不存在。

##### V1.Pod

*   ​`v1.Pod.hostIPC`​、​`v1.Pod.hostPID`​ - Windows 不支持共享宿主系统的名字空间
*   ​`v1.Pod.hostNetwork`​ - Windows 操作系统不支持共享宿主网络
*   ​`v1.Pod.dnsPolicy`​ - 不支持 ​`ClusterFirstWithHostNet`​，因为 Windows 不支持宿主网络
*   ​`v1.Pod.podSecurityContext`​ - 参见下面的 ​`v1.PodSecurityContext`​
*   ​`v1.Pod.shareProcessNamespace`​ - 此为 Beta 特性且依赖于 Windows 上未实现 的 Linux 名字空间。 Windows 无法共享进程名字空间或者容器的根文件系统。只能共享网络。
*   ​`v1.Pod.terminationGracePeriodSeconds`​ - 这一特性未在 Windows 版本的 Docker 中完全实现。 参见[问题报告](https://github.com/moby/moby/issues/25982)。 目前实现的行为是向 ​`ENTRYPOINT` ​进程发送 ​`CTRL_SHUTDOWN_EVENT` ​事件，之后 Windows 默认 等待 5 秒钟，并最终使用正常的 Windows 关机行为关闭所有进程。 这里的 5 秒钟默认值实际上保存在  [容器内](https://github.com/moby/moby/issues/25982 target=) 的 Windows 注册表中，因此可以在构造容器时重载。
*   ​`v1.Pod.volumeDevices`​ - 此为 Beta 特性且未在 Windows 上实现。Windows 无法挂接 原生的块设备到 Pod 中。
*   ​`v1.Pod.volumes`​ - ​`emptyDir`​、​`secret`​、​`configMap` ​和 ​`hostPath` ​都可正常工作且在 TestGrid 中测试。

*   ​`v1.emptyDir.volumeSource`​ - Windows 上节点的默认介质是磁盘。 不支持将内存作为介质，因为 Windows 不支持内置的 RAM 磁盘。

*   ​`v1.VolumeMount.mountPropagation`​ - Windows 上不支持挂载传播。

##### V1.PodSecurityContext

PodSecurityContext 的所有选项在 Windows 上都无法工作。这些选项列在下面仅供参考。

*   ​`v1.PodSecurityContext.seLinuxOptions`​ - Windows 上无 SELinux
*   ​`v1.PodSecurityContext.runAsUser`​ - 提供 UID；Windows 不支持
*   ​`v1.PodSecurityContext.runAsGroup`​ - 提供 GID；Windows 不支持
*   ​`v1.PodSecurityContext.runAsNonRoot`​ - Windows 上没有 root 用户 最接近的等价账号是 ​`ContainerAdministrator`​，而该身份标识在节点上不存在
*   ​`v1.PodSecurityContext.supplementalGroups`​ - 提供 GID；Windows 不支持
*   ​`v1.PodSecurityContext.sysctls`​ - 这些是 Linux sysctl 接口的一部分；Windows 上 没有等价机制。

#### 操作系统版本限制 

Windows 有着严格的兼容性规则，宿主 OS 的版本必须与容器基准镜像 OS 的版本匹配。 目前仅支持容器操作系统为 Windows Server 2019 的 Windows 容器。 对于容器的 Hyper-V 隔离、允许一定程度上的 Windows 容器镜像版本向后兼容性等等， 都是将来版本计划的一部分。

获取帮助和故障排查 
----------

Kubernetes 中日志是故障排查的一个重要元素。确保你在尝试从其他贡献者那里获得 故障排查帮助时提供日志信息。你可以按照 SIG-Windows [贡献指南和收集日志](https://github.com/kubernetes/community/blob/master/sig-windows/CONTRIBUTING.md target=) 所给的指令来操作。

*   我怎样知道 ​`start.ps1`​ 是否已成功完成？

你应该能看到节点上运行的 kubelet、kube-proxy 和（如果你选择 Flannel 作为联网方案）flanneld 宿主代理进程，它们的运行日志显示在不同的 PowerShell 窗口中。此外，你的 Windows 节点应该在你的 Kubernetes 集群 列举为 "Ready" 节点。

*   我可以将 Kubernetes 节点进程配置为服务运行在后台么？

kubelet 和 kube-proxy 都已经被配置为以本地 Windows 服务运行， 并且在出现失效事件（例如进程意外结束）时通过自动重启服务来提供一定的弹性。 你有两种办法将这些节点组件配置为服务。

*   以本地 Windows 服务的形式

Kubelet 和 kube-proxy 可以用 ​`sc.exe`​ 以本地 Windows 服务的形式运行：

`# 用两个单独的命令为 kubelet 和 kube-proxy 创建服务 sc.exe create <组件名称> binPath="<可执行文件路径> -service <其它参数>"  # 请注意如果参数中包含空格，必须使用转义 sc.exe create kubelet binPath= "C:\kubelet.exe --service --hostname-override 'minion' <其它参数>"  # 启动服务 Start-Service kubelet Start-Service kube-proxy  # 停止服务 Stop-Service kubelet (-Force) Stop-Service kube-proxy (-Force)  # 查询服务状态 Get-Service kubelet Get-Service kube-proxy`

*   使用 nssm.exe

你也总是可以使用替代的服务管理器，例如[nssm.exe](https://nssm.cc/)，来为你在后台运行 这些进程（​`flanneld`​、​`kubelet` ​和 ​`kube-proxy`​）。你可以使用这一 [示例脚本](https://github.com/microsoft/SDN/blob/master/Kubernetes/flannel/register-svc.ps1)， 利用 ​`nssm.exe`​ 将 ​`kubelet`​、​`kube-proxy`​ 和 ​`flanneld.exe`​ 注册为要在后台运行的 Windows 服务。

`register-svc.ps1 -NetworkMode <网络模式> -ManagementIP <Windows 节点 IP> -ClusterCIDR <集群子网> -KubeDnsServiceIP <kube-dns 服务 IP> -LogDir <日志目录>`

这里的参数解释如下：

*   ​`NetworkMode`​：网络模式 l2bridge（flannel host-gw，也是默认值）或 overlay（flannel vxlan）选做网络方案
*   ​`ManagementIP`​：分配给 Windows 节点的 IP 地址。你可以使用 ipconfig 得到此值
*   ​`ClusterCIDR`​：集群子网范围（默认值为 10.244.0.0/16）
*   ​`KubeDnsServiceIP`​：Kubernetes DNS 服务 IP（默认值为 10.96.0.10）
*   ​`LogDir`​：kubelet 和 kube-proxy 的日志会被重定向到这一目录中的对应输出文件， 默认值为 ​`C:\k`​。

若以上所引用的脚本不适合，你可以使用下面的例子手动配置 ​`nssm.exe`​。

注册 flanneld.exe：

`nssm install flanneld C:\flannel\flanneld.exe nssm set flanneld AppParameters --kubeconfig-file=c:\k\config --iface=<ManagementIP> --ip-masq=1 --kube-subnet-mgr=1 nssm set flanneld AppEnvironmentExtra NODE_NAME=<hostname> nssm set flanneld AppDirectory C:\flannel nssm start flanneld`

注册 kubelet.exe：

`nssm install kubelet C:\k\kubelet.exe nssm set kubelet AppParameters --hostname-override=<hostname> --v=6 --pod-infra-container-image=k8s.gcr.io/pause:3.5 --resolv-conf="" --allow-privileged=true --enable-debugging-handlers --cluster-dns=<DNS-service-IP> --cluster-domain=cluster.local --kubeconfig=c:\k\config --hairpin-mode=promiscuous-bridge --image-pull-progress-deadline=20m --cgroups-per-qos=false  --log-dir=<log directory> --logtostderr=false --enforce-node-allocatable="" --network-plugin=cni --cni-bin-dir=c:\k\cni --cni-conf-dir=c:\k\cni\config nssm set kubelet AppDirectory C:\k nssm start kubelet`

注册 kube-proxy.exe（二层网桥模式和主机网关模式）

`nssm install kube-proxy C:\k\kube-proxy.exe nssm set kube-proxy AppDirectory c:\k nssm set kube-proxy AppParameters --v=4 --proxy-mode=kernelspace --hostname-override=<hostname>--kubeconfig=c:\k\config --enable-dsr=false --log-dir=<log directory> --logtostderr=false nssm.exe set kube-proxy AppEnvironmentExtra KUBE_NETWORK=cbr0 nssm set kube-proxy DependOnService kubelet nssm start kube-proxy`

注册 kube-proxy.exe（覆盖网络模式或 VxLAN 模式）

`nssm install kube-proxy C:\k\kube-proxy.exe nssm set kube-proxy AppDirectory c:\k nssm set kube-proxy AppParameters --v=4 --proxy-mode=kernelspace --feature-gates="WinOverlay=true" --hostname-override=<hostname> --kubeconfig=c:\k\config --network-name=vxlan0 --source-vip=<source-vip> --enable-dsr=false --log-dir=<log directory> --logtostderr=false nssm set kube-proxy DependOnService kubelet nssm start kube-proxy`

作为初始的故障排查操作，你可以使用在 [nssm.exe](https://nssm.cc/) 中使用下面的标志 以便将标准输出和标准错误输出重定向到一个输出文件：

`nssm set <服务名称> AppStdout C:\k\mysvc.log nssm set <服务名称> AppStderr C:\k\mysvc.log`

要了解更多的细节，可参见官方的 [nssm 用法](https://nssm.cc/usage)文档。

*   我的 Windows Pods 无法连接网络

如果你在使用虚拟机，请确保 VM 网络适配器均已开启 MAC 侦听（Spoofing）。

*   我的 Windows Pods 无法 ping 外部资源

Windows Pods 目前没有为 ICMP 协议提供出站规则。不过 TCP/UDP 是支持的。 尝试与集群外资源连接时，可以将 ​`ping <IP>`​ 命令替换为对应的 ​`curl <IP>`​ 命令。

如果你还遇到问题，很可能你在 [cni.conf](https://github.com/Microsoft/SDN/blob/master/Kubernetes/flannel/l2bridge/cni/config/cni.conf) 中的网络配置值得额外的注意。你总是可以编辑这一静态文件。 配置的更新会应用到所有新创建的 Kubernetes 资源上。

Kubernetes 网络的需求之一是集群内部无需网络地址转译（NAT）即可实现通信。 为了符合这一要求，对所有我们不希望出站时发生 NAT 的通信都存在一个 [ExceptionList](https://github.com/Microsoft/SDN/blob/master/Kubernetes/flannel/l2bridge/cni/config/cni.conf target=)。 然而这也意味着你需要将你要查询的外部 IP 从 ExceptionList 中移除。 只有这时，从你的 Windows Pod 发起的网络请求才会被正确地通过 SNAT 转换以接收到 来自外部世界的响应。 就此而言，你在 ​`cni.conf`​ 中的 ​`ExceptionList` ​应该看起来像这样：

`"ExceptionList": [     "10.244.0.0/16",  # 集群子网     "10.96.0.0/12",   # 服务子网     "10.127.130.0/24" # 管理（主机）子网 ]`

*   我的 Windows 节点无法访问 NodePort 服务

从节点自身发起的本地 NodePort 请求会失败。这是一个已知的局限。 NodePort 服务的访问从其他节点或者外部客户端都可正常进行。

*   容器的 vNICs 和 HNS 端点被删除了

这一问题可能因为 ​`hostname-override`​ 参数未能传递给 kube-proxy 而导致。解决这一问题时，用户需要按如下方式将主机名传递给 kube-proxy：

`C:\k\kube-proxy.exe --hostname-override=$(hostname)`

*   使用 Flannel 时，我的节点在重新加入集群时遇到问题

无论何时，当一个之前被删除的节点被重新添加到集群时，flannelD 都会将为节点分配 一个新的 Pod 子网。 用户需要将将下面路径中的老的 Pod 子网配置文件删除：

`Remove-Item C:\k\SourceVip.json Remove-Item C:\k\SourceVipRequest.json`

*   在启动了 ​`start.ps1`​ 之后，flanneld 一直停滞在 "Waiting for the Network to be created" 状态

关于这一[问题](https://github.com/flannel-io/flannel/issues/1066)有很多的报告； 最可能的一种原因是关于何时设置 Flannel 网络的管理 IP 的时间问题。 一种解决办法是重新启动 ​`start.ps1`​ 或者按如下方式手动重启之：

`[Environment]::SetEnvironmentVariable("NODE_NAME", "<Windows 工作节点主机名>") C:\flannel\flanneld.exe --kubeconfig-file=c:\k\config --iface=<Windows 工作节点 IP> --ip-masq=1 --kube-subnet-mgr=1`

*   我的 Windows Pods 无法启动，因为缺少 ​`/run/flannel/subnet.env`​ 文件

这表明 Flannel 网络未能正确启动。你可以尝试重启 flanneld.exe 或者将文件手动地 从 Kubernetes 主控节点的 ​`/run/flannel/subnet.env`​ 路径复制到 Windows 工作 节点的 ​`C:\run\flannel\subnet.env`​ 路径，并将 ​`FLANNEL_SUBNET` ​行改为一个 不同的数值。例如，如果期望节点子网为 ​`10.244.4.1/24`​：

`FLANNEL_NETWORK=10.244.0.0/16 FLANNEL_SUBNET=10.244.4.1/24 FLANNEL_MTU=1500 FLANNEL_IPMASQ=true`

*   我的 Windows 节点无法使用服务 IP 访问我的服务

这是 Windows 上当前网络协议栈的一个已知的限制。 Windows Pods 能够访问服务 IP。

*   启动 kubelet 时找不到网络适配器

Windows 网络堆栈需要一个虚拟的适配器，这样 Kubernetes 网络才能工作。 如果下面的命令（在管理员 Shell 中）没有任何返回结果，证明虚拟网络创建 （kubelet 正常工作的必要前提之一）失败了：

`Get-HnsNetwork | ? Name -ieq "cbr0" Get-NetAdapter | ? Name -Like "vEthernet (Ethernet*"`

当宿主系统的网络适配器名称不是 "Ethernet" 时，通常值得更改 ​`start.ps1`​ 脚本中的 [InterfaceName](https://github.com/microsoft/SDN/blob/master/Kubernetes/flannel/start.ps1 target=) 参数来重试。否则可以查验 ​`start-kubelet.ps1`​ 的输出，看看是否在虚拟网络创建 过程中报告了其他错误。

*   我的 Pods 停滞在 "Container Creating" 状态或者反复重启

检查你的 pause 镜像是与你的 OS 版本兼容的。 如果你安装的是更新版本的 Windows，比如说 某个 Insider 构造版本，你需要相应地调整要使用的镜像。 请参照 Microsoft 的 [Docker 仓库](https://hub.docker.com/u/microsoft/) 了解镜像。不管怎样，pause 镜像的 Dockerfile 和示例服务都期望镜像的标签 为 ​`:latest`​。

*   ​`kubectl port-forward`​ 失败，错误信息为 "unable to do port forwarding: wincat not found"

此功能是在 Kubernetes v1.15 中实现的，pause 基础设施容器 ​`mcr.microsoft.com/oss/kubernetes/pause:3.4.1`​ 中包含了 wincat.exe。 请确保你使用的是这些版本或者更新版本。 如果你想要自行构造你自己的 pause 基础设施容器，要确保其中包含了 [wincat](https://github.com/kubernetes-sigs/sig-windows-tools/tree/master/cmd/wincat)

Windows 的端口转发支持需要在 pause 基础设施容器 中提供 wincat.exe。 确保你使用的是与你的 Windows 操作系统版本兼容的受支持镜像。 如果你想构建自己的 pause 基础架构容器，请确保包含 [wincat](https://github.com/kubernetes/kubernetes/tree/master/build/pause/windows/wincat)。

*   我的 Kubernetes 安装失败，因为我的 Windows Server 节点在防火墙后面

如果你处于防火墙之后，那么必须定义如下 PowerShell 环境变量：

`[Environment]::SetEnvironmentVariable("HTTP_PROXY", "http://proxy.example.com:80/", [EnvironmentVariableTarget]::Machine) [Environment]::SetEnvironmentVariable("HTTPS_PROXY", "http://proxy.example.com:443/", [EnvironmentVariableTarget]::Machine)`

*   ​`pause` ​容器是什么？

在一个 Kubernetes Pod 中，一个基础设施容器，或称 "pause" 容器，会被首先创建出来， 用以托管容器端点。属于同一 Pod 的容器，包括基础设施容器和工作容器，会共享相同的 网络名字空间和端点（相同的 IP 和端口空间）。我们需要 pause 容器来工作容器崩溃或 重启的状况，以确保不会丢失任何网络配置。

### 进一步探究 

如果以上步骤未能解决你遇到的问题，你可以通过以下方式获得在 Kubernetes 中的 Windows 节点上运行 Windows 容器的帮助：

*   StackOverflow [Windows Server Container](https://stackoverflow.com/questions/tagged/windows-server-container) 主题
*   Kubernetes 官方论坛 [discuss.kubernetes.io](https://discuss.kubernetes.io/)
*   Kubernetes Slack [#SIG-Windows 频道](https://kubernetes.slack.com/?redir=%2Fmessages%2Fsig-windows)

报告问题和功能需求 
----------

如果你遇到看起来像是软件缺陷的问题，或者你想要提起某种功能需求，请使用 [GitHub 问题跟踪系统](https://github.com/kubernetes/kubernetes/issues)。 你可以在 [GitHub](https://github.com/kubernetes/kubernetes/issues/new/choose) 上发起 Issue 并将其指派给 SIG-Windows。你应该首先搜索 Issue 列表，看看是否 该 Issue 以前曾经被报告过，以评论形式将你在该 Issue 上的体验追加进去，并附上 额外的日志信息。SIG-Windows Slack 频道也是一个获得初步支持的好渠道，可以在 生成新的 Ticket 之前对一些想法进行故障分析。

在登记软件缺陷时，请给出如何重现该问题的详细信息，例如：

*   Kubernetes 版本：kubectl 版本
*   环境细节：云平台、OS 版本、网络选型和配置情况以及 Docker 版本
*   重现该问题的详细步骤
*   [相关的日志](https://github.com/kubernetes/community/blob/master/sig-windows/CONTRIBUTING.md#gathering-logs)
*   通过为该 Issue 添加 ​`/sig windows`​ 评论为其添加 ​`sig/windows`​ 标签， 进而引起 SIG-Windows 成员的注意。

##  13.  Kubernetes Windows容器的调度指南
Kubernetes 中 Windows 容器的调度指南  

Windows 应用程序构成了许多组织中运行的服务和应用程序的很大一部分。 本指南将引导你完成在 Kubernetes 中配置和部署 Windows 容器的步骤。

目标
--

*   配置一个示例 deployment 以在 Windows 节点上运行 Windows 容器
*   （可选）使用组托管服务帐户（GMSA）为你的 Pod 配置 Active Directory 身份

在开始之前
-----

*   创建一个 Kubernetes 集群，其中包括一个控制平面和 运行 Windows 服务器的工作节点
*   重要的是要注意，对于 Linux 和 Windows 容器，在 Kubernetes 上创建和部署服务和工作负载的行为几乎相同。 与集群接口的 kubectl 命令相同。 提供以下部分中的示例只是为了快速启动 Windows 容器的使用体验。

入门：部署 Windows 容器
----------------

要在 Kubernetes 上部署 Windows 容器，你必须首先创建一个示例应用程序。 下面的示例 YAML 文件创建了一个简单的 Web 服务器应用程序。 创建一个名为 ​`win-webserver.yaml`​ 的服务规约，其内容如下：

`apiVersion: v1 kind: Service metadata:   name: win-webserver   labels:     app: win-webserver spec:   ports:     # the port that this service should serve on     - port: 80       targetPort: 80   selector:     app: win-webserver   type: NodePort --- apiVersion: apps/v1 kind: Deployment metadata:   labels:     app: win-webserver   name: win-webserver spec:   replicas: 2   selector:     matchLabels:       app: win-webserver   template:     metadata:       labels:         app: win-webserver       name: win-webserver     spec:      containers:       - name: windowswebserver         image: mcr.microsoft.com/windows/servercore:ltsc2019         command:         - powershell.exe         - -command         - "<#code used from https://gist.github.com/19WAS85/5424431#> ; $listener = New-Object System.Net.HttpListener ; $listener.Prefixes.Add('http://*:80/') ; $listener.Start() ; $callerCounts = @{} ; Write-Host('Listening at http://*:80/') ; while ($listener.IsListening) { ;$context = $listener.GetContext() ;$requestUrl = $context.Request.Url ;$clientIP = $context.Request.RemoteEndPoint.Address ;$response = $context.Response ;Write-Host '' ;Write-Host('> {0}' -f $requestUrl) ;  ;$count = 1 ;$k=$callerCounts.Get_Item($clientIP) ;if ($k -ne $null) { $count += $k } ;$callerCounts.Set_Item($clientIP, $count) ;$ip=(Get-NetAdapter | Get-NetIpAddress); $header='<html><body><H1>Windows Container Web Server</H1>' ;$callerCountsString='' ;$callerCounts.Keys | % { $callerCountsString+='<p>IP {0} callerCount {1} ' -f $ip[1].IPAddress,$callerCounts.Item($_) } ;$footer='</body></html>' ;$content='{0}{1}{2}' -f $header,$callerCountsString,$footer ;Write-Output $content ;$buffer = [System.Text.Encoding]::UTF8.GetBytes($content) ;$response.ContentLength64 = $buffer.Length ;$response.OutputStream.Write($buffer, 0, $buffer.Length) ;$response.Close() ;$responseStatus = $response.StatusCode ;Write-Host('< {0}' -f $responseStatus)  } ; "      nodeSelector:       kubernetes.io/os: windows`

> Note: 端口映射也是支持的，但为简单起见，在此示例中容器端口 80 直接暴露给服务。

1.  检查所有节点是否健康：

`kubectl get nodes`

3.  部署服务并观察 pod 更新：

`kubectl apply -f win-webserver.yaml kubectl get pods -o wide -w`

正确部署服务后，两个 Pod 都标记为 “Ready”。要退出 watch 命令，请按 Ctrl + C。

6.  检查部署是否成功。验证：

*   Windows 节点上每个 Pod 有两个容器，使用 ​`docker ps`​
*   Linux 控制平面节点列出两个 Pod，使用 ​`kubectl get pods`​
*   跨网络的节点到 Pod 通信，从 Linux 控制平面节点 ​`curl` ​你的 pod IPs 的端口 80，以检查 Web 服务器响应
*   Pod 到 Pod 的通信，使用 docker exec 或 kubectl exec 在 Pod 之间 （以及跨主机，如果你有多个 Windows 节点）进行 ping 操作
*   服务到 Pod 的通信，从 Linux 控制平面节点和各个 Pod 中 ​`curl` ​虚拟服务 IP （在 ​`kubectl get services`​ 下可见）
*   服务发现，使用 Kubernetes ​`curl` ​服务名称 默认 DNS 后缀
*   入站连接，从 Linux 控制平面节点或集群外部的计算机 ​`curl` ​NodePort
*   出站连接，使用 kubectl exec 从 Pod 内部 curl 外部 IP

> Note: 由于当前平台对 Windows 网络堆栈的限制，Windows 容器主机无法访问在其上调度的服务的 IP。只有 Windows pods 才能访问服务 IP。

可观测性 
-----

### 抓取来自工作负载的日志

日志是可观测性的重要一环；使用日志用户可以获得对负载运行状况的洞察， 因而日志是故障排查的一个重要手法。 因为 Windows 容器中的 Windows 容器和负载与 Linux 容器的行为不同， 用户很难收集日志，因此运行状态的可见性很受限。 例如，Windows 工作负载通常被配置为将日志输出到 Windows 事件跟踪 （Event Tracing for Windows，ETW），或者将日志条目推送到应用的事件日志中。  [LogMonitor](https://github.com/microsoft/windows-container-tools/tree/master/LogMonitor) 是 Microsoft 提供的一个开源工具，是监视 Windows 容器中所配置的日志源 的推荐方式。 LogMonitor 支持监视时间日志、ETW 提供者模块以及自定义的应用日志， 并使用管道的方式将其输出到标准输出（stdout），以便 ​`kubectl logs <pod>`​ 这类命令能够读取这些数据。

请遵照 LogMonitor GitHub 页面上的指令，将其可执行文件和配置文件复制到 你的所有容器中，并为其添加必要的入口点（Entrypoint），以便 LogMonitor 能够将你的日志输出推送到标准输出（stdout）。

使用可配置的容器用户名
-----------

从 Kubernetes v1.16 开始，可以为 Windows 容器配置与其镜像默认值不同的用户名 来运行其入口点和进程。 此能力的实现方式和 Linux 容器有些不同。

使用组托管服务帐户管理工作负载身份
-----------------

从 Kubernetes v1.14 开始，可以将 Windows 容器工作负载配置为使用组托管服务帐户（GMSA）。 组托管服务帐户是 Active Directory 帐户的一种特定类型，它提供自动密码管理， 简化的服务主体名称（SPN）管理以及将管理委派给跨多台服务器的其他管理员的功能。 配置了 GMSA 的容器可以访问外部 Active Directory 域资源，同时携带通过 GMSA 配置的身份。

污点和容忍度
------

目前，用户需要将 Linux 和 Windows 工作负载运行在各自特定的操作系统的节点上， 因而需要结合使用污点和节点选择算符。这可能仅给 Windows 用户造成不便。 推荐的方法概述如下，其主要目标之一是该方法不应破坏与现有 Linux 工作负载的兼容性。

如果 ​`IdentifyPodOS` ​特性门控是启用的， 你可以（并且应该）为 Pod 设置 ​`.spec.os.name`​ 以表明该 Pod 中的容器所针对的操作系统。对于运行 Linux 容器的 Pod，设置 ​`.spec.os.name`​ 为 ​`linux`​。对于运行 Windows 容器的 Pod，设置 ​`.spec.os.name`​ 为 ​`Windows`​。

> Note: 从 1.24 开始，​`IdentifyPodOS` ​功能处于 Beta 阶段，默认启用。

在将 Pod 分配给节点时，调度程序不使用 ​`.spec.os.name`​ 的值。你应该使用正常的 Kubernetes 机制将 Pod 分配给节点， 确保集群的控制平面将 Pod 放置到适合运行的操作系统。 ​`.spec.os.name`​ 值对 Windows Pod 的调度没有影响，因此仍然需要污点、容忍度以及节点选择器， 以确保 Windows Pod 调度至合适的 Windows 节点。

### 确保特定操作系统的工作负载落在适当的容器主机上

用户可以使用污点和容忍度确保 Windows 容器可以调度在适当的主机上。目前所有 Kubernetes 节点都具有以下默认标签：

*   kubernetes.io/os = \[windows|linux\]
*   kubernetes.io/arch = \[amd64|arm64|...\]

如果 Pod 规范未指定诸如 ​`"kubernetes.io/os": windows`​ 之类的 nodeSelector，则该 Pod 可能会被调度到任何主机（Windows 或 Linux）上。 这是有问题的，因为 Windows 容器只能在 Windows 上运行，而 Linux 容器只能在 Linux 上运行。 最佳实践是使用 nodeSelector。

但是，我们了解到，在许多情况下，用户都有既存的大量的 Linux 容器部署，以及一个现成的配置生态系统， 例如社区 Helm charts，以及程序化 Pod 生成案例，例如 Operators。 在这些情况下，你可能会不愿意更改配置添加 nodeSelector。替代方法是使用污点。 由于 kubelet 可以在注册期间设置污点，因此可以轻松修改它，使其仅在 Windows 上运行时自动添加污点。

例如：​`--register-with-taints='os=windows:NoSchedule'` ​

向所有 Windows 节点添加污点后，Kubernetes 将不会在它们上调度任何负载（包括现有的 Linux Pod）。 为了使某 Windows Pod 调度到 Windows 节点上，该 Pod 需要 nodeSelector 和合适的匹配的容忍度设置来选择 Windows。

`nodeSelector:     kubernetes.io/os: windows     node.kubernetes.io/windows-build: '10.0.17763' tolerations:     - key: "os"       operator: "Equal"       value: "windows"       effect: "NoSchedule"`

### 处理同一集群中的多个 Windows 版本

每个 Pod 使用的 Windows Server 版本必须与该节点的 Windows Server 版本相匹配。 如果要在同一集群中使用多个 Windows Server 版本，则应该设置其他节点标签和 nodeSelector。

Kubernetes 1.17 自动添加了一个新标签 ​`node.kubernetes.io/windows-build`​ 来简化此操作。 如果你运行的是旧版本，则建议手动将此标签添加到 Windows 节点。

此标签反映了需要兼容的 Windows 主要、次要和内部版本号。以下是当前每个 Windows Server 版本使用的值。

产品名称

内部编号

Windows Server 2019

10.0.17763

Windows Server version 1809

10.0.17763

Windows Server version 1903

10.0.18362

### 使用 RuntimeClass 简化

RuntimeClass 可用于 简化使用污点和容忍度的过程。 集群管理员可以创建 ​`RuntimeClass` ​对象，用于封装这些污点和容忍度。

1.  将此文件保存到 ​`runtimeClasses.yml`​ 文件。 它包括适用于 Windows 操作系统、体系结构和版本的 ​`nodeSelector`​。

`apiVersion: node.k8s.io/v1 kind: RuntimeClass metadata:   name: windows-2019 handler: 'docker' scheduling:   nodeSelector:     kubernetes.io/os: 'windows'     kubernetes.io/arch: 'amd64'     node.kubernetes.io/windows-build: '10.0.17763'   tolerations:   - effect: NoSchedule     key: os     operator: Equal     value: "windows"`

3.  集群管理员执行 ​`kubectl create -f runtimeClasses.yml`​ 操作
4.  根据需要向 Pod 规约中添加 ​`runtimeClassName: windows-2019`​，例如：

`apiVersion: apps/v1 kind: Deployment metadata:   name: iis-2019   labels:     app: iis-2019 spec:   replicas: 1   template:     metadata:       name: iis-2019       labels:         app: iis-2019     spec:       runtimeClassName: windows-2019       containers:       - name: iis         image: mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2019         resources:           limits:             cpu: 1             memory: 800Mi           requests:             cpu: .1             memory: 300Mi         ports:           - containerPort: 80  selector:     matchLabels:       app: iis-2019 --- apiVersion: v1 kind: Service metadata:   name: iis spec:   type: LoadBalancer   ports:   - protocol: TCP     port: 80   selector:     app: iis-2019`

##  2.  Kubernetes 最佳实践

###  2.1.  Kubernetes 运行于多可用区环境
背景 
---

Kubernetes 从设计上允许同一个 Kubernetes 集群跨多个失效区来运行， 通常这些区位于某个称作 区域（region） 逻辑分组中。 主要的云提供商都将区域定义为一组失效区的集合（也称作 可用区（Availability Zones））， 能够提供一组一致的功能特性：每个区域内，各个可用区提供相同的 API 和服务。

典型的云体系结构都会尝试降低某个区中的失效影响到其他区中服务的概率。

控制面行为 
------

所有的控制面组件 都支持以一组可相互替换的资源池的形式来运行，每个组件都有多个副本。

当你部署集群控制面时，应将控制面组件的副本跨多个失效区来部署。 如果可用性是一个很重要的指标，应该选择至少三个失效区，并将每个 控制面组件（API 服务器、调度器、etcd、控制器管理器）复制多个副本， 跨至少三个失效区来部署。如果你在运行云控制器管理器，则也应该将 该组件跨所选的三个失效区来部署。

> Note:  
> Kubernetes 并不会为 API 服务器端点提供跨失效区的弹性。 你可以为集群 API 服务器使用多种技术来提升其可用性，包括使用 DNS 轮转、SRV 记录或者带健康检查的第三方负载均衡解决方案等等。

节点行为 
-----

Kubernetes 自动为负载资源（如Deployment 或 StatefulSet)） 跨集群中不同节点来部署其 Pods。 这种分布逻辑有助于降低失效带来的影响。

节点启动时，每个节点上的 kubelet 会向 Kubernetes API 中代表该 kubelet 的 Node 对象 添加 标签。 这些标签可能包含区信息。

如果你的集群跨了多个可用区或者地理区域，你可以使用节点标签，结合 Pod 拓扑分布约束 来控制如何在你的集群中多个失效域之间分布 Pods。这里的失效域可以是 地理区域、可用区甚至是特定节点。 这些提示信息使得调度器 能够更好地分布 Pods，以实现更好的可用性，降低因为某种失效给整个工作负载 带来的风险。

例如，你可以设置一种约束，确保某个 StatefulSet 中的三个副本都运行在 不同的可用区中，只要其他条件允许。你可以通过声明的方式来定义这种约束， 而不需要显式指定每个工作负载使用哪些可用区。

#### 跨多个区分布节点

Kubernetes 的核心逻辑并不会帮你创建节点，你需要自行完成此操作，或者使用 类似 [Cluster API](https://cluster-api.sigs.k8s.io/) 这类工具来替你管理节点。

使用类似 Cluster API 这类工具，你可以跨多个失效域来定义一组用做你的集群 工作节点的机器，以及当整个区的服务出现中断时如何自动治愈集群的策略。

为 Pods 手动指定区
------------

你可以应用节点选择算符约束 到你所创建的 Pods 上，或者为 Deployment、StatefulSet 或 Job 这类工作负载资源 中的 Pod 模板设置此类约束。

跨区的存储访问
-------

当创建持久卷时，​`PersistentVolumeLabel` ​准入控制器 会自动向那些链接到特定区的 PersistentVolume 添加区标签。 调度器通过其 ​`NoVolumeZoneConflict` ​断言确保申领给定 PersistentVolume 的 Pods 只会 被调度到该卷所在的可用区。

你可以为 PersistentVolumeClaim 指定StorageClass 以设置该类中的存储可以使用的失效域（区）。

网络 
---

Kubernetes 自身不提供与可用区相关的联网配置。 你可以使用网络插件 来配置集群的联网，该网络解决方案可能拥有一些与可用区相关的元素。 例如，如果你的云提供商支持 ​`type=LoadBalancer`​ 的 Service，则负载均衡器 可能仅会将请求流量发送到运行在负责处理给定连接的负载均衡器组件所在的区。 请查阅云提供商的文档了解详细信息。

对于自定义的或本地集群部署，也可以考虑这些因素 Service Ingress 的行为， 包括处理不同失效区的方法，在很大程度上取决于你的集群是如何搭建的。

失效恢复 
-----

在搭建集群时，你可能需要考虑当某区域中的所有失效区都同时掉线时，是否以及如何 恢复服务。例如，你是否要求在某个区中至少有一个节点能够运行 Pod？ 请确保任何对集群很关键的修复工作都不要指望集群中至少有一个健康节点。 例如：当所有节点都不健康时，你可能需要运行某个修复性的 Job， 该 Job 要设置特定的容忍度 以便修复操作能够至少将一个节点恢复为可用状态。

Kubernetes 对这类问题没有现成的解决方案；不过这也是要考虑的因素之一。

###  2.2.  Kubernetes 大规模集群的注意事项
大规模集群的注意事项
----------

集群是运行 Kubernetes 代理的、 由控制平面管理的一组 节点（物理机或虚拟机）。 Kubernetes v1.24 支持的最大节点数为 5000。 更具体地说，Kubernetes旨在适应满足以下所有标准的配置：

*   每个节点的 Pod 数量不超过 110
*   节点数不超过 5000
*   Pod 总数不超过 150000
*   容器总数不超过 300000

你可以通过添加或删除节点来扩展集群。集群扩缩的方式取决于集群的部署方式。

云供应商资源配额
--------

为避免遇到云供应商配额问题，在创建具有大规模节点的集群时，请考虑以下事项：

*   请求增加云资源的配额，例如：

*   计算实例
*   CPUs
*   存储卷
*   使用中的 IP 地址
*   数据包过滤规则集
*   负载均衡数量
*   网络子网
*   日志流

*   由于某些云供应商限制了创建新实例的速度，因此通过分批启动新节点来控制集群扩展操作，并在各批之间有一个暂停。

控制面组件
-----

对于大型集群，你需要一个具有足够计算能力和其他资源的控制平面。

通常，你将在每个故障区域运行一个或两个控制平面实例， 先垂直缩放这些实例，然后在到达下降点（垂直）后再水平缩放。

你应该在每个故障区域至少应运行一个实例，以提供容错能力。 Kubernetes 节点不会自动将流量引向相同故障区域中的控制平面端点。 但是，你的云供应商可能有自己的机制来执行此操作。

例如，使用托管的负载均衡器时，你可以配置负载均衡器发送源自故障区域 A 中的 kubelet 和 Pod 的流量， 并将该流量仅定向到也位于区域 A 中的控制平面主机。 如果单个控制平面主机或端点故障区域 A 脱机，则意味着区域 A 中的节点的所有控制平面流量现在都在区域之间发送。 在每个区域中运行多个控制平面主机能降低出现这种结果的可能性。

#### etcd 存储

为了提高大规模集群的性能，你可以将事件对象存储在单独的专用 etcd 实例中。

在创建集群时，你可以（使用自定义工具）：

*   启动并配置额外的 etcd 实例
*   配置 API 服务器，将它用于存储事件

#### 插件资源 

Kubernetes 资源限制 有助于最大程度地减少内存泄漏的影响以及 Pod 和容器可能对其他组件的其他方式的影响。 这些资源限制适用于插件资源， 就像它们适用于应用程序工作负载一样。

例如，你可以对日志组件设置 CPU 和内存限制

  `...   containers:   - name: fluentd-cloud-logging     image: fluent/fluentd-kubernetes-daemonset:v1     resources:       limits:         cpu: 100m         memory: 200Mi`

插件的默认限制通常基于从中小规模 Kubernetes 集群上运行每个插件的经验收集的数据。 插件在大规模集群上运行时，某些资源消耗常常比其默认限制更多。 如果在不调整这些值的情况下部署了大规模集群，则插件可能会不断被杀死，因为它们不断达到内存限制。 或者，插件可能会运行，但由于 CPU 时间片的限制而导致性能不佳。

为避免遇到集群插件资源问题，在创建大规模集群时，请考虑以下事项：

*   部分垂直扩展插件 —— 总有一个插件副本服务于整个集群或服务于整个故障区域。 对于这些附加组件，请在扩大集群时加大资源请求和资源限制。
*   许多水平扩展插件 —— 你可以通过运行更多的 Pod 来增加容量——但是在大规模集群下， 可能还需要稍微提高 CPU 或内存限制。 VerticalPodAutoscaler 可以在 recommender 模式下运行， 以提供有关请求和限制的建议数字。
*   一些插件在每个节点上运行一个副本，并由 DaemonSet 控制： 例如，节点级日志聚合器。与水平扩展插件的情况类似， 你可能还需要稍微提高 CPU 或内存限制。

###  2.3.  Kubernetes 校验节点设置
节点一致性测试 
--------

节点一致性测试 是一个容器化的测试框架，提供了针对节点的系统验证和功能测试。 测试验证节点是否满足 Kubernetes 的最低要求；通过测试的节点有资格加入 Kubernetes 集群。

该测试主要检测节点是否满足 Kubernetes 的最低要求，通过检测的节点有资格加入 Kubernetes 集群。

节点的前提条件 
--------

要运行节点一致性测试，节点必须满足与标准 Kubernetes 节点相同的前提条件。节点至少应安装以下守护程序：

*   容器运行时 (Docker)
*   Kubelet

运行节点一致性测试 
----------

要运行节点一致性测试，请执行以下步骤：

1.  得出 kubelet 的 ​`--kubeconfig`​ 的值；例如：​`--kubeconfig=/var/lib/kubelet/config.yaml`​。 由于测试框架启动了本地控制平面来测试 kubelet，因此使用 ​`http://localhost:8080`​ 作为API 服务器的 URL。 一些其他的 kubelet 命令行参数可能会被用到：

*   ​`--cloud-provider`​：如果使用 ​`--cloud-provider=gce`​，需要移除这个参数来运行测试。

3.  使用以下命令运行节点一致性测试：

`# $CONFIG_DIR 是你 Kubelet 的 pod manifest 路径。 # $LOG_DIR 是测试的输出路径。 sudo docker run -it --rm --privileged --net=host \   -v /:/rootfs -v $CONFIG_DIR:$CONFIG_DIR -v $LOG_DIR:/var/result \   k8s.gcr.io/node-test:0.2`

针对其他硬件体系结构运行节点一致性测试 
--------------------

Kubernetes 也为其他硬件体系结构的系统提供了节点一致性测试的 Docker 镜像：

架构

镜像

amd64

node-test-amd64

arm

node-test-arm

arm64

node-test-arm64

运行特定的测试 
--------

要运行特定测试，请使用你希望运行的测试的特定表达式覆盖环境变量 ​`FOCUS`​。

`sudo docker run -it --rm --privileged --net=host \   -v /:/rootfs:ro -v $CONFIG_DIR:$CONFIG_DIR -v $LOG_DIR:/var/result \   -e FOCUS=MirrorPod \ # Only run MirrorPod test   k8s.gcr.io/node-test:0.2`

要跳过特定的测试，请使用你希望跳过的测试的常规表达式覆盖环境变量 ​`SKIP`​。

`sudo docker run -it --rm --privileged --net=host \   -v /:/rootfs:ro -v $CONFIG_DIR:$CONFIG_DIR -v $LOG_DIR:/var/result \   -e SKIP=MirrorPod \ # 运行除 MirrorPod 测试外的所有一致性测试内容   k8s.gcr.io/node-test:0.2`

节点一致性测试是[节点端到端测试](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/e2e-node-tests.md)的容器化版本。

默认情况下，它会运行所有一致性测试。

理论上，只要合理地配置容器和挂载所需的卷，就可以运行任何的节点端到端测试用例。但是这里强烈建议只运行一致性测试，因为运行非一致性测试需要很多复杂的配置。

注意事项 
-----

*   测试会在节点上遗留一些 Docker 镜像，包括节点一致性测试本身的镜像和功能测试相关的镜像。
*   测试会在节点上遗留一些死的容器。这些容器是在功能测试的过程中创建的。

###  2.4.  Kubernetes PKI证书和要求
PKI 证书和要求
---------

Kubernetes 需要 PKI 证书才能进行基于 TLS 的身份验证。如果你是使用 kubeadm 安装的 Kubernetes， 则会自动生成集群所需的证书。你还可以生成自己的证书。 例如，不将私钥存储在 API 服务器上，可以让私钥更加安全。此页面说明了集群必需的证书。

集群是如何使用证书的 
-----------

Kubernetes 需要 PKI 才能执行以下操作：

*   Kubelet 的客户端证书，用于 API 服务器身份验证
*   Kubelet 服务端证书， 用于 API 服务器与 Kubelet 的会话
*   API 服务器端点的证书
*   集群管理员的客户端证书，用于 API 服务器身份认证
*   API 服务器的客户端证书，用于和 Kubelet 的会话
*   API 服务器的客户端证书，用于和 etcd 的会话
*   控制器管理器的客户端证书/kubeconfig，用于和 API 服务器的会话
*   调度器的客户端证书/kubeconfig，用于和 API 服务器的会话
*   前端代理 的客户端及服务端证书

> Note: 只有当你运行 kube-proxy 并要支持 扩展 API 服务器 时，才需要 ​`front-proxy`​ 证书  

etcd 还实现了双向 TLS 来对客户端和对其他对等节点进行身份验证。

证书存放的位置 
--------

假如通过 kubeadm 安装 Kubernetes，大多数证书都存储在 ​`/etc/kubernetes/pki`​。 本文档中的所有路径都是相对于该目录的，但用户账户证书除外，kubeadm 将其放在 ​`/etc/kubernetes`​ 中。

手动配置证书 
-------

如果你不想通过 kubeadm 生成这些必需的证书，你可以使用一个单一的根 CA 来创建这些证书或者直接提供所有证书。 

#### 单根 CA 

你可以创建一个单根 CA，由管理员控制器它。该根 CA 可以创建多个中间 CA，并将所有进一步的创建委托给 Kubernetes。

需要这些 CA：

路径

默认 CN

描述

ca.crt,key

kubernetes-ca

Kubernetes 通用 CA

etcd/ca.crt,key

etcd-ca

与 etcd 相关的所有功能

front-proxy-ca.crt,key

kubernetes-front-proxy-ca

用于 前端代理

上面的 CA 之外，还需要获取用于服务账户管理的密钥对，也就是 ​`sa.key`​ 和 ​`sa.pub`​。

下面的例子说明了上表中所示的 CA 密钥和证书文件。

`/etc/kubernetes/pki/ca.crt /etc/kubernetes/pki/ca.key /etc/kubernetes/pki/etcd/ca.crt /etc/kubernetes/pki/etcd/ca.key /etc/kubernetes/pki/front-proxy-ca.crt /etc/kubernetes/pki/front-proxy-ca.key`

#### 所有的证书 

如果你不想将 CA 的私钥拷贝至你的集群中，你也可以自己生成全部的证书。

需要这些证书：

默认 CN

父级 CA

O (位于 Subject 中)

类型

主机 (SAN)

kube-etcd

etcd-ca

server, client

`<hostname>`, `<Host_IP>`, `localhost`, `127.0.0.1`

kube-etcd-peer

etcd-ca

server, client

`<hostname>`, `<Host_IP>`, `localhost`, `127.0.0.1`

kube-etcd-healthcheck-client

etcd-ca

client

kube-apiserver-etcd-client

etcd-ca

system:masters

client

kube-apiserver

kubernetes-ca

server

`<hostname>`, `<Host_IP>`, `<advertise_IP>`, `[1]`

kube-apiserver-kubelet-client

kubernetes-ca

system:masters

client

front-proxy-client

kubernetes-front-proxy-ca

client

\[1\]: 用来连接到集群的不同 IP 或 DNS 名 （就像 kubeadm 为负载均衡所使用的固定 IP 或 DNS 名，​`kubernetes`​、​`kubernetes.default`​、​`kubernetes.default.svc`​、 ​`kubernetes.default.svc.cluster`​、​`kubernetes.default.svc.cluster.local`​）。

其中，​`kind` ​对应一种或多种类型的 [x509 密钥用途](https://pkg.go.dev/k8s.io/api/certificates/v1beta1 target=)：

kind

密钥用途

server

数字签名、密钥加密、服务端认证

client

数字签名、密钥加密、客户端认证

> Note:  
> 上面列出的 Hosts/SAN 是推荐的配置方式；如果需要特殊安装，则可以在所有服务器证书上添加其他 SAN。

> Note:  
> 对于 kubeadm 用户：  
> 
> *   不使用私钥，将证书复制到集群 CA 的方案，在 kubeadm 文档中将这种方案称为外部 CA。
> *   如果将以上列表与 kubeadm 生成的 PKI 进行比较，你会注意到，如果使用外部 etcd，则不会生成 ​`kube-etcd`​、​`kube-etcd-peer`​ 和 ​`kube-etcd-healthcheck-client`​ 证书。

#### 证书路径 

证书应放置在建议的路径中（以便 kubeadm 使用）。无论使用什么位置，都应使用给定的参数指定路径。

默认 CN

建议的密钥路径

建议的证书路径

命令

密钥参数

证书参数

etcd-ca

etcd/ca.key

etcd/ca.crt

kube-apiserver

\--etcd-cafile

kube-apiserver-etcd-client

apiserver-etcd-client.key

apiserver-etcd-client.crt

kube-apiserver

\--etcd-keyfile

\--etcd-certfile

kubernetes-ca

ca.key

ca.crt

kube-apiserver

\--client-ca-file

kubernetes-ca

ca.key

ca.crt

kube-controller-manager

\--cluster-signing-key-file

\--client-ca-file, --root-ca-file, --cluster-signing-cert-file

kube-apiserver

apiserver.key

apiserver.crt

kube-apiserver

\--tls-private-key-file

\--tls-cert-file

kube-apiserver-kubelet-client

apiserver-kubelet-client.key

apiserver-kubelet-client.crt

kube-apiserver

\--kubelet-client-key

\--kubelet-client-certificate

front-proxy-ca

front-proxy-ca.key

front-proxy-ca.crt

kube-apiserver

\--requestheader-client-ca-file

front-proxy-ca

front-proxy-ca.key

front-proxy-ca.crt

kube-controller-manager

\--requestheader-client-ca-file

front-proxy-client

front-proxy-client.key

front-proxy-client.crt

kube-apiserver

\--proxy-client-key-file

\--proxy-client-cert-file

etcd-ca

etcd/ca.key

etcd/ca.crt

etcd

\--trusted-ca-file, --peer-trusted-ca-file

kube-etcd

etcd/server.key

etcd/server.crt

etcd

\--key-file

\--cert-file

kube-etcd-peer

etcd/peer.key

etcd/peer.crt

etcd

\--peer-key-file

\--peer-cert-file

etcd-ca

etcd/ca.crt

etcdctl

\--cacert

kube-etcd-healthcheck-client

etcd/healthcheck-client.key

etcd/healthcheck-client.crt

etcdctl

\--key

\--cert

注意事项同样适用于服务帐户密钥对：

私钥路径

公钥路径

命令

参数

sa.key

kube-controller-manager

\--service-account-private-key-file

sa.pub

kube-apiserver

\--service-account-key-file

下面的例子展示了自行生成所有密钥和证书时所需要提供的文件路径。 这些路径基于前面的表格。

`/etc/kubernetes/pki/etcd/ca.key /etc/kubernetes/pki/etcd/ca.crt /etc/kubernetes/pki/apiserver-etcd-client.key /etc/kubernetes/pki/apiserver-etcd-client.crt /etc/kubernetes/pki/ca.key /etc/kubernetes/pki/ca.crt /etc/kubernetes/pki/apiserver.key /etc/kubernetes/pki/apiserver.crt /etc/kubernetes/pki/apiserver-kubelet-client.key /etc/kubernetes/pki/apiserver-kubelet-client.crt /etc/kubernetes/pki/front-proxy-ca.key /etc/kubernetes/pki/front-proxy-ca.crt /etc/kubernetes/pki/front-proxy-client.key /etc/kubernetes/pki/front-proxy-client.crt /etc/kubernetes/pki/etcd/server.key /etc/kubernetes/pki/etcd/server.crt /etc/kubernetes/pki/etcd/peer.key /etc/kubernetes/pki/etcd/peer.crt /etc/kubernetes/pki/etcd/healthcheck-client.key /etc/kubernetes/pki/etcd/healthcheck-client.crt /etc/kubernetes/pki/sa.key /etc/kubernetes/pki/sa.pub`

为用户帐户配置证书 
----------

你必须手动配置以下管理员帐户和服务帐户：

文件名

凭据名称

默认 CN

O (位于 Subject 中)

admin.conf

default-admin

kubernetes-admin

system:masters

kubelet.conf

default-auth

system:node:`<nodeName>` （参阅注释）

system:nodes

controller-manager.conf

default-controller-manager

system:kube-controller-manager

scheduler.conf

default-scheduler

system:kube-scheduler

> Note: ​`kubelet.conf`​ 中 ​`<nodeName>`​ 的值 必须 与 kubelet 向 apiserver 注册时提供的节点名称的值完全匹配。

1.  对于每个配置，请都使用给定的 CN 和 O 生成 x509 证书/密钥偶对。
2.  为每个配置运行下面的 ​`kubectl` ​命令：

`KUBECONFIG=<filename> kubectl config set-cluster default-cluster --server=https://<host ip>:6443 --certificate-authority <path-to-kubernetes-ca> --embed-certs KUBECONFIG=<filename> kubectl config set-credentials <credential-name> --client-key <path-to-key>.pem --client-certificate <path-to-cert>.pem --embed-certs KUBECONFIG=<filename> kubectl config set-context default-system --cluster default-cluster --user <credential-name> KUBECONFIG=<filename> kubectl config use-context default-system`

这些文件用途如下：

文件名

命令

说明

admin.conf

kubectl

配置集群的管理员

kubelet.conf

kubelet

集群中的每个节点都需要一份

controller-manager.conf

kube-controller-manager

必需添加到 `manifests/kube-controller-manager.yaml` 清单中

scheduler.conf

kube-scheduler

必需添加到 `manifests/kube-scheduler.yaml` 清单中

下面是前表中所列文件的完整路径。

`/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf`

###  2.5.  Kubernetes 强制实施Pod安全性标准
使用内置的 Pod 安全性准入控制器 
-------------------

FEATURE STATE: Kubernetes v1.23 \[beta\]

Pod 安全性准入控制器 尝试替换已被废弃的 PodSecurityPolicies。

#### 配置所有集群名字空间 

完全未经配置的名字空间应该被视为集群安全模型中的重大缺陷。 我们建议花一些时间来分析在每个名字空间中执行的负载的类型， 并通过引用 Pod 安全性标准来确定每个负载的合适级别。 未设置标签的名字空间应该视为尚未被评估。

针对所有名字空间中的所有负载都具有相同的安全性需求的场景， 我们提供了一个[示例](https://www.w3cschool.cn/kubernetes/kubernetes-pm913o6d.html) 用来展示如何批量应用 Pod 安全性标签。

#### 拥抱最小特权原则

在一个理想环境中，每个名字空间中的每个 Pod 都会满足 ​`restricted` ​策略的需求。 不过，这既不可能也不现实，某些负载会因为合理的原因而需要特权上的提升。

*   允许 ​`privileged` ​负载的名字空间需要建立并实施适当的访问控制机制。
*   对于运行在特权宽松的名字空间中的负载，需要维护其独特安全性需求的文档。 如果可能的话，要考虑如何进一步约束这些需求。

#### 采用多种模式的策略

Pod 安全性标准准入控制器的 ​`audit` ​和 ​`warn` ​模式（mode） 能够在不影响现有负载的前提下，让该控制器更方便地收集关于 Pod 的重要的安全信息。

针对所有名字空间启用这些模式是一种好的实践，将它们设置为你最终打算 ​`enforce` ​的 期望的 级别和版本。这一阶段中所生成的警告和审计注解信息可以帮助你到达这一状态。 如果你期望负载的作者能够作出变更以便适应期望的级别，可以启用 ​`warn` ​模式。 如果你希望使用审计日志了监控和驱动变更，以便负载能够适应期望的级别，可以启用 ​`audit` ​模式。

当你将 ​`enforce` ​模式设置为期望的取值时，这些模式在不同的场合下仍然是有用的：

*   通过将 ​`warn` ​设置为 ​`enforce` ​相同的级别，客户可以在尝试创建无法通过合法检查的 Pod （或者包含 Pod 模板的资源）时收到警告信息。这些信息会帮助于更新资源使其合规。
*   在将 ​`enforce` ​锁定到特定的非最新版本的名字空间中，将 ​`audit` ​和 ​`warn` ​模式设置为 ​`enforce` ​一样的级别而非 ​`latest` ​版本， 这样可以方便看到之前版本所允许但当前最佳实践中被禁止的设置。

第三方替代方案
-------

Kubernetes 生态系统中也有一些其他强制实施安全设置的替代方案处于开发状态中：

*   [Kubewarden](https://github.com/kubewarden)
*   [Kyverno](https://kyverno.io/policies/)
*   [OPA Gatekeeper](https://github.com/open-policy-agent/gatekeeper)

采用 内置的 方案（例如 PodSecurity 准入控制器）还是第三方工具， 这一决策完全取决于你自己的情况。在评估任何解决方案时，对供应链的信任都是至关重要的。 最终，使用前述方案中的 任何 一种都好过放任自流。

#  2.  Kubernetes 概述

##  1.  Kubernetes 简介
简介
--

Kubernetes 是一个可移植的、可扩展的开源平台，用于管理容器化的工作负载和服务，可促进声明式配置和自动化。 Kubernetes 拥有一个庞大且快速增长的生态系统。Kubernetes 的服务、支持和工具广泛可用。

Kubernetes 这个名字源于希腊语，意为“舵手”或“飞行员”。k8s 这个缩写是因为 k 和 s 之间有八个字符的关系。 Google 在 2014 年开源了 Kubernetes 项目。Kubernetes 建立在 [Google 在大规模运行生产工作负载方面拥有十几年的经验](https://research.google/pubs/pub43438/) 的基础上，结合了社区中最好的想法和实践。

时光回溯
----

让我们回顾一下为什么 Kubernetes 如此有用。

![](https://atts.w3cschool.cn/attachments/image/20220428/1651109065276234.jpg)  

### 传统部署时代：

早期，各个组织机构在物理服务器上运行应用程序。无法为物理服务器中的应用程序定义资源边界，这会导致资源分配问题。 例如，如果在物理服务器上运行多个应用程序，则可能会出现一个应用程序占用大部分资源的情况， 结果可能导致其他应用程序的性能下降。 一种解决方案是在不同的物理服务器上运行每个应用程序，但是由于资源利用不足而无法扩展， 并且维护许多物理服务器的成本很高。

### 虚拟化部署时代：

作为解决方案，引入了虚拟化。虚拟化技术允许你在单个物理服务器的 CPU 上运行多个虚拟机（VM）。 虚拟化允许应用程序在 VM 之间隔离，并提供一定程度的安全，因为一个应用程序的信息 不能被另一应用程序随意访问。

虚拟化技术能够更好地利用物理服务器上的资源，并且因为可轻松地添加或更新应用程序 而可以实现更好的可伸缩性，降低硬件成本等等。

每个 VM 是一台完整的计算机，在虚拟化硬件之上运行所有组件，包括其自己的操作系统。

### 容器部署时代：

容器类似于 VM，但是它们具有被放宽的隔离属性，可以在应用程序之间共享操作系统（OS）。 因此，容器被认为是轻量级的。容器与 VM 类似，具有自己的文件系统、CPU、内存、进程空间等。 由于它们与基础架构分离，因此可以跨云和 OS 发行版本进行移植。

容器因具有许多优势而变得流行起来。下面列出的是容器的一些好处：

*   敏捷应用程序的创建和部署：与使用 VM 镜像相比，提高了容器镜像创建的简便性和效率。
*   持续开发、集成和部署：通过快速简单的回滚（由于镜像不可变性），支持可靠且频繁的 容器镜像构建和部署。
*   关注开发与运维的分离：在构建/发布时而不是在部署时创建应用程序容器镜像， 从而将应用程序与基础架构分离。
*   可观察性：不仅可以显示操作系统级别的信息和指标，还可以显示应用程序的运行状况和其他指标信号。
*   跨开发、测试和生产的环境一致性：在便携式计算机上与在云中相同地运行。
*   跨云和操作系统发行版本的可移植性：可在 Ubuntu、RHEL、CoreOS、本地、 Google Kubernetes Engine 和其他任何地方运行。
*   以应用程序为中心的管理：提高抽象级别，从在虚拟硬件上运行 OS 到使用逻辑资源在 OS 上运行应用程序。
*   松散耦合、分布式、弹性、解放的微服务：应用程序被分解成较小的独立部分， 并且可以动态部署和管理 - 而不是在一台大型单机上整体运行。
*   资源隔离：可预测的应用程序性能。
*   资源利用：高效率和高密度。

为什么需要 Kubernetes，它能做什么?
-----------------------

容器是打包和运行应用程序的好方式。在生产环境中，你需要管理运行应用程序的容器，并确保不会停机。 例如，如果一个容器发生故障，则需要启动另一个容器。如果系统处理此行为，会不会更容易？

这就是 Kubernetes 来解决这些问题的方法！ Kubernetes 为你提供了一个可弹性运行分布式系统的框架。 Kubernetes 会满足你的扩展要求、故障转移、部署模式等。 例如，Kubernetes 可以轻松管理系统的 Canary 部署。

Kubernetes 为你提供：

*   服务发现和负载均衡
Kubernetes 可以使用 DNS 名称或自己的 IP 地址公开容器，如果进入容器的流量很大， Kubernetes 可以负载均衡并分配网络流量，从而使部署稳定。

*   存储编排

Kubernetes 允许你自动挂载你选择的存储系统，例如本地存储、公共云提供商等。

*   自动部署和回滚
    

你可以使用 Kubernetes 描述已部署容器的所需状态，它可以以受控的速率将实际状态 更改为期望状态。例如，你可以自动化 Kubernetes 来为你的部署创建新容器， 删除现有容器并将它们的所有资源用于新容器。

*   自动完成装箱计算
    

Kubernetes 允许你指定每个容器所需 CPU 和内存（RAM）。 当容器指定了资源请求时，Kubernetes 可以做出更好的决策来管理容器的资源。

*   自我修复
    

Kubernetes 重新启动失败的容器、替换容器、杀死不响应用户定义的 运行状况检查的容器，并且在准备好服务之前不将其通告给客户端。

*   密钥与配置管理
    

Kubernetes 允许你存储和管理敏感信息，例如密码、OAuth 令牌和 ssh 密钥。 你可以在不重建容器镜像的情况下部署和更新密钥和应用程序配置，也无需在堆栈配置中暴露密钥。

Kubernetes 不是什么
---------------

Kubernetes 不是传统的、包罗万象的 PaaS（平台即服务）系统。 由于 Kubernetes 在容器级别而不是在硬件级别运行，它提供了 PaaS 产品共有的一些普遍适用的功能， 例如部署、扩展、负载均衡、日志记录和监视。 但是，Kubernetes 不是单体系统，默认解决方案都是可选和可插拔的。 Kubernetes 提供了构建开发人员平台的基础，但是在重要的地方保留了用户的选择和灵活性。

Kubernetes：

*   不限制支持的应用程序类型。 Kubernetes 旨在支持极其多种多样的工作负载，包括无状态、有状态和数据处理工作负载。 如果应用程序可以在容器中运行，那么它应该可以在 Kubernetes 上很好地运行。
*   不部署源代码，也不构建你的应用程序。 持续集成(CI)、交付和部署（CI/CD）工作流取决于组织的文化和偏好以及技术要求。
*   不提供应用程序级别的服务作为内置服务，例如中间件（例如，消息中间件）、 数据处理框架（例如，Spark）、数据库（例如，mysql）、缓存、集群存储系统 （例如，Ceph）。这样的组件可以在 Kubernetes 上运行，并且/或者可以由运行在 Kubernetes 上的应用程序通过可移植机制（例如， [开放服务代理](https://www.openservicebrokerapi.org/)）来访问。
*   不要求日志记录、监视或警报解决方案。 它提供了一些集成作为概念证明，并提供了收集和导出指标的机制。
*   不提供或不要求配置语言/系统（例如 jsonnet），它提供了声明性 API， 该声明性 API 可以由任意形式的声明性规范所构成。
*   不提供也不采用任何全面的机器配置、维护、管理或自我修复系统。
*   此外，Kubernetes 不仅仅是一个编排系统，实际上它消除了编排的需要。 编排的技术定义是执行已定义的工作流程：首先执行 A，然后执行 B，再执行 C。 相比之下，Kubernetes 包含一组独立的、可组合的控制过程， 这些过程连续地将当前状态驱动到所提供的所需状态。 如何从 A 到 C 的方式无关紧要，也不需要集中控制，这使得系统更易于使用 且功能更强大、系统更健壮、更为弹性和可扩展。

##  2.  Kubernetes 组件
组件
--

当你部署完 Kubernetes, 即拥有了一个完整的集群。

一个 Kubernetes 集群由一组被称作节点的机器组成。这些节点上运行 Kubernetes 所管理的容器化应用。集群具有至少一个工作节点。

工作节点托管作为应用负载的组件的 Pod 。控制平面管理集群中的工作节点和 Pod 。 为集群提供故障转移和高可用性，这些控制平面一般跨多主机运行，集群跨多个节点运行。

本文档概述了交付正常运行的 Kubernetes 集群所需的各种组件。

![](https://atts.w3cschool.cn/attachments/image/20220428/1651110837317224.svg)  

控制平面组件（Control Plane Components）
--------------------------------

控制平面的组件对集群做出全局决策(比如调度)，以及检测和响应集群事件（例如，当不满足部署的 ​`replicas` ​字段时，启动新的 pod）。

控制平面组件可以在集群中的任何节点上运行。 然而，为了简单起见，设置脚本通常会在同一个计算机上启动所有控制平面组件， 并且不会在此计算机上运行用户容器。 

### kube-apiserver

API 服务器是 Kubernetes 控制面的组件， 该组件公开了 Kubernetes API。 API 服务器是 Kubernetes 控制面的前端。

Kubernetes API 服务器的主要实现是 kube-apiserver。 kube-apiserver 设计上考虑了水平伸缩，也就是说，它可通过部署多个实例进行伸缩。 你可以运行 kube-apiserver 的多个实例，并在这些实例之间平衡流量。

### etcd

etcd 是兼具一致性和高可用性的键值数据库，可以作为保存 Kubernetes 所有集群数据的后台数据库。

您的 Kubernetes 集群的 etcd 数据库通常需要有个备份计划。

### kube-scheduler

控制平面组件，负责监视新创建的、未指定运行节点（node）的 Pods，选择节点让 Pod 在上面运行。

调度决策考虑的因素包括单个 Pod 和 Pod 集合的资源需求、硬件/软件/策略约束、亲和性和反亲和性规范、数据位置、工作负载间的干扰和最后时限。

### kube-controller-manager

运行控制器进程的控制平面组件。

从逻辑上讲，每个控制器都是一个单独的进程， 但是为了降低复杂性，它们都被编译到同一个可执行文件，并在一个进程中运行。

这些控制器包括:

*   节点控制器（Node Controller）: 负责在节点出现故障时进行通知和响应
*   任务控制器（Job controller）: 监测代表一次性任务的 Job 对象，然后创建 Pods 来运行这些任务直至完成
*   端点控制器（Endpoints Controller）: 填充端点(Endpoints)对象(即加入 Service 与 Pod)
*   服务帐户和令牌控制器（Service Account & Token Controllers）: 为新的命名空间创建默认帐户和 API 访问令牌

### cloud-controller-manager

云控制器管理器是指嵌入特定云的控制逻辑的 控制平面组件。 云控制器管理器使得你可以将你的集群连接到云提供商的 API 之上， 并将与该云平台交互的组件同与你的集群交互的组件分离开来。

​`cloud-controller-manager`​ 仅运行特定于云平台的控制回路。 如果你在自己的环境中运行 Kubernetes，或者在本地计算机中运行学习环境， 所部署的环境中不需要云控制器管理器。

与 ​`kube-controller-manager`​ 类似，​`cloud-controller-manager`​ 将若干逻辑上独立的 控制回路组合到同一个可执行文件中，供你以同一进程的方式运行。 你可以对其执行水平扩容（运行不止一个副本）以提升性能或者增强容错能力。

下面的控制器都包含对云平台驱动的依赖：

*   节点控制器（Node Controller）: 用于在节点终止响应后检查云提供商以确定节点是否已被删除
*   路由控制器（Route Controller）: 用于在底层云基础架构中设置路由
*   服务控制器（Service Controller）: 用于创建、更新和删除云提供商负载均衡器

Node 组件 
--------

节点组件在每个节点上运行，维护运行的 Pod 并提供 Kubernetes 运行环境。

### kubelet

一个在集群中每个节点（node）上运行的代理。 它保证容器（containers）都 运行在 Pod 中。

kubelet 接收一组通过各类机制提供给它的 PodSpecs，确保这些 PodSpecs 中描述的容器处于运行状态且健康。 kubelet 不会管理不是由 Kubernetes 创建的容器。

### kube-proxy

kube-proxy 是集群中每个节点上运行的网络代理， 实现 Kubernetes 服务（Service） 概念的一部分。

kube-proxy 维护节点上的网络规则。这些网络规则允许从集群内部或外部的网络会话与 Pod 进行网络通信。

如果操作系统提供了数据包过滤层并可用的话，kube-proxy 会通过它来实现网络规则。否则， kube-proxy 仅转发流量本身。

### 容器运行时（Container Runtime） 

容器运行环境是负责运行容器的软件。

Kubernetes 支持容器运行时，例如 Docker、 containerd、CRI-O 以及 [Kubernetes CRI (容器运行环境接口)](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md) 的其他任何实现。

插件（Addons） 
-----------

插件使用 Kubernetes 资源（DaemonSet、 Deployment等）实现集群功能。 因为这些插件提供集群级别的功能，插件中命名空间域的资源属于 ​`kube-system`​ 命名空间。

下面描述众多插件中的几种。

### DNS 

尽管其他插件都并非严格意义上的必需组件，但几乎所有 Kubernetes 集群都应该 有集群 DNS， 因为很多示例都需要 DNS 服务。

集群 DNS 是一个 DNS 服务器，和环境中的其他 DNS 服务器一起工作，它为 Kubernetes 服务提供 DNS 记录。

Kubernetes 启动的容器自动将此 DNS 服务器包含在其 DNS 搜索列表中。

### Web 界面（仪表盘）

Dashboard 是 Kubernetes 集群的通用的、基于 Web 的用户界面。 它使用户可以管理集群中运行的应用程序以及集群本身并进行故障排除。

### 容器资源监控

容器资源监控 将关于容器的一些常见的时间序列度量值保存到一个集中的数据库中，并提供用于浏览这些数据的界面。

### 集群层面日志

集群层面日志 机制负责将容器的日志数据 保存到一个集中的日志存储中，该存储能够提供搜索和浏览接口。

##  3.  Kubernetes API
API
---

Kubernetes 控制面 的核心是 API 服务器。 API 服务器负责提供 HTTP API，以供用户、集群中的不同部分和集群外部组件相互通信。

Kubernetes API 使你可以查询和操纵 Kubernetes API 中对象（例如：Pod、Namespace、ConfigMap 和 Event）的状态。

大部分操作都可以通过 kubectl 命令行接口或 类似 kubeadm 这类命令行工具来执行， 这些工具在背后也是调用 API。不过，你也可以使用 REST 调用来访问这些 API。

如果你正在编写程序来访问 Kubernetes API，可以考虑使用 客户端库之一。

OpenAPI 规范
----------

完整的 API 细节是用 [OpenAPI](https://www.openapis.org/) 来表述的。

### OpenAPI V2

Kubernetes API 服务器通过 ​`/openapi/v2`​ 端点提供聚合的 OpenAPI v2 规范。 你可以按照下表所给的请求头部，指定响应的格式：

头部

可选值

说明

`Accept-Encoding`

`gzip`

_不指定此头部也是可以的_

`Accept`

`application/com.github.proto-openapi.spec.v2@v1.0+protobuf`

_主要用于集群内部_

`application/json`

_默认值_

`*`

_提供_`application/json`

OpenAPI v2 查询请求的合法头部值

Kubernetes 为 API 实现了一种基于 Protobuf 的序列化格式，主要用于集群内部通信。 关于此格式的详细信息，可参考 [Kubernetes Protobuf 序列化](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/protobuf.md) 设计提案。每种模式对应的接口描述语言（IDL）位于定义 API 对象的 Go 包中。

### OpenAPI V3

FEATURE STATE: Kubernetes v1.23 \[alpha\]

Kubernetes v1.23 提供将其 API 以 OpenAPI v3 形式发布的初始支持；这一功能特性处于 Alpha 状态，默认被禁用。 你可以通过为 kube-apiserver 组件启用 ​`OpenAPIV3` ​特性门控来启用此 Alpha 特性。

特性被启用时，Kubernetes API 服务器会在端点 ​`/openapi/v3/apis/<group>/<version>`​ 提供按 Kubernetes 组版本聚合的 OpenAPI v3 规范。 请参阅下表了解可接受的请求头部。

头部

可选值

说明

`Accept-Encoding`

`gzip`

_不提供此头部也是可接受的_

`Accept`

`application/com.github.proto-openapi.spec.v3@v1.0+protobuf`

_主要用于集群内部使用_

`application/json`

_默认_

`*`

_以_ `application/json` 形式返回

发现端点 ​`/openapi/v3`​ 被提供用来查看可用的所有组、版本列表。 此列表仅返回 JSON。

API 变更 
-------

任何成功的系统都要随着新的使用案例的出现和现有案例的变化来成长和变化。 为此，Kubernetes 的功能特性设计考虑了让 Kubernetes API 能够持续变更和成长的因素。 Kubernetes 项目的目标是 不要 引发现有客户端的兼容性问题，并在一定的时期内 维持这种兼容性，以便其他项目有机会作出适应性变更。

一般而言，新的 API 资源和新的资源字段可以被频繁地添加进来。 删除资源或者字段则要遵从 API 废弃策略。

Kubernetes 对维护达到正式发布（GA）阶段的官方 API 的兼容性有着很强的承诺， 通常这一 API 版本为 v1。此外，Kubernetes 在可能的时候还会保持 Beta API 版本的兼容性：如果你采用了 Beta API，你可以继续在集群上使用该 API， 即使该功能特性已进入稳定期也是如此。

> Note:  
> 尽管 Kubernetes 也努力为 Alpha API 版本维护兼容性，在有些场合兼容性是无法做到的。 如果你使用了任何 Alpha API 版本，需要在升级集群时查看 Kubernetes 发布说明， 以防 API 的确发生变更。

API 扩展 
-------

有两种途径来扩展 Kubernetes API：

1.  你可以使用自定义资源 来以声明式方式定义 API 服务器如何提供你所选择的资源 API。
2.  你也可以选择实现自己的 聚合层 来扩展 Kubernetes API。

#  3.  Kubernetes 安装

##  1.  Kubernetes Linux安装
kubectl 版本和集群版本之间的差异必须在一个小版本号内。 例如：v1.23 版本的客户端能与 v1.22、 v1.23 和 v1.24 版本的控制面通信。 用最新兼容版的 kubectl 有助于避免不可预见的问题。

在 Linux 系统中安装 kubectl
---------------------

在 Linux 系统中安装 kubectl 有如下几种方法：

*   用 curl 在 Linux 系统中安装 kubectl
*   用原生包管理工具安装
*   用其他包管理工具安装

用 curl 在 Linux 系统中安装 kubectl 
-----------------------------

1、用以下命令下载最新发行版：  

`curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"`

> Note:  
> 如需下载某个指定的版本，请用指定版本号替换该命令的这一部分：​ `$(curl -L -s https://dl.k8s.io/release/stable.txt)。` ​  
> 例如，要在 Linux 中下载 v1.23.0 版本，请输入：  
> 
> `curl -LO https://dl.k8s.io/release/v1.23.0/bin/linux/amd64/kubectl`

2、验证该可执行文件（可选步骤）  

*   下载 kubectl 校验和文件：

`curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"`

*   基于校验和文件，验证 kubectl 的可执行文件：

`echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check`

*   验证通过时，输出为：

`kubectl: OK`

*   验证失败时，sha256 将以非零值退出，并打印如下输出：

`kubectl: FAILED sha256sum: WARNING: 1 computed checksum did NOT match`

> 下载的 kubectl 与校验和文件版本必须相同。

3、安装 kubectl

`sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl`

> 即使你没有目标系统的 root 权限，仍然可以将 kubectl 安装到目录 ~/.local/bin 中：
> 
> `chmod +x kubectl mkdir -p ~/.local/bin mv ./kubectl ~/.local/bin/kubectl # 之后将 ~/.local/bin 附加（或前置）到 $PATH`

4、执行测试，以保障你安装的版本是最新的：

`kubectl version --client`

*   或者使用如下命令来查看版本的详细信息：

`kubectl version --client --output=yaml`

用原生包管理工具安装
----------

### Ubuntu、Debian 或 HypriotOS

1、更新 ​`apt` ​包索引，并安装使用 Kubernetes apt 仓库所需要的包：

`sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl`

2、下载 Google Cloud 公开签名秘钥：

`sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg`

3、添加 Kubernetes ​`apt` ​仓库：

`echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list`

4、更新 ​`apt` ​包索引，使之包含新的仓库并安装 kubectl：

`sudo apt-get update sudo apt-get install -y kubectl`

### 基于 Red Hat 的发行版

`cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF sudo yum install -y kubectl`

用其他包管理工具安装
----------

### Snap

如果你使用的 Ubuntu 或其他 Linux 发行版，内建支持 [snap](https://snapcraft.io/docs/installing-snapd) 包管理工具， 则可用 [snap](https://snapcraft.io/) 命令安装 kubectl。

`snap install kubectl --classic kubectl version --client`

### Homebrew

如果你使用 Linux 系统，并且装了 [Homebrew](https://docs.brew.sh/Homebrew-on-Linux) 包管理工具， 则可以使用这种方式安装 kubectl。

`brew install kubectl kubectl version --client`

验证 kubectl 配置 
--------------

为了让 kubectl 能发现并访问 Kubernetes 集群，你需要一个 kubeconfig 文件， 该文件在 [kube-up.sh](https://github.com/kubernetes/kubernetes/blob/master/cluster/kube-up.sh) 创建集群时，或成功部署一个 Miniube 集群时，均会自动生成。 通常，kubectl 的配置信息存放于文件 ​`~/.kube/config`​ 中。

通过获取集群状态的方法，检查是否已恰当的配置了 kubectl：

`kubectl cluster-info`

如果返回一个 URL，则意味着 kubectl 成功的访问到了你的集群。

如果你看到如下所示的消息，则代表 kubectl 配置出了问题，或无法连接到 Kubernetes 集群。

`The connection to the server <server-name:port> was refused - did you specify the right host or port? （访问 <server-name:port> 被拒绝 - 你指定的主机和端口是否有误？）`

例如，如果你想在自己的笔记本上（本地）运行 Kubernetes 集群，你需要先安装一个 Minikube 这样的工具，然后再重新运行上面的命令。

如果命令 ​`kubectl cluster-info`​ 返回了 url，但你还不能访问集群，那可以用以下命令来检查配置是否妥当：

`kubectl cluster-info dump`

kubectl 的可选配置和插件
----------------

### 启用 shell 自动补全功能

kubectl 为 Bash、Zsh、Fish 和 PowerShell 提供自动补全功能，可以为你节省大量的输入。

下面是为 Bash、Fish 和 Zsh 设置自动补全功能的操作步骤。

### Bash

kubectl 的 Bash 补全脚本可以用命令 ​`kubectl completion bash`​ 生成。 在 shell 中导入（Sourcing）补全脚本，将启用 kubectl 自动补全功能。

然而，补全脚本依赖于工具 [bash-completion](https://github.com/scop/bash-completion)， 所以要先安装它（可以用命令 ​`type _init_completion`​ 检查 bash-completion 是否已安装）。

#### 安装 bash-completion

很多包管理工具均支持 bash-completion（参见[这里](https://github.com/scop/bash-completion target=)）。 可以通过 ​`apt-get install bash-completion`​ 或 ​`yum install bash-completion`​ 等命令来安装它。

上述命令将创建文件 ​`/usr/share/bash-completion/bash_completion`​，它是 bash-completion 的主脚本。 依据包管理工具的实际情况，你需要在 ​`~/.bashrc`​ 文件中手工导入此文件。

要查看结果，请重新加载你的 shell，并运行命令 ​`type _init_completion`​。 如果命令执行成功，则设置完成，否则将下面内容添加到文件 ​`~/.bashrc`​ 中：

`source /usr/share/bash-completion/bash_completion`

重新加载 shell，再输入命令 ​`type _init_completion`​ 来验证 bash-completion 的安装状态。

#### 启动 kubectl 自动补全功能 

你现在需要确保一点：kubectl 补全脚本已经导入（sourced）到 shell 会话中。 可以通过以下两种方法进行设置：

*   当前用户

`echo 'source <(kubectl completion bash)' >>~/.bashrc`

*   系统全局

`kubectl completion bash | sudo tee /etc/bash_completion.d/kubectl > /dev/null`

如果 kubectl 有关联的别名，你可以扩展 shell 补全来适配此别名：

`echo 'alias k=kubectl' >>~/.bashrc echo 'complete -F __start_kubectl k' >>~/.bashrc`

> bash-completion 负责导入 ​`/etc/bash_completion.d`​ 目录中的所有补全脚本。

两种方式的效果相同。重新加载 shell 后，kubectl 自动补全功能即可生效。

### Fish

kubectl 通过命令 ​`kubectl completion fish`​ 生成 Fish 自动补全脚本。 在 shell 中导入（Sourcing）该自动补全脚本，将启动 kubectl 自动补全功能。

为了在所有的 shell 会话中实现此功能，请将下面内容加入到文件 ​`~/.config/fish/config.fish`​ 中。

`kubectl completion fish | source`

重新加载 shell 后，kubectl 自动补全功能将立即生效。

### Zsh

kubectl 通过命令 ​`kubectl completion zsh`​ 生成 Zsh 自动补全脚本。 在 shell 中导入（Sourcing）该自动补全脚本，将启动 kubectl 自动补全功能。

为了在所有的 shell 会话中实现此功能，请将下面内容加入到文件 ​`~/.zshrc`​ 中。

`source <(kubectl completion zsh)`

如果你为 kubectl 定义了别名，kubectl 自动补全将自动使用它。

重新加载 shell 后，kubectl 自动补全功能将立即生效。

如果你收到 ​`2: command not found: compdef`​ 这样的错误提示，那请将下面内容添加到 ​`~/.zshrc`​ 文件的开头：

`autoload -Uz compinit compinit`

### 安装 kubectl convert 插件

一个 Kubernetes 命令行工具 ​`kubectl` ​的插件，允许你将清单在不同 API 版本间转换。 这对于将清单迁移到新的 Kubernetes 发行版上未被废弃的 API 版本时尤其有帮助。

1、用以下命令下载最新发行版：

`curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl-convert"`

2、验证该可执行文件（可选步骤）

*   下载 kubectl-convert 校验和文件：

`curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl-convert.sha256"`

*   基于校验和，验证 kubectl-convert 的可执行文件：

`echo "$(cat kubectl-convert.sha256) kubectl-convert" | sha256sum --check`

*   验证通过时，输出为：

`kubectl-convert: OK`

验证失败时，​`sha256` ​将以非零值退出，并打印输出类似于：

`kubectl-convert: FAILED sha256sum: WARNING: 1 computed checksum did NOT match`

> 下载相同版本的可执行文件和校验和。

3、安装 kubectl-convert

`sudo install -o root -g root -m 0755 kubectl-convert /usr/local/bin/kubectl-convert`

4、验证插件是否安装成功

`kubectl convert --help`

如果你没有看到任何错误就代表插件安装成功了。

##  2.  Kubernetes macOS安装
kubectl 版本和集群之间的差异必须在一个小版本号之内。 例如：v1.23 版本的客户端能与 v1.22、 v1.23 和 v1.24 版本的控制面通信。 用最新兼容版本的 kubectl 有助于避免不可预见的问题。

在 macOS 系统上安装 kubectl
---------------------

在 macOS 系统上安装 kubectl 有如下方法：

*   用 curl 在 macOS 系统上安装 kubectl
*   用 Homebrew 在 macOS 系统上安装
*   用 Macports 在 macOS 上安装
*   作为谷歌云 SDK 的一部分，在 macOS 上安装

用 curl 在 macOS 系统上安装 kubectl 
-----------------------------

1、下载最新的发行版：

*   Intel

   `curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/amd64/kubectl"`

*   Apple Silicon

   `curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/arm64/kubectl"`

> 如果需要下载某个指定的版本，用该指定版本号替换掉命令的这个部分：​`$(curl -L -s https://dl.k8s.io/release/stable.txt)`​。 例如：要为 Intel macOS 系统下载 v1.23.0 版本，则输入：
> 
> `curl -LO "https://dl.k8s.io/release/v1.23.0/bin/darwin/amd64/kubectl"`
> 
> 对于 Apple Silicon 版本的 macOS，输入：
> 
> `curl -LO "https://dl.k8s.io/release/v1.23.0/bin/darwin/arm64/kubectl"`

  

2、验证可执行文件（可选操作）

下载 kubectl 的校验和文件：

*   Intel

   `curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/amd64/kubectl.sha256"`

*   Apple Silicon

   `curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/arm64/kubectl.sha256"`

*   根据校验和文件，验证 kubectl：

`echo "$(cat kubectl.sha256)  kubectl" | shasum -a 256 --check`

*   验证通过时，输出如下：

`kubectl: OK`

*   验证失败时，​`shasum` ​将以非零值退出，并打印如下输出：

`kubectl: FAILED shasum: WARNING: 1 computed checksum did NOT match`

> 下载的 kubectl 与校验和文件版本要相同。

3、将 kubectl 置为可执行文件：

`chmod +x ./kubectl`

4、将可执行文件 kubectl 移动到系统可寻址路径 ​`PATH` ​内的一个位置：

`sudo mv ./kubectl /usr/local/bin/kubectl sudo chown root: /usr/local/bin/kubectl`

> 确保 ​`/usr/local/bin`​ 在你的 PATH 环境变量中。

5、测试一下，确保你安装的是最新的版本：

`kubectl version --client`

*   或者使用下面命令来查看版本的详细信息：

`kubectl version --client --output=yaml`

用 Homebrew 在 macOS 系统上安装
------------------------

如果你是 macOS 系统，且用的是 [Homebrew](https://brew.sh/) 包管理工具， 则可以用 Homebrew 安装 kubectl。

1、运行安装命令：

`brew install kubectl` 

*   或

`brew install kubernetes-cli`

2、测试一下，确保你安装的是最新的版本：

`kubectl version --client`

用 Macports 在 macOS 上安装
----------------------

如果你用的是 macOS，且用 [Macports](https://macports.org/) 包管理工具，则你可以用 Macports 安装kubectl。

1、运行安装命令：

`sudo port selfupdate sudo port install kubectl`

2、测试一下，确保你安装的是最新的版本：

`kubectl version --client`

验证 kubectl 配置
-------------

为了让 kubectl 能发现并访问 Kubernetes 集群，你需要一个 kubeconfig 文件， 该文件在 [kube-up.sh](https://github.com/kubernetes/kubernetes/blob/master/cluster/kube-up.sh) 创建集群时，或成功部署一个 Miniube 集群时，均会自动生成。 通常，kubectl 的配置信息存放于文件 ​`~/.kube/config`​ 中。

通过获取集群状态的方法，检查是否已恰当的配置了 kubectl：

`kubectl cluster-info`

如果返回一个 URL，则意味着 kubectl 成功的访问到了你的集群。

如果你看到如下所示的消息，则代表 kubectl 配置出了问题，或无法连接到 Kubernetes 集群。

`The connection to the server <server-name:port> was refused - did you specify the right host or port? （访问 <server-name:port> 被拒绝 - 你指定的主机和端口是否有误？）`

例如，如果你想在自己的笔记本上（本地）运行 Kubernetes 集群，你需要先安装一个 Minikube 这样的工具，然后再重新运行上面的命令。

如果命令 ​`kubectl cluster-info`​ 返回了 url，但你还不能访问集群，那可以用以下命令来检查配置是否妥当：

`kubectl cluster-info dump`

可选的 kubectl 配置和插件
-----------------

### 启用 shell 自动补全功能

kubectl 为 Bash、Zsh、Fish 和 PowerShell 提供自动补全功能，可以为你节省大量的输入。

下面是为 Bash、Fish 和 Zsh 设置自动补全功能的操作步骤。

### Bash

kubectl 的 Bash 补全脚本可以通过 ​`kubectl completion bash`​ 命令生成。 在你的 shell 中导入（Sourcing）这个脚本即可启用补全功能。

此外，kubectl 补全脚本依赖于工具 [bash-completion](https://github.com/scop/bash-completion)， 所以你必须先安装它。

> **Warning**:  
> bash-completion 有两个版本：v1 和 v2。v1 对应 Bash3.2（也是 macOS 的默认安装版本），v2 对应 Bash 4.1+。 kubectl 的补全脚本**无法适配** bash-completion v1 和 Bash 3.2。 必须为它配备 **bash-completion v2** 和 **Bash 4.1+**。 有鉴于此，为了在 macOS 上使用 kubectl 补全功能，你必须要安装和使用 Bash 4.1+ ([说明](https://itnext.io/upgrading-bash-on-macos-7138bd1066ba?gi=9cba5d635a48))。 后续说明假定你用的是 Bash 4.1+（也就是 Bash 4.1 或更新的版本）

#### 升级 Bash

后续说明假定你已使用 Bash 4.1+。你可以运行以下命令检查 Bash 版本：

`echo $BASH_VERSION`

如果版本太旧，可以用 Homebrew 安装/升级：

`brew install bash`

重新加载 shell，并验证所需的版本已经生效：

`echo $BASH_VERSION $SHELL`

Homebrew 通常把它安装为 ​`/usr/local/bin/bash`​。

#### 安装 bash-completion 

> 如前所述，本说明假定你使用的 Bash 版本为 4.1+，这意味着你要安装 bash-completion v2 （不同于 Bash 3.2 和 bash-completion v1，kubectl 的补全功能在该场景下无法工作）。

你可以用命令 ​`type _init_completion`​ 测试 bash-completion v2 是否已经安装。 如未安装，用 Homebrew 来安装它：

`brew install bash-completion@2`

如命令的输出信息所显示的，将如下内容添加到文件 ​`~/.bash_profile`​ 中：

`export BASH_COMPLETION_COMPAT_DIR="/usr/local/etc/bash_completion.d" [[ -r "/usr/local/etc/profile.d/bash_completion.sh" ]] && . "/usr/local/etc/profile.d/bash_completion.sh"`

重新加载 shell，并用命令 ​`type _init_completion`​ 验证 bash-completion v2 已经恰当的安装。

#### 启用 kubectl 自动补全功能

你现在需要确保在所有的 shell 环境中均已导入（sourced） kubectl 的补全脚本， 有若干种方法可以实现这一点：

*   在文件 ​`~/.bash_profile`​ 中导入（Source）补全脚本：

`echo 'source <(kubectl completion bash)' >>~/.bash_profile`

*   将补全脚本添加到目录 ​`/usr/local/etc/bash_completion.d`​ 中：

`kubectl completion bash >/usr/local/etc/bash_completion.d/kubectl`

*   如果你为 kubectl 定义了别名，则可以扩展 shell 补全来兼容该别名：

`echo 'alias k=kubectl' >>~/.bash_profile echo 'complete -F __start_kubectl k' >>~/.bash_profile`

如果你是用 Homebrew 安装的 kubectl，则kubectl 补全脚本应该已经安装到目录 ​`/usr/local/etc/bash_completion.d/kubectl`​ 中了。这种情况下，你什么都不需要做。

> 用 Hommbrew 安装的 bash-completion v2 会初始化 目录 ​`BASH_COMPLETION_COMPAT_DIR`​ 中的所有文件，这就是后两种方法能正常工作的原因。

总之，重新加载 shell 之后，kubectl 补全功能将立即生效。

### Fish

kubectl 通过命令 ​`kubectl completion fish`​ 生成 Fish 自动补全脚本。 在 shell 中导入（Sourcing）该自动补全脚本，将启动 kubectl 自动补全功能。

为了在所有的 shell 会话中实现此功能，请将下面内容加入到文件 ​`~/.config/fish/config.fish`​ 中。

`kubectl completion fish | source`

重新加载 shell 后，kubectl 自动补全功能将立即生效。

### Zsh

kubectl 通过命令 ​`kubectl completion zsh`​ 生成 Zsh 自动补全脚本。 在 shell 中导入（Sourcing）该自动补全脚本，将启动 kubectl 自动补全功能。

为了在所有的 shell 会话中实现此功能，请将下面内容加入到文件 ​`~/.zshrc`​ 中。

`source <(kubectl completion zsh)`

如果你为 kubectl 定义了别名，kubectl 自动补全将自动使用它。

重新加载 shell 后，kubectl 自动补全功能将立即生效。

如果你收到 ​`2: command not found: compdef`​ 这样的错误提示，那请将下面内容添加到 ​`~/.zshrc`​ 文件的开头：

`autoload -Uz compinit compinit`

### 安装 kubectl convert 插件 

一个 Kubernetes 命令行工具 kubectl 的插件，允许你将清单在不同 API 版本间转换。 这对于将清单迁移到新的 Kubernetes 发行版上未被废弃的 API 版本时尤其有帮助。 更多信息请访问 迁移到非弃用 API

1、用以下命令下载最新发行版：

*   Intel

   `curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/amd64/kubectl-convert"`

*   Apple Silicon

   `curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/arm64/kubectl-convert"`

2、验证该可执行文件（可选步骤）

下载 kubectl-convert 校验和文件：

*   Intel

   `curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/amd64/kubectl-convert.sha256"`

*   Apple Silicon

   `curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/arm64/kubectl-convert.sha256"`

基于校验和，验证 kubectl-convert 的可执行文件：

`echo "$(cat kubectl-convert.sha256)  kubectl-convert" | shasum -a 256 --check`

验证通过时，输出为：

`kubectl-convert: OK`

验证失败时，​`sha256` ​将以非零值退出，并打印输出类似于：

`kubectl-convert: FAILED shasum: WARNING: 1 computed checksum did NOT match`

> 下载相同版本的可执行文件和校验和。

3、使 kubectl-convert 二进制文件可执行

`chmod +x ./kubectl-convert`

4、将 kubectl-convert 可执行文件移动到系统 PATH 环境变量中的一个位置。

`sudo mv ./kubectl-convert /usr/local/bin/kubectl-convert sudo chown root: /usr/local/bin/kubectl-convert`

> 确保你的 PATH 环境变量中存在 ​`/usr/local/bin`​

5、验证插件是否安装成功

`kubectl convert --help`

如果你没有看到任何错误就代表插件安装成功了。

##  3.  Kubernetes Windows安装
kubectl 版本和集群版本之间的差异必须在一个小版本号内。 例如：v1.23 版本的客户端能与 v1.22、 v1.23 和 v1.24 版本的控制面通信。 用最新兼容版的 kubectl 有助于避免不可预见的问题。

在 Windows 上安装 kubectl 
----------------------

在 Windows 系统中安装 kubectl 有如下几种方法：

*   用 curl 在 Windows 上安装 kubectl
*   在 Windows 上用 Chocolatey 或 Scoop 安装

用 curl 在 Windows 上安装 kubectl
----------------------------

1、下载 [最新发行版 v1.23.0](https://storage.googleapis.com/kubernetes-release/release/v1.23.0/bin/windows/amd64/kubectl.exe)。

如果你已安装了 ​`curl`​,也可以使用此命令：

`curl -LO "https://dl.k8s.io/release/v1.23.0/bin/windows/amd64/kubectl.exe"`

2、验证该可执行文件（可选步骤）

下载 ​`kubectl` ​校验和文件：

`curl -LO "https://dl.k8s.io/v1.23.0/bin/windows/amd64/kubectl.exe.sha256"`

基于校验和文件，验证 kubectl 的可执行文件：

*   在命令行环境中，手工对比 CertUtil 命令的输出与校验和文件：

`CertUtil -hashfile kubectl.exe SHA256 type kubectl.exe.sha256`

用 PowerShell 自动验证，用运算符 ​`-eq`​ 来直接取得 ​`True` ​或 ​`False` ​的结果：

`$($(CertUtil -hashfile .\kubectl.exe SHA256)[1] -replace " ", "") -eq $(type .\kubectl.exe.sha256)`

3、将 ​`kubectl` ​二进制文件夹追加或插入到你的 ​`PATH` ​环境变量中。

4、测试一下，确保此 ​`kubectl` ​的版本和期望版本一致：

`kubectl version --client`

或者使用下面命令来查看版本的详细信息：

`kubectl version --client --output=yaml`

> [Windows 版的 Docker Desktop](https://docs.docker.com/desktop/windows/) 将其自带版本的 ​`kubectl` ​添加到 ​`PATH`​。 如果你之前安装过 Docker Desktop，可能需要把此 ​`PATH` ​条目置于 Docker Desktop 安装的条目之前， 或者直接删掉 Docker Desktop 的 ​`kubectl`​。

在 Windows 上用 Chocolatey 或 Scoop 安装
----------------------------------

1、要在 Windows 上安装 kubectl，你可以使用包管理器 [Chocolatey](https://chocolatey.org/) 或是命令行安装器 [Scoop](https://scoop.sh/)。

*   choco

`choco install kubernetes-cli`

*   scoop

`scoop install kubectl`

2、测试一下，确保安装的是最新版本：

`kubectl version --client`

3、导航到你的 home 目录：

`# 当你用 cmd.exe 时，则运行： cd %USERPROFILE% cd ~`

4、创建目录 ​`.kube`​：

`mkdir .kube`

5、切换到新创建的目录 ​`.kube` ​

`cd .kube`

6、配置 kubectl，以接入远程的 Kubernetes 集群：

`New-Item config -type file`

> 编辑配置文件，你需要先选择一个文本编辑器，比如 Notepad。

验证 kubectl 配置
-------------

为了让 kubectl 能发现并访问 Kubernetes 集群，你需要一个 kubeconfig 文件， 该文件在 [kube-up.sh](https://github.com/kubernetes/kubernetes/blob/master/cluster/kube-up.sh) 创建集群时，或成功部署一个 Miniube 集群时，均会自动生成。 通常，kubectl 的配置信息存放于文件 ​`~/.kube/config`​ 中。

通过获取集群状态的方法，检查是否已恰当的配置了 kubectl：

`kubectl cluster-info`

如果返回一个 URL，则意味着 kubectl 成功的访问到了你的集群。

如果你看到如下所示的消息，则代表 kubectl 配置出了问题，或无法连接到 Kubernetes 集群。

`The connection to the server <server-name:port> was refused - did you specify the right host or port? （访问 <server-name:port> 被拒绝 - 你指定的主机和端口是否有误？）`

例如，如果你想在自己的笔记本上（本地）运行 Kubernetes 集群，你需要先安装一个 Minikube 这样的工具，然后再重新运行上面的命令。

如果命令 ​`kubectl cluster-info`​ 返回了 url，但你还不能访问集群，那可以用以下命令来检查配置是否妥当：

`kubectl cluster-info dump`

kubectl 可选配置和插件
---------------

### 启用 shell 自动补全功能

kubectl 为 Bash、Zsh、Fish 和 PowerShell 提供自动补全功能，可以为你节省大量的输入。

下面是设置 PowerShell 自动补全功能的操作步骤。

使用命令 ​`kubectl completion powershell`​ 生成 PowerShell 的 kubectl 自动补全脚本。

如果需要自动补全在所有 shell 会话中生效，请将以下命令添加到 ​`$PROFILE`​ 文件中：

`kubectl completion powershell | Out-String | Invoke-Expression`

此命令将在每次 PowerShell 启动时重新生成自动补全脚本。你还可以将生成的自动补全脚本添加到 ​`$PROFILE`​ 文件中。

如果需要将自动补全脚本直接添加到 ​`$PROFILE`​ 文件中，请在 PowerShell 终端运行以下命令：

`kubectl completion powershell >> $PROFILE`

完成上述操作后重启 shell，kubectl的自动补全就可以工作了。

### 安装 kubectl convert 插件

一个 Kubernetes 命令行工具 ​`kubectl` ​的插件，允许你将清单在不同 API 版本间转换。 这对于将清单迁移到新的 Kubernetes 发行版上未被废弃的 API 版本时尤其有帮助。

1、用以下命令下载最新发行版：

`curl -LO "https://dl.k8s.io/release/v1.23.0/bin/windows/amd64/kubectl-convert.exe"`

2、验证该可执行文件（可选步骤）

下载 ​`kubectl-convert`​ 校验和文件：

`curl -LO "https://dl.k8s.io/v1.23.0/bin/windows/amd64/kubectl-convert.exe.sha256"`

基于校验和，验证 ​`kubectl-convert`​ 的可执行文件：

*   用提示的命令对 ​`CertUtil` ​的输出和下载的校验和文件进行手动比较。

`CertUtil -hashfile kubectl-convert.exe SHA256 type kubectl-convert.exe.sha256`

*   使用 PowerShell ​`-eq`​ 操作使验证自动化，获得 ​`True` ​或者 ​`False` ​的结果：

`$($(CertUtil -hashfile .\kubectl-convert.exe SHA256)[1] -replace " ", "") -eq $(type .\kubectl-convert.exe.sha256)`

3、将 kubectl-convert 二进制文件夹附加或添加到你的 PATH 环境变量中。

4、验证插件是否安装成功

`kubectl convert --help`

如果你没有看到任何错误就代表插件安装成功了。

#  4.  Kubernetes 对象

##  1.  Kubernetes 对象简介
本页说明了 Kubernetes 对象在 Kubernetes API 中是如何表示的，以及如何在 ​`.yaml`​ 格式的文件中表示。

理解 Kubernetes 对象
----------------

在 Kubernetes 系统中，Kubernetes 对象 是持久化的实体。 Kubernetes 使用这些实体去表示整个集群的状态。特别地，它们描述了如下信息：

*   哪些容器化应用在运行（以及在哪些节点上）
*   可以被应用使用的资源
*   关于应用运行时表现的策略，比如重启策略、升级策略，以及容错策略

Kubernetes 对象是 “目标性记录” —— 一旦创建对象，Kubernetes 系统将持续工作以确保对象存在。 通过创建对象，本质上是在告知 Kubernetes 系统，所需要的集群工作负载看起来是什么样子的， 这就是 Kubernetes 集群的 期望状态（Desired State）。

操作 Kubernetes 对象 —— 无论是创建、修改，或者删除 —— 需要使用 Kubernetes API。 比如，当使用 kubectl 命令行接口时，CLI 会执行必要的 Kubernetes API 调用， 也可以在程序中使用 客户端库直接调用 Kubernetes API。

对象规约（Spec）与状态（Status） 
----------------------

几乎每个 Kubernetes 对象包含两个嵌套的对象字段，它们负责管理对象的配置： 对象 ​`spec`​（规约） 和 对象 ​`status`​（状态） 。 对于具有 ​`spec` ​的对象，你必须在创建对象时设置其内容，描述你希望对象所具有的特征： 期望状态（Desired State） 。

​`status` ​描述了对象的 当前状态（Current State），它是由 Kubernetes 系统和组件 设置并更新的。在任何时刻，Kubernetes 控制平面 都一直积极地管理着对象的实际状态，以使之与期望状态相匹配。

例如，Kubernetes 中的 Deployment 对象能够表示运行在集群中的应用。 当创建 Deployment 时，可能需要设置 Deployment 的 ​`spec`​，以指定该应用需要有 3 个副本运行。 Kubernetes 系统读取 Deployment 规约，并启动我们所期望的应用的 3 个实例 —— 更新状态以与规约相匹配。 如果这些实例中有的失败了（一种状态变更），Kubernetes 系统通过执行修正操作 来响应规约和状态间的不一致 —— 在这里意味着它会启动一个新的实例来替换。

关于对象 spec、status 和 metadata 的更多信息，可参阅 [Kubernetes API 约定](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md)。

描述 Kubernetes 对象
----------------

创建 Kubernetes 对象时，必须提供对象的规约，用来描述该对象的期望状态， 以及关于对象的一些基本信息（例如名称）。 当使用 Kubernetes API 创建对象时（或者直接创建，或者基于​`kubectl`​）， API 请求必须在请求体中包含 JSON 格式的信息。 大多数情况下，需要在 ​`.yaml`​ 文件中为 ​`kubectl` ​提供这些信息。 ​`kubectl` ​在发起 API 请求时，将这些信息转换成 JSON 格式。

这里有一个 ​`.yaml`​ 示例文件，展示了 Kubernetes Deployment 的必需字段和对象规约：

`apiVersion: apps/v1 kind: Deployment metadata:   name: nginx-deployment spec:   selector:     matchLabels:       app: nginx   replicas: 2 # tells deployment to run 2 pods matching the template   template:     metadata:       labels:         app: nginx     spec:       containers:       - name: nginx         image: nginx:1.14.2         ports:         - containerPort: 80`

使用类似于上面的 ​`.yaml`​ 文件来创建 Deployment的一种方式是使用 ​`kubectl` ​命令行接口（CLI）中的 [kubectl apply](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands target=) 命令， 将 ​`.yaml`​ 文件作为参数。下面是一个示例：

`kubectl apply -f https://k8s.io/examples/application/deployment.yaml`

输出类似如下这样：

`deployment.apps/nginx-deployment created`

必需字段 
-----

在想要创建的 Kubernetes 对象对应的 ​`.yaml`​ 文件中，需要配置如下的字段：

*   ​`apiVersion` ​- 创建该对象所使用的 Kubernetes API 的版本
*   ​`kind` ​- 想要创建的对象的类别
*   ​`metadata` ​- 帮助唯一性标识对象的一些数据，包括一个 name 字符串、UID 和可选的 namespace
*   ​`spec` ​- 你所期望的该对象的状态

对象 ​`spec` ​的精确格式对每个 Kubernetes 对象来说是不同的，包含了特定于该对象的嵌套字段。

例如，参阅 Pod API 参考文档中 ​`spec`​ 字段。 对于每个 Pod，其 ​`.spec`​ 字段设置了 Pod 及其期望状态（例如 Pod 中每个容器的容器镜像名称）。 另一个对象规约的例子是 StatefulSet API 中的 ​`spec` ​字段。 对于 StatefulSet 而言，其 ​`.spec`​ 字段设置了 StatefulSet 及其期望状态。 在 StatefulSet 的 ​`.spec`​ 内，有一个为 Pod 对象提供的模板。该模板描述了 StatefulSet 控制器为了满足 StatefulSet 规约而要创建的 Pod。 不同类型的对象可以由不同的 ​`.status`​ 信息。API 参考页面给出了 ​`.status`​ 字段的详细结构， 以及针对不同类型 API 对象的具体内容。

##  2.  Kubernetes 对象管理
对象管理
----

​`kubectl` ​命令行工具支持多种不同的方式来创建和管理 Kubernetes 对象。 本文档概述了不同的方法。 阅读 [Kubectl book](https://kubectl.docs.kubernetes.io/) 来了解 kubectl 管理对象的详细信息。

管理技巧
----

> 应该只使用一种技术来管理 Kubernetes 对象。混合和匹配技术作用在同一对象上将导致未定义行为。

管理技术

作用于

建议的环境

支持的写者

学习难度

指令式命令

活跃对象

开发项目

1+

最低

指令式对象配置

单个文件

生产项目

1

中等

声明式对象配置

文件目录

生产项目

1+

最高

指令式命令
-----

使用指令式命令时，用户可以在集群中的活动对象上进行操作。用户将操作传给 ​`kubectl` ​命令作为参数或标志。

这是开始或者在集群中运行一次性任务的推荐方法。因为这个技术直接在活跃对象 上操作，所以它不提供以前配置的历史记录。

### 例子

通过创建 Deployment 对象来运行 nginx 容器的实例：

`kubectl create deployment nginx --image nginx`

### 权衡 

与对象配置相比的优点：

*   命令简单，易学且易于记忆。
*   命令仅需一步即可对集群进行更改。

与对象配置相比的缺点：

*   命令不与变更审查流程集成。
*   命令不提供与更改关联的审核跟踪。
*   除了实时内容外，命令不提供记录源。
*   命令不提供用于创建新对象的模板。

指令式对象配置
-------

在指令式对象配置中，kubectl 命令指定操作（创建，替换等），可选标志和 至少一个文件名。指定的文件必须包含 YAML 或 JSON 格式的对象的完整定义。

有关对象定义的详细信息，请查看 [API 参考](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/)。

> **Warning**:
> 
> ​`replace` ​指令式命令将现有规范替换为新提供的规范，并放弃对配置文件中 缺少的对象的所有更改。此方法不应与对象规约被独立于配置文件进行更新的 资源类型一起使用。比如类型为 ​`LoadBalancer` ​的服务，它的 ​`externalIPs` ​字段就是独立于集群配置进行更新。

### 例子

创建配置文件中定义的对象：

`kubectl create -f nginx.yaml`

删除两个配置文件中定义的对象：

`kubectl delete -f nginx.yaml -f redis.yaml`

通过覆盖活动配置来更新配置文件中定义的对象：

`kubectl replace -f nginx.yaml`

### 权衡

与指令式命令相比的优点：

*   对象配置可以存储在源控制系统中，比如 Git。
*   对象配置可以与流程集成，例如在推送和审计之前检查更新。
*   对象配置提供了用于创建新对象的模板。

与指令式命令相比的缺点：

*   对象配置需要对对象架构有基本的了解。
*   对象配置需要额外的步骤来编写 YAML 文件。

与声明式对象配置相比的优点：

*   指令式对象配置行为更加简单易懂。
*   从 Kubernetes 1.5 版本开始，指令对象配置更加成熟。

与声明式对象配置相比的缺点：

*   指令式对象配置更适合文件，而非目录。
*   对活动对象的更新必须反映在配置文件中，否则会在下一次替换时丢失。

声明式对象配置 
--------

使用声明式对象配置时，用户对本地存储的对象配置文件进行操作，但是用户 未定义要对该文件执行的操作。 ​`kubectl` ​会自动检测每个文件的创建、更新和删除操作。 这使得配置可以在目录上工作，根据目录中配置文件对不同的对象执行不同的操作。

> 声明式对象配置保留其他编写者所做的修改，即使这些更改并未合并到对象配置文件中。 可以通过使用 ​`patch` ​API 操作仅写入观察到的差异，而不是使用 ​`replace` ​API 操作来替换整个对象配置来实现。

### 例子

处理 ​`configs` ​目录中的所有对象配置文件，创建并更新活跃对象。 可以首先使用 ​`diff` ​子命令查看将要进行的更改，然后在进行应用：

`kubectl diff -f configs/ kubectl apply -f configs/`

递归处理目录：

`kubectl diff -R -f configs/ kubectl apply -R -f configs/`

### 权衡

与指令式对象配置相比的优点：

*   对活动对象所做的更改即使未合并到配置文件中，也会被保留下来。
*   声明性对象配置更好地支持对目录进行操作并自动检测每个文件的操作类型（创建，修补，删除）。

与指令式对象配置相比的缺点：

*   声明式对象配置难于调试并且出现异常时结果难以理解。
*   使用 diff 产生的部分更新会创建复杂的合并和补丁操作。

##  3.  Kubernetes 对象名称和IDs
对象名称和 IDs
---------

集群中的每一个对象都有一个名称 来标识在同类资源中的唯一性。

每个 Kubernetes 对象也有一个UID 来标识在整个集群中的唯一性。

比如，在同一个名字空间 中有一个名为 ​`myapp-1234`​ 的 Pod, 但是可以命名一个 Pod 和一个 Deployment 同为 ​`myapp-1234`​.

对于用户提供的非唯一性的属性，Kubernetes 提供了 标签（Labels）和 注解（Annotation）机制。

名称 
---

客户端提供的字符串，引用资源 url 中的对象，如​`/api/v1/pods/some name`​。

某一时刻，只能有一个给定类型的对象具有给定的名称。但是，如果删除该对象，则可以创建同名的新对象。

> 当对象所代表的是一个物理实体（例如代表一台物理主机的 Node）时， 如果在 Node 对象未被删除并重建的条件下，重新创建了同名的物理主机， 则 Kubernetes 会将新的主机看作是老的主机，这可能会带来某种不一致性。

以下是比较常见的四种资源命名约束。

### DNS 子域名 

很多资源类型需要可以用作 DNS 子域名的名称。 DNS 子域名的定义可参见 [RFC 1123](https://datatracker.ietf.org/doc/html/rfc1123)。 这一要求意味着名称必须满足如下规则：

*   不能超过253个字符
*   只能包含小写字母、数字，以及'-' 和 '.'
*   须以字母数字开头
*   须以字母数字结尾

### RFC 1123 标签名 

某些资源类型需要其名称遵循 [RFC 1123](https://datatracker.ietf.org/doc/html/rfc1123) 所定义的 DNS 标签标准。也就是命名必须满足如下规则：

*   最多 63 个字符
*   只能包含小写字母、数字，以及 '-'
*   须以字母数字开头
*   须以字母数字结尾

### RFC 1035 标签名 

某些资源类型需要其名称遵循 [RFC 1035](https://tools.ietf.org/html/rfc1035) 所定义的 DNS 标签标准。也就是命名必须满足如下规则：

*   最多 63 个字符
*   只能包含小写字母、数字，以及 '-'
*   须以字母开头
*   须以字母数字结尾

### 路径分段名称 

某些资源类型要求名称能被安全地用作路径中的片段。 换句话说，其名称不能是 ​`.`​、​`..`​，也不可以包含 ​`/`​ 或 ​`%`​ 这些字符。

下面是一个名为​`nginx-demo`​的 Pod 的配置清单：

`apiVersion: v1 kind: Pod metadata:   name: nginx-demo spec:   containers:   - name: nginx     image: nginx:1.14.2     ports:     - containerPort: 80`

> 某些资源类型可能具有额外的命名约束。

UIDs
----

Kubernetes 系统生成的字符串，唯一标识对象。

在 Kubernetes 集群的整个生命周期中创建的每个对象都有一个不同的 uid，它旨在区分类似实体的历史事件。

Kubernetes UIDs 是全局唯一标识符（也叫 UUIDs）。 UUIDs 是标准化的，见 ISO/IEC 9834-8 和 ITU-T X.667.

##  4.  Kubernetes 名字空间
名字空间
----

在 Kubernetes 中，“名字空间（Namespace）”提供一种机制，将同一集群中的资源划分为相互隔离的组。 同一名字空间内的资源名称要唯一，但跨名字空间时没有这个要求。 名字空间作用域仅针对带有名字空间的对象，例如 Deployment、Service 等， 这种作用域对集群访问的对象不适用，例如 StorageClass、Node、PersistentVolume 等。

何时使用多个名字空间
----------

名字空间适用于存在很多跨多个团队或项目的用户的场景。对于只有几到几十个用户的集群，根本不需要创建或考虑名字空间。当需要名称空间提供的功能时，请开始使用它们。

名字空间为名称提供了一个范围。资源的名称需要在名字空间内是唯一的，但不能跨名字空间。 名字空间不能相互嵌套，每个 Kubernetes 资源只能在一个名字空间中。

名字空间是在多个用户之间划分集群资源的一种方法（通过资源配额）。

不必使用多个名字空间来分隔仅仅轻微不同的资源，例如同一软件的不同版本： 应该使用标签 来区分同一名字空间中的不同资源。

使用名字空间
------

### 创建名字空间

> 避免使用前缀 ​`kube-`​ 创建名字空间，因为它是为 Kubernetes 系统名字空间保留的。

新建一个名为 my-namespace.yaml 的 YAML 文件，并写入下列内容：

`apiVersion: v1 kind: Namespace metadata:   name: <insert-namespace-name-here>`

然后运行：

`kubectl create -f ./my-namespace.yaml`

2、或者，你可以使用下面的命令创建名字空间：

`kubectl create namespace <insert-namespace-name-here>`

请注意，名字空间的名称必须是一个合法的 DNS 标签。

可选字段 finalizers 允许观察者们在名字空间被删除时清除资源。记住如果指定了一个不存在的终结器，名字空间仍会被创建，但如果用户试图删除它，它将陷入 Terminating 状态。

更多有关 finalizers 的信息请查阅 [设计文档](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/namespaces.md target=) 中名字空间部分。

### 删除名字空间 

删除名字空间使用命令：

`kubectl delete namespaces <insert-some-namespace-name>`

> **Warning**: 这会删除名字空间下的 所有内容 ！

删除是异步的，所以有一段时间你会看到名字空间处于 ​`Terminating` ​状态。

查看名字空间  

你可以使用以下命令列出集群中现存的名字空间：

`kubectl get namespace`

Kubernetes 会创建四个初始名字空间：

*   ​`default` ​没有指明使用其它名字空间的对象所使用的默认名字空间
*   ​`kube-system`​ Kubernetes 系统创建对象所使用的名字空间
*   ​`kube-public`​ 这个名字空间是自动创建的，所有用户（包括未经过身份验证的用户）都可以读取它。 这个名字空间主要用于集群使用，以防某些资源在整个集群中应该是可见和可读的。 这个名字空间的公共方面只是一种约定，而不是要求。
*   ​`kube-node-lease`​ 此名字空间用于与各个节点相关的 租约（Lease）对象。 节点租期允许 kubelet 发送心跳，由此控制面能够检测到节点故障。

### 为请求设置名字空间 

要为当前请求设置名字空间，请使用 ​`--namespace`​ 参数。

例如：

`kubectl run nginx --image=nginx --namespace=<名字空间名称> kubectl get pods --namespace=<名字空间名称>`

### 设置名字空间偏好 

你可以永久保存名字空间，以用于对应上下文中所有后续 kubectl 命令。

`kubectl config set-context --current --namespace=<名字空间名称> # 验证之 kubectl config view | grep namespace:`

名字空间和 DNS 
----------

当你创建一个服务 时， Kubernetes 会创建一个相应的 DNS 条目。

该条目的形式是 ​`<服务名称>.<名字空间名称>.svc.cluster.local`​，这意味着如果容器只使用 ​`<服务名称>`​，它将被解析到本地名字空间的服务。这对于跨多个名字空间（如开发、分级和生产） 使用相同的配置非常有用。如果你希望跨名字空间访问，则需要使用完全限定域名（FQDN）。

因此，所有的名字空间名称都必须是合法的 RFC 1123 DNS 标签。

> **Warning**:  
> 通过创建与[公共顶级域名](https://data.iana.org/TLD/tlds-alpha-by-domain.txt) 同名的名字空间，这些名字空间中的服务可以拥有与公共 DNS 记录重叠的、较短的 DNS 名称。 所有名字空间中的负载在执行 DNS 查找时，如果查找的名称没有 [尾部句点](https://datatracker.ietf.org/doc/html/rfc1034 target=)， 就会被重定向到这些服务上，因此呈现出比公共 DNS 更高的优先序。  
> 为了缓解这类问题，需要将创建名字空间的权限授予可信的用户。 如果需要，你可以额外部署第三方的安全控制机制，例如以 准入 Webhook 的形式，阻止用户创建与公共 [TLD](https://data.iana.org/TLD/tlds-alpha-by-domain.txt) 同名的名字空间。

并非所有对象都在名字空间中
-------------

大多数 kubernetes 资源（例如 Pod、Service、副本控制器等）都位于某些名字空间中。 但是名字空间资源本身并不在名字空间中。而且底层资源，例如 节点 和持久化卷不属于任何名字空间。

查看哪些 Kubernetes 资源在名字空间中，哪些不在名字空间中：

`# 位于名字空间中的资源 kubectl api-resources --namespaced=true  # 不在名字空间中的资源 kubectl api-resources --namespaced=false`

自动打标签 
------

FEATURE STATE: Kubernetes 1.21 \[beta\]

Kubernetes 控制面会为所有名字空间设置一个不可变更的 标签 ​`kubernetes.io/metadata.name`​，只要 ​`NamespaceDefaultLabelName`​ 这一 特性门控 被启用。标签的值是名字空间的名称。

##  5.  Kubernetes 标签和选择算符
标签和选择算符
-------

标签（Labels） 是附加到 Kubernetes 对象（比如 Pods）上的键值对。 标签旨在用于指定对用户有意义且相关的对象的标识属性，但不直接对核心系统有语义含义。 标签可以用于组织和选择对象的子集。标签可以在创建时附加到对象，随后可以随时添加和修改。 每个对象都可以定义一组键/值标签。每个键对于给定对象必须是唯一的。

`"metadata": {   "labels": {     "key1" : "value1",     "key2" : "value2"   } }`

标签能够支持高效的查询和监听操作，对于用户界面和命令行是很理想的。 应使用注解 记录非识别信息。

动机
--

标签使用户能够以松散耦合的方式将他们自己的组织结构映射到系统对象，而无需客户端存储这些映射。

服务部署和批处理流水线通常是多维实体（例如，多个分区或部署、多个发行序列、多个层，每层多个微服务）。 管理通常需要交叉操作，这打破了严格的层次表示的封装，特别是由基础设施而不是用户确定的严格的层次结构。

示例标签：

*   ​`"release" : "stable"`​, ​`"release" : "canary"` ​
*   ​`"environment" : "dev"`​, ​`"environment" : "qa"`​, ​`"environment" : "production"` ​
*   ​`"tier" : "frontend"`​, ​`"tier" : "backend"`​, ​`"tier" : "cache"` ​
*   ​`"partition" : "customerA"`​, ​`"partition" : "customerB"` ​
*   ​`"track" : "daily"`​, ​`"track" : "weekly"`​

有一些常用标签的例子; 你可以任意制定自己的约定。 请记住，标签的 Key 对于给定对象必须是唯一的。

语法和字符集 
-------

标签 是键值对。有效的标签键有两个段：可选的前缀和名称，用斜杠（​`/`​）分隔。 名称段是必需的，必须小于等于 63 个字符，以字母数字字符（​`[a-z0-9A-Z]`​）开头和结尾， 带有破折号（​`-`​），下划线（​`_`​），点（ ​`.`​）和之间的字母数字。 前缀是可选的。如果指定，前缀必须是 DNS 子域：由点（​`.`​）分隔的一系列 DNS 标签，总共不超过 253 个字符， 后跟斜杠（​`/`​）。

如果省略前缀，则假定标签键对用户是私有的。 向最终用户对象添加标签的自动系统组件（例如 ​`kube-scheduler`​、​`kube-controller-manager`​、 ​`kube-apiserver`​、​`kubectl`​ 或其他第三方自动化工具）必须指定前缀。

​`kubernetes.io/`​ 和 ​`k8s.io/`​ 前缀是为 Kubernetes 核心组件保留的。

有效标签值：

*   必须为 63 个字符或更少（可以为空）
*   除非标签值为空，必须以字母数字字符（​`[a-z0-9A-Z]`​）开头和结尾
*   包含破折号（​`-`​）、下划线（​`_`​）、点（​`.`​）和字母或数字。

标签选择算符 
-------

与名称和 UID 不同， 标签不支持唯一性。通常，我们希望许多对象携带相同的标签。

通过 标签选择算符，客户端/用户可以识别一组对象。标签选择算符是 Kubernetes 中的核心分组原语。

API 目前支持两种类型的选择算符：基于等值的 和 基于集合的。 标签选择算符可以由逗号分隔的多个 需求 组成。 在多个需求的情况下，必须满足所有要求，因此逗号分隔符充当逻辑 与（​`&&`​）运算符。

空标签选择算符或者未指定的选择算符的语义取决于上下文， 支持使用选择算符的 API 类别应该将算符的合法性和含义用文档记录下来。

> 对于某些 API 类别（例如 ReplicaSet）而言，两个实例的标签选择算符不得在命名空间内重叠， 否则它们的控制器将互相冲突，无法确定应该存在的副本个数。

> 对于基于等值的和基于集合的条件而言，不存在逻辑或（||）操作符。 你要确保你的过滤语句按合适的方式组织。

### 基于等值的需求

基于等值 或 基于不等值 的需求允许按标签键和值进行过滤。 匹配对象必须满足所有指定的标签约束，尽管它们也可能具有其他标签。 可接受的运算符有​`=`​、​`==`​ 和 ​`!=`​ 三种。 前两个表示 相等（并且只是同义词），而后者表示 不相等。例如：

`environment = production tier != frontend`

前者选择所有资源，其键名等于 ​`environment`​，值等于 ​`production`​。 后者选择所有资源，其键名等于 ​`tier`​，值不同于 ​`frontend`​，所有资源都没有带有 ​`tier` ​键的标签。 可以使用逗号运算符来过滤 ​`production` ​环境中的非 ​`frontend` ​层资源：​`environment=production,tier!=frontend`​。

基于等值的标签要求的一种使用场景是 Pod 要指定节点选择标准。 例如，下面的示例 Pod 选择带有标签 "​`accelerator=nvidia-tesla-p100`​"。

`apiVersion: v1 kind: Pod metadata:   name: cuda-test spec:   containers:     - name: cuda-test       image: "k8s.gcr.io/cuda-vector-add:v0.1"       resources:         limits:           nvidia.com/gpu: 1   nodeSelector:     accelerator: nvidia-tesla-p100`

### 基于集合的需求

基于集合 的标签需求允许你通过一组值来过滤键。 支持三种操作符：​`in`​、​`notin` ​和 ​`exists` ​(只可以用在键标识符上)。例如：

`environment in (production, qa) tier notin (frontend, backend) partition !partition`

*   第一个示例选择了所有键等于 ​`environment` ​并且值等于 ​`production` ​或者 ​`qa` ​的资源。
*   第二个示例选择了所有键等于 ​`tier` ​并且值不等于 ​`frontend` ​或者 ​`backend` ​的资源，以及所有没有 ​`tier` ​键标签的资源。
*   第三个示例选择了所有包含了有 ​`partition` ​标签的资源；没有校验它的值。
*   第四个示例选择了所有没有 ​`partition` ​标签的资源；没有校验它的值。

类似地，逗号分隔符充当 与 运算符。因此，使用 ​`partition` ​键（无论为何值）和 ​`environment` ​不同于 ​`qa`​ 来过滤资源可以使用 ​`partition, environment notin（qa)`​ 来实现。

基于集合 的标签选择算符是相等标签选择算符的一般形式，因为 ​`environment=production`​ 等同于 ​`environment in（production）`​；​`!=`​ 和 ​`notin` ​也是类似的。

基于集合 的要求可以与基于 相等 的要求混合使用。例如：​`partition in (customerA, customerB),environment!=qa`​。

API
---

### LIST 和 WATCH 过滤

LIST 和 WATCH 操作可以使用查询参数指定标签选择算符过滤一组对象。 两种需求都是允许的。（这里显示的是它们出现在 URL 查询字符串中）

*   基于等值 的需求: ​`?labelSelector=environment%3Dproduction,tier%3Dfrontend` ​
*   基于集合 的需求: ​`?labelSelector=environment+in+%28production%2Cqa%29%2Ctier+in+%28frontend%29`​

两种标签选择算符都可以通过 REST 客户端用于 list 或者 watch 资源。 例如，使用 ​`kubectl` ​定位 ​`apiserver`​，可以使用 基于等值 的标签选择算符可以这么写：

`kubectl get pods -l environment=production,tier=frontend`

或者使用 基于集合的 需求：

`kubectl get pods -l 'environment in (production),tier in (frontend)'`

正如刚才提到的，基于集合 的需求更具有表达力。例如，它们可以实现值的 或 操作：

`kubectl get pods -l 'environment in (production, qa)'`

或者通过 exists 运算符限制不匹配：

`kubectl get pods -l 'environment,environment notin (frontend)'`

### 在 API 对象中设置引用

一些 Kubernetes 对象，例如 ​`services` ​和 ​`replicationcontrollers` ​， 也使用了标签选择算符去指定了其他资源的集合，例如 pods。

#### Service 和 ReplicationController

一个 ​`Service` ​指向的一组 Pods 是由标签选择算符定义的。同样，一个 ​`ReplicationController` ​应该管理的 pods 的数量也是由标签选择算符定义的。

两个对象的标签选择算符都是在 ​`json` ​或者 ​`yaml` ​文件中使用映射定义的，并且只支持 基于等值 需求的选择算符：

`"selector": {     "component" : "redis", }`

或者

`selector:     component: redis`

这个选择算符(分别在 ​`json` ​或者 ​`yaml` ​格式中) 等价于 ​`component=redis`​ 或 ​`component in (redis)`​ 。

#### 支持基于集合需求的资源

比较新的资源，例如 ​`Job`​、 ​`Deployment`​、 ​`Replica Set`​ 和 ​`DaemonSet` ​， 也支持 基于集合的 需求。

`selector:   matchLabels:     component: redis   matchExpressions:     - {key: tier, operator: In, values: [cache]}     - {key: environment, operator: NotIn, values: [dev]}`

​`matchLabels`​ 是由 ​`{key,value}`​ 对组成的映射。 ​`matchLabels`​ 映射中的单个 ​`{key,value }`​ 等同于 ​`matchExpressions` ​的元素， 其 ​`key` ​字段为 "key"，​`operator` ​为 "In"，而 ​`values` ​数组仅包含 "value"。 ​`matchExpressions` ​是 Pod 选择算符需求的列表。 有效的运算符包括 ​`In`​、​`NotIn`​、​`Exists` ​和 ​`DoesNotExist`​。 在 ​`In` ​和 ​`NotIn` ​的情况下，设置的值必须是非空的。 来自 ​`matchLabels` ​和 ​`matchExpressions` ​的所有要求都按逻辑与的关系组合到一起 -- 它们必须都满足才能匹配。

#### 选择节点集 

通过标签进行选择的一个用例是确定节点集，方便 Pod 调度。

##  6.  Kubernetes 注解
注解
--

你可以使用 Kubernetes 注解为对象附加任意的非标识的元数据。客户端程序（例如工具和库）能够获取这些元数据信息。

为对象附加元数据
--------

你可以使用标签或注解将元数据附加到 Kubernetes 对象。 标签可以用来选择对象和查找满足某些条件的对象集合。 相反，注解不用于标识和选择对象。 注解中的元数据，可以很小，也可以很大，可以是结构化的，也可以是非结构化的，能够包含标签不允许的字符。

注解和标签一样，是键/值对:

`"metadata": {   "annotations": {     "key1" : "value1",     "key2" : "value2"   } }`

> Map 中的键和值必须是字符串。 换句话说，你不能使用数字、布尔值、列表或其他类型的键或值。

以下是一些例子，用来说明哪些信息可以使用注解来记录:

*   由声明性配置所管理的字段。 将这些字段附加为注解，能够将它们与客户端或服务端设置的默认值、 自动生成的字段以及通过自动调整大小或自动伸缩系统设置的字段区分开来。
*   构建、发布或镜像信息（如时间戳、发布 ID、Git 分支、PR 数量、镜像哈希、仓库地址）。
*   指向日志记录、监控、分析或审计仓库的指针。
*   可用于调试目的的客户端库或工具信息：例如，名称、版本和构建信息。
*   用户或者工具/系统的来源信息，例如来自其他生态系统组件的相关对象的 URL。
*   轻量级上线工具的元数据信息：例如，配置或检查点。
*   负责人员的电话或呼机号码，或指定在何处可以找到该信息的目录条目，如团队网站。
*   从用户到最终运行的指令，以修改行为或使用非标准功能。

你可以将这类信息存储在外部数据库或目录中而不使用注解， 但这样做就使得开发人员很难生成用于部署、管理、自检的客户端共享库和工具。

语法和字符集
------

注解（Annotations） 存储的形式是键/值对。有效的注解键分为两部分： 可选的前缀和名称，以斜杠（​`/`​）分隔。 名称段是必需项，并且必须在63个字符以内，以字母数字字符（​`[a-z0-9A-Z]`​）开头和结尾， 并允许使用破折号（​`-`​），下划线（​`_`​），点（​`.`​）和字母数字。 前缀是可选的。如果指定，则前缀必须是DNS子域：一系列由点（​`.`​）分隔的DNS标签， 总计不超过253个字符，后跟斜杠（​`/`​）。 如果省略前缀，则假定注解键对用户是私有的。 由系统组件添加的注解 （例如，​`kube-scheduler`​，​`kube-controller-manager`​，​`kube-apiserver`​，​`kubectl`​ 或其他第三方组件），必须为终端用户添加注解前缀。

​`kubernetes.io/`​ 和 ​`k8s.io/`​ 前缀是为Kubernetes核心组件保留的。

例如，下面是一个 Pod 的配置文件，其注解中包含 ​`imageregistry: https://hub.docker.com/`​：

`apiVersion: v1 kind: Pod metadata:   name: annotations-demo   annotations:     imageregistry: "https://hub.docker.com/" spec:   containers:   - name: nginx     image: nginx:1.7.9     ports:     - containerPort: 80`

##  7.  Kubernetes Finalizers
Finalizers
----------

Finalizer 是带有命名空间的键，告诉 Kubernetes 等到特定的条件被满足后， 再完全删除被标记为删除的资源。 Finalizer 提醒控制器清理被删除的对象拥有的资源。

当你告诉 Kubernetes 删除一个指定了 Finalizer 的对象时， Kubernetes API 通过填充 ​`.metadata.deletionTimestamp`​ 来标记要删除的对象， 并返回​`202`​状态码 (HTTP "已接受") 使其进入只读状态。 此时控制平面或其他组件会采取 Finalizer 所定义的行动， 而目标对象仍然处于终止中（Terminating）的状态。 这些行动完成后，控制器会删除目标对象相关的 Finalizer。 当 ​`metadata.finalizers`​ 字段为空时，Kubernetes 认为删除已完成。

你可以使用 Finalizer 控制资源的垃圾收集。 例如，你可以定义一个 Finalizer，在删除目标资源前清理相关资源或基础设施。

你可以通过使用 Finalizers 提醒控制器 在删除目标资源前执行特定的清理任务， 来控制资源的垃圾收集。

Finalizers 通常不指定要执行的代码。 相反，它们通常是特定资源上的键的列表，类似于注解。 Kubernetes 自动指定了一些 Finalizers，但你也可以指定你自己的。

Finalizers 如何工作 
----------------

当你使用清单文件创建资源时，你可以在 ​`metadata.finalizers`​ 字段指定 Finalizers。 当你试图删除该资源时，处理删除请求的 API 服务器会注意到 ​`finalizers`​ 字段中的值， 并进行以下操作：

*   修改对象，将你开始执行删除的时间添加到 ​`metadata.deletionTimestamp`​ 字段。
*   禁止对象被删除，直到其 ​`metadata.finalizers`​ 字段为空。
*   返回 ​`202`​ 状态码（HTTP "Accepted"）。

管理 finalizer 的控制器注意到对象上发生的更新操作，对象的 ​`metadata.deletionTimestamp`​ 被设置，意味着已经请求删除该对象。然后，控制器会试图满足资源的 Finalizers 的条件。 每当一个 Finalizer 的条件被满足时，控制器就会从资源的 ​`finalizers` ​字段中删除该键。 当 ​`finalizers` ​字段为空时，​`deletionTimestamp` ​字段被设置的对象会被自动删除。 你也可以使用 Finalizers 来阻止删除未被管理的资源。

一个常见的 Finalizer 的例子是 ​`kubernetes.io/pv-protection`​， 它用来防止意外删除 ​`PersistentVolume` ​对象。 当一个 ​`PersistentVolume` ​对象被 Pod 使用时， Kubernetes 会添加 ​`pv-protection`​ Finalizer。 如果你试图删除 ​`PersistentVolume`​，它将进入 ​`Terminating` ​状态， 但是控制器因为该 Finalizer 存在而无法删除该资源。 当 Pod 停止使用 ​`PersistentVolume` ​时， Kubernetes 清除 ​`pv-protection`​ Finalizer，控制器就会删除该卷。

属主引用、标签和 Finalizers
-------------------

与标签类似， 属主引用 描述了 Kubernetes 中对象之间的关系，但它们作用不同。 当一个控制器 管理类似于 Pod 的对象时，它使用标签来跟踪相关对象组的变化。 例如，当 Job 创建一个或多个 Pod 时， Job 控制器会给这些 Pod 应用上标签，并跟踪集群中的具有相同标签的 Pod 的变化。

Job 控制器还为这些 Pod 添加了属主引用，指向创建 Pod 的 Job。 如果你在这些 Pod 运行的时候删除了 Job， Kubernetes 会使用属主引用（而不是标签）来确定集群中哪些 Pod 需要清理。

当 Kubernetes 识别到要删除的资源上的属主引用时，它也会处理 Finalizers。

在某些情况下，Finalizers 会阻止依赖对象的删除， 这可能导致目标属主对象被保留的时间比预期的长，而没有被完全删除。 在这些情况下，你应该检查目标属主和附属对象上的 Finalizers 和属主引用，来排查原因。

> 在对象卡在删除状态的情况下，要避免手动移除 Finalizers，以允许继续删除操作。 Finalizers 通常因为特殊原因被添加到资源上，所以强行删除它们会导致集群出现问题。 只有了解 finalizer 的用途时才能这样做，并且应该通过一些其他方式来完成 （例如，手动清除其余的依赖对象）。

##  8.  Kubernetes 字段选择器
字段选择器
-----

“字段选择器（Field selectors）”允许你根据一个或多个资源字段的值 筛选 Kubernetes 资源。 下面是一些使用字段选择器查询的例子：

*   ​`metadata.name=my-service` ​
*   ​`metadata.namespace!=default` ​
*   ​`status.phase=Pending` ​

下面这个 ​`kubectl` ​命令将筛选出 ​`status.phase`​ 字段值为 ​`Running` ​的所有 Pod：

`kubectl get pods --field-selector status.phase=Running`

> 字段选择器本质上是资源过滤器（Filters）。默认情况下，字段选择器/过滤器是未被应用的， 这意味着指定类型的所有资源都会被筛选出来。 这使得以下的两个 ​kubectl ​查询是等价的：
> 
> `kubectl get pods kubectl get pods --field-selector ""`

支持的字段 
------

不同的 Kubernetes 资源类型支持不同的字段选择器。 所有资源类型都支持 ​`metadata.name`​ 和 ​`metadata.namespace`​ 字段。 使用不被支持的字段选择器会产生错误。例如：

`kubectl get ingress --field-selector foo.bar=baz`

`Error from server (BadRequest): Unable to find "ingresses" that match label selector "", field selector "foo.bar=baz": "foo.bar" is not a known field selector: only "metadata.name", "metadata.namespace"`

支持的操作符 
-------

你可在字段选择器中使用 ​`=`​、​`==`​和 ​`!=`​ （​`=`​ 和 ​`==`​ 的意义是相同的）操作符。 例如，下面这个 ​`kubectl`​ 命令将筛选所有不属于 ​`default` ​命名空间的 Kubernetes 服务：

`kubectl get services  --all-namespaces --field-selector metadata.namespace!=default`

链式选择器 
------

同标签和其他选择器一样， 字段选择器可以通过使用逗号分隔的列表组成一个选择链。 下面这个 ​`kubectl` ​命令将筛选 ​`status.phase`​ 字段不等于 ​`Running` ​同时 ​`spec.restartPolicy`​ 字段等于 ​`Always` ​的所有 Pod：

`kubectl get pods --field-selector=status.phase!=Running,spec.restartPolicy=Always`

多种资源类型 
-------

你能够跨多种资源类型来使用字段选择器。 下面这个 ​`kubectl` ​命令将筛选出所有不在 ​`default` ​命名空间中的 StatefulSet 和 Service：

`kubectl get statefulsets,services --all-namespaces --field-selector metadata.namespace!=default`

##  9.  Kubernetes 属主与附属
属主与附属
-----

在 Kubernetes 中，一些对象是其他对象的属主（Owner）。 例如，ReplicaSet 是一组 Pod 的属主。 具有属主的对象是属主的附属（Dependent） 。

属主关系不同于一些资源使用的标签和选择算符机制。 例如，有一个创建 ​`EndpointSlice` ​对象的 Service， 该 Service 使用标签来让控制平面确定，哪些 ​`EndpointSlice` ​对象属于该 Service。 除开标签，每个代表 Service 所管理的 ​`EndpointSlice` ​都有一个属主引用。 属主引用避免 Kubernetes 的不同部分干扰到不受它们控制的对象。

对象规约中的属主引用 
-----------

附属对象有一个 ​`metadata.ownerReferences`​ 字段，用于引用其属主对象。 一个有效的属主引用，包含与附属对象同在一个命名空间下的对象名称和一个 UID。 Kubernetes 自动为一些对象的附属资源设置属主引用的值， 这些对象包含 ReplicaSet、DaemonSet、Deployment、Job、CronJob、ReplicationController 等。 你也可以通过改变这个字段的值，来手动配置这些关系。 然而，你通常不需要这么做，你可以让 Kubernetes 自动管理附属关系。

附属对象还有一个 ​`ownerReferences.blockOwnerDeletion`​ 字段，该字段使用布尔值， 用于控制特定的附属对象是否可以阻止垃圾收集删除其属主对象。 如果控制器（例如 Deployment 控制器） 设置了 ​`metadata.ownerReferences`​ 字段的值，Kubernetes 会自动设置 ​`blockOwnerDeletion` ​的值为 ​`true`​。 你也可以手动设置 ​`blockOwnerDeletion` ​字段的值，以控制哪些附属对象会阻止垃圾收集。

> 根据设计，kubernetes 不允许跨名字空间指定属主。 名字空间范围的附属可以指定集群范围的或者名字空间范围的属主。 名字空间范围的属主必须和该附属处于相同的名字空间。 如果名字空间范围的属主和附属不在相同的名字空间，那么该属主引用就会被认为是缺失的， 并且当附属的所有属主引用都被确认不再存在之后，该附属就会被删除。  
>   
> 集群范围的附属只能指定集群范围的属主。 在 v1.20+ 版本，如果一个集群范围的附属指定了一个名字空间范围类型的属主， 那么该附属就会被认为是拥有一个不可解析的属主引用，并且它不能够被垃圾回收。  
>   
> 在 v1.20+ 版本，如果垃圾收集器检测到无效的跨名字空间的属主引用， 或者一个集群范围的附属指定了一个名字空间范围类型的属主， 那么它就会报告一个警告事件。该事件的原因是 ​`OwnerRefInvalidNamespace`​， ​`involvedObject` ​属性中包含无效的附属。 你可以运行 ​`kubectl get events -A --field-selector=reason=OwnerRefInvalidNamespace`​ 来获取该类型的事件。

属主关系与 Finalizer 
----------------

当你告诉 Kubernetes 删除一个资源，API 服务器允许管理控制器处理该资源的任何 Finalizer 规则。 Finalizer 防止意外删除你的集群所依赖的、用于正常运作的资源。 例如，如果你试图删除一个仍被 Pod 使用的 ​`PersistentVolume`​，该资源不会被立即删除， 因为 ​`PersistentVolume` ​有 ​`kubernetes.io/pv-protection`​ Finalizer。 相反，它将进入 ​`Terminating` ​状态，直到 Kubernetes 清除这个 Finalizer， 而这种情况只会发生在 ​`PersistentVolume` ​不再被挂载到 Pod 上时。

当你使用前台或孤立级联删除时， Kubernetes 也会向属主资源添加 Finalizer。 在前台删除中，会添加 ​`foreground` ​Finalizer，这样控制器必须在删除了拥有 ​`ownerReferences.blockOwnerDeletion=true`​ 的附属资源后，才能删除属主对象。 如果你指定了孤立删除策略，Kubernetes 会添加 ​`orphan` ​Finalizer， 这样控制器在删除属主对象后，会忽略附属资源。

##  10.  Kubernetes 推荐使用的标签
推荐使用的标签
-------

除了 kubectl 和 dashboard 之外，您可以使用其他工具来可视化和管理 Kubernetes 对象。一组通用的标签可以让多个工具之间相互操作，用所有工具都能理解的通用方式描述对象。

除了支持工具外，推荐的标签还以一种可以查询的方式描述了应用程序。

元数据围绕 应用（application） 的概念进行组织。Kubernetes 不是 平台即服务（PaaS），没有或强制执行正式的应用程序概念。 相反，应用程序是非正式的，并使用元数据进行描述。应用程序包含的定义是松散的。

> 这些是推荐的标签。它们使管理应用程序变得更容易但不是任何核心工具所必需的。

共享标签和注解都使用同一个前缀：​`app.kubernetes.io`​。没有前缀的标签是用户私有的。共享前缀可以确保共享标签不会干扰用户自定义的标签。

标签
--

为了充分利用这些标签，应该在每个资源对象上都使用它们。

键

描述

示例

类型

`app.kubernetes.io/name`

应用程序的名称

`mysql`

字符串

`app.kubernetes.io/instance`

用于唯一确定应用实例的名称

`mysql-abcxzy`

字符串

`app.kubernetes.io/version`

应用程序的当前版本（例如，语义版本，修订版哈希等）

`5.7.21`

字符串

`app.kubernetes.io/component`

架构中的组件

`database`

字符串

`app.kubernetes.io/part-of`

此级别的更高级别应用程序的名称

`wordpress`

字符串

`app.kubernetes.io/managed-by`

用于管理应用程序的工具

`helm`

字符串

`app.kubernetes.io/created-by`

创建该资源的控制器或者用户

`controller-manager`

字符串

为说明这些标签的实际使用情况，请看下面的 StatefulSet 对象：

`# 这是一段节选 apiVersion: apps/v1 kind: StatefulSet metadata:   labels:     app.kubernetes.io/name: mysql     app.kubernetes.io/instance: mysql-abcxzy     app.kubernetes.io/version: "5.7.21"     app.kubernetes.io/component: database     app.kubernetes.io/part-of: wordpress     app.kubernetes.io/managed-by: helm     app.kubernetes.io/created-by: controller-manager`

应用和应用实例
-------

应用可以在 Kubernetes 集群中安装一次或多次。在某些情况下，可以安装在同一命名空间中。例如，可以不止一次地为不同的站点安装不同的 WordPress。

应用的名称和实例的名称是分别记录的。例如，WordPress 应用的 ​`app.kubernetes.io/name`​ 为 ​`wordpress`​，而其实例名称 ​`app.kubernetes.io/instance`​ 为 ​`wordpress-abcxzy`​。 这使得应用和应用的实例均可被识别，应用的每个实例都必须具有唯一的名称。

示例
--

为了说明使用这些标签的不同方式，以下示例具有不同的复杂性。

### 一个简单的无状态服务

考虑使用 ​`Deployment` ​和 ​`Service` ​对象部署的简单无状态服务的情况。以下两个代码段表示如何以最简单的形式使用标签。

下面的 ​`Deployment` ​用于监督运行应用本身的 pods。

`apiVersion: apps/v1 kind: Deployment metadata:   labels:     app.kubernetes.io/name: myservice     app.kubernetes.io/instance: myservice-abcxzy ...`

下面的 ​`Service` ​用于暴露应用。

`apiVersion: v1 kind: Service metadata:   labels:     app.kubernetes.io/name: myservice     app.kubernetes.io/instance: myservice-abcxzy ...`

### 带有一个数据库的 Web 应用程序

考虑一个稍微复杂的应用：一个使用 Helm 安装的 Web 应用（WordPress），其中 使用了数据库（MySQL）。以下代码片段说明用于部署此应用程序的对象的开始。

以下 ​`Deployment` ​的开头用于 WordPress：

`apiVersion: apps/v1 kind: Deployment metadata:   labels:     app.kubernetes.io/name: wordpress     app.kubernetes.io/instance: wordpress-abcxzy     app.kubernetes.io/version: "4.9.4"     app.kubernetes.io/managed-by: helm     app.kubernetes.io/component: server     app.kubernetes.io/part-of: wordpress ...`

这个 ​`Service` ​用于暴露 WordPress：

`apiVersion: v1 kind: Service metadata:   labels:     app.kubernetes.io/name: wordpress     app.kubernetes.io/instance: wordpress-abcxzy     app.kubernetes.io/version: "4.9.4"     app.kubernetes.io/managed-by: helm     app.kubernetes.io/component: server     app.kubernetes.io/part-of: wordpress ...`

MySQL 作为一个 ​`StatefulSet` ​暴露，包含它和它所属的较大应用程序的元数据：

`apiVersion: apps/v1 kind: StatefulSet metadata:   labels:     app.kubernetes.io/name: mysql     app.kubernetes.io/instance: mysql-abcxzy     app.kubernetes.io/version: "5.7.21"     app.kubernetes.io/managed-by: helm     app.kubernetes.io/component: database     app.kubernetes.io/part-of: wordpress ...`

​`Service` ​用于将 MySQL 作为 WordPress 的一部分暴露：

`apiVersion: v1 kind: Service metadata:   labels:     app.kubernetes.io/name: mysql     app.kubernetes.io/instance: mysql-abcxzy     app.kubernetes.io/version: "5.7.21"     app.kubernetes.io/managed-by: helm     app.kubernetes.io/component: database     app.kubernetes.io/part-of: wordpress ...`

使用 MySQL ​`StatefulSet` ​和 ​`Service`​，您会注意到有关 MySQL 和 Wordpress 的信息，包括更广泛的应用程序。

#  5.  Kubernetes 架构

##  1.  Kubernetes 节点
节点
--

Kubernetes 通过将容器放入在节点（Node）上运行的 Pod 中来执行你的工作负载。 节点可以是一个虚拟机或者物理机器，取决于所在的集群配置。 每个节点包含运行 Pods 所需的服务； 这些节点由 控制面 负责管理。

通常集群中会有若干个节点；而在一个学习用或者资源受限的环境中，你的集群中也可能 只有一个节点。

节点上的组件包括 kubelet、 容器运行时以及 kube-proxy。

管理 
---

向 API 服务器添加节点的方式主要有两种：

1.  节点上的 ​`kubelet` ​向控制面执行自注册；
2.  你，或者别的什么人，手动添加一个 Node 对象。

在你创建了 Node 对象或者节点上的 ​`kubelet` ​执行了自注册操作之后，控制面会检查新的 Node 对象是否合法。 例如，如果你尝试使用下面的 JSON 对象来创建 Node 对象：

`{   "kind": "Node",   "apiVersion": "v1",   "metadata": {     "name": "10.240.79.157",     "labels": {       "name": "my-first-k8s-node"     }   } }`

Kubernetes 会在内部创建一个 Node 对象作为节点的表示。Kubernetes 检查 ​`kubelet` ​向 API 服务器注册节点时使用的 ​`metadata.name`​ 字段是否匹配。 如果节点是健康的（即所有必要的服务都在运行中），则该节点可以用来运行 Pod。 否则，直到该节点变为健康之前，所有的集群活动都会忽略该节点。

> Kubernetes 会一直保存着非法节点对应的对象，并持续检查该节点是否已经变得健康。 你，或者某个控制器必须显式地删除该 Node 对象以停止健康检查操作。

Node 对象的名称必须是合法的 DNS 子域名。

### 节点名称唯一性 

节点的名称用来标识 Node 对象。 没有两个 Node 可以同时使用相同的名称。 Kubernetes 还假定名字相同的资源是同一个对象。 就 Node 而言，隐式假定使用相同名称的实例会具有相同的状态（例如网络配置、根磁盘内容） 和类似节点标签这类属性。这可能在节点被更改但其名称未变时导致系统状态不一致。 如果某个 Node 需要被替换或者大量变更，需要从 API 服务器移除现有的 Node 对象， 之后再在更新之后重新将其加入。

### 节点自注册

当 kubelet 标志 ​`--register-node`​ 为 true（默认）时，它会尝试向 API 服务注册自己。 这是首选模式，被绝大多数发行版选用。

对于自注册模式，kubelet 使用下列参数启动：

*   ​`--kubeconfig`​ - 用于向 API 服务器执行身份认证所用的凭据的路径。
*   ​`--cloud-provider`​ - 与某云驱动 进行通信以读取与自身相关的元数据的方式。
*   ​`--register-node`​ - 自动向 API 服务注册。
*   ​`--register-with-taints`​ - 使用所给的污点列表 （逗号分隔的 ​`<key>=<value>:<effect>`​）注册节点。当 ​`register-node`​ 为 false 时无效。
*   ​`--node-ip`​ - 节点 IP 地址。
*   ​`--node-labels`​ - 在集群中注册节点时要添加的标签。
*   ​`--node-status-update-frequency`​ - 指定 kubelet 向控制面发送状态的频率。

启用Node 鉴权模式和 NodeRestriction 准入插件时， 仅授权 ​`kubelet` ​创建或修改其自己的节点资源。

> 正如节点名称唯一性一节所述，当 Node 的配置需要被更新时， 一种好的做法是重新向 API 服务器注册该节点。例如，如果 kubelet 重启时其 ​`--node-labels`​ 是新的值集，但同一个 Node 名称已经被使用，则所作变更不会起作用， 因为节点标签是在 Node 注册时完成的。  
>   
> 如果在 kubelet 重启期间 Node 配置发生了变化，已经被调度到某 Node 上的 Pod 可能会出现行为不正常或者出现其他问题，例如，已经运行的 Pod 可能通过污点机制设置了与 Node 上新设置的标签相排斥的规则，也有一些其他 Pod， 本来与此 Pod 之间存在不兼容的问题，也会因为新的标签设置而被调到到同一节点。 节点重新注册操作可以确保节点上所有 Pod 都被排空并被正确地重新调度。

### 手动节点管理 

你可以使用 kubectl 来创建和修改 Node 对象。

如果你希望手动创建节点对象时，请设置 kubelet 标志 ​`--register-node=false`​。

你可以修改 Node 对象（忽略 ​`--register-node`​ 设置）。 例如，修改节点上的标签或标记其为不可调度。

你可以结合使用 Node 上的标签和 Pod 上的选择算符来控制调度。 例如，你可以限制某 Pod 只能在符合要求的节点子集上运行。

如果标记节点为不可调度（unschedulable），将阻止新 Pod 调度到该 Node 之上， 但不会影响任何已经在其上的 Pod。 这是重启节点或者执行其他维护操作之前的一个有用的准备步骤。

要标记一个 Node 为不可调度，执行以下命令：

`kubectl cordon $NODENAME`

被 DaemonSet 控制器创建的 Pod 能够容忍节点的不可调度属性。 DaemonSet 通常提供节点本地的服务，即使节点上的负载应用已经被腾空， 这些服务也仍需运行在节点之上。

节点状态 
-----

一个节点的状态包含以下信息:

*   地址（Addresses）
*   状况（Condition）
*   容量与可分配（Capacity）
*   信息（Info）

你可以使用 ​`kubectl` ​来查看节点状态和其他细节信息：

`kubectl describe node <节点名称>`

下面对每个部分进行详细描述。

### 地址 

这些字段的用法取决于你的云服务商或者物理机配置。

*   HostName：由节点的内核报告。可以通过 kubelet 的 ​`--hostname-override`​ 参数覆盖。
*   ExternalIP：通常是节点的可外部路由（从集群外可访问）的 IP 地址。
*   InternalIP：通常是节点的仅可在集群内部路由的 IP 地址。

### 状况

​`conditions` ​字段描述了所有 ​`Running` ​节点的状况。状况的示例包括：

节点状况

描述

`Ready`

如节点是健康的并已经准备好接收 Pod 则为 `True`；`False` 表示节点不健康而且不能接收 Pod；`Unknown` 表示节点控制器在最近 `node-monitor-grace-period` 期间（默认 40 秒）没有收到节点的消息

`DiskPressure`

`True` 表示节点存在磁盘空间压力，即磁盘可用量低, 否则为 `False`

`MemoryPressure`

`True` 表示节点存在内存压力，即节点内存可用量低，否则为 `False`

`PIDPressure`

`True` 表示节点存在进程压力，即节点上进程过多；否则为 `False`

`NetworkUnavailable`

`True` 表示节点网络配置不正确；否则为 `False`

> 如果使用命令行工具来打印已保护（Cordoned）节点的细节，其中的 Condition 字段可能包括 ​`SchedulingDisabled`​。​`SchedulingDisabled` ​不是 Kubernetes API 中定义的 Condition，被保护起来的节点在其规约中被标记为不可调度（Unschedulable）。

在 Kubernetes API 中，节点的状况表示节点资源中​`.status`​ 的一部分。 例如，以下 JSON 结构描述了一个健康节点：

`"conditions": [   {     "type": "Ready",     "status": "True",     "reason": "KubeletReady",     "message": "kubelet is posting ready status",     "lastHeartbeatTime": "2019-06-05T18:38:35Z",     "lastTransitionTime": "2019-06-05T11:41:27Z"   } ]`

如果 Ready 状况的 ​`status` ​处于 ​`Unknown` ​或者 ​`False` ​状态的时间超过了 ​`pod-eviction-timeout`​ 值（一个传递给 kube-controller-manager 的参数），节点控制器会对节点上的所有 Pod 触发 API-发起的驱逐。 默认的逐出超时时长为 5 分钟。

某些情况下，当节点不可达时，API 服务器不能和其上的 kubelet 通信。 删除 Pod 的决定不能传达给 kubelet，直到它重新建立和 API 服务器的连接为止。 与此同时，被计划删除的 Pod 可能会继续在游离的节点上运行。

节点控制器在确认 Pod 在集群中已经停止运行前，不会强制删除它们。 你可以看到可能在这些无法访问的节点上运行的 Pod 处于 ​`Terminating` ​或者 ​`Unknown` ​状态。 如果 kubernetes 不能基于下层基础设施推断出某节点是否已经永久离开了集群， 集群管理员可能需要手动删除该节点对象。 从 Kubernetes 删除节点对象将导致 API 服务器删除节点上所有运行的 Pod 对象并释放它们的名字。

当节点上出现问题时，Kubernetes 控制面会自动创建与影响节点的状况对应的 污点。 调度器在将 Pod 指派到某 Node 时会考虑 Node 上的污点设置。 Pod 也可以设置容忍度， 以便能够在设置了特定污点的 Node 上运行。

### 容量（Capacity）与可分配（Allocatable） 

这两个值描述节点上的可用资源：CPU、内存和可以调度到节点上的 Pod 的个数上限。

​`capacity` ​块中的字段标示节点拥有的资源总量。 ​`allocatable` ​块指示节点上可供普通 Pod 消耗的资源量。

### 信息（Info）

Info 指的是节点的一般信息，如内核版本、Kubernetes 版本（​`kubelet` ​和 ​`kube-proxy`​ 版本）、 容器运行时详细信息，以及节点使用的操作系统。 ​`kubelet` ​从节点收集这些信息并将其发布到 Kubernetes API。

心跳 
---

Kubernetes 节点发送的心跳帮助你的集群确定每个节点的可用性，并在检测到故障时采取行动。

对于节点，有两种形式的心跳:

*   更新节点的 ​`.status` ​
*   ​`kube-node-lease`​ 名字空间中的 Lease（租约）对象。 每个节点都有一个关联的 Lease 对象。

与 Node 的 ​`.status`​ 更新相比，Lease 是一种轻量级资源。 使用 Lease 来表达心跳在大型集群中可以减少这些更新对性能的影响。

kubelet 负责创建和更新节点的 ​`.status`​，以及更新它们对应的 Lease。

*   当节点状态发生变化时，或者在配置的时间间隔内没有更新事件时，kubelet 会更新 ​`.status`​。 ​`.status`​ 更新的默认间隔为 5 分钟（比节点不可达事件的 40 秒默认超时时间长很多）。
*   ​`kubelet` ​会创建并每 10 秒（默认更新间隔时间）更新 Lease 对象。 Lease 的更新独立于 Node 的 ​`.status`​ 更新而发生。 如果 Lease 的更新操作失败，kubelet 会采用指数回退机制，从 200 毫秒开始重试， 最长重试间隔为 7 秒钟。

节点控制器 
------

节点控制器是 Kubernetes 控制面组件， 管理节点的方方面面。

节点控制器在节点的生命周期中扮演多个角色。 第一个是当节点注册时为它分配一个 CIDR 区段（如果启用了 CIDR 分配）。

第二个是保持节点控制器内的节点列表与云服务商所提供的可用机器列表同步。 如果在云环境下运行，只要某节点不健康，节点控制器就会询问云服务是否节点的虚拟机仍可用。 如果不可用，节点控制器会将该节点从它的节点列表删除。

第三个是监控节点的健康状况。节点控制器负责：

*   在节点不可达的情况下，在 Node 的 ​`.status`​ 中更新 ​`Ready` ​状况。 在这种情况下，节点控制器将 NodeReady 状况更新为 ​`Unknown` ​。
*   如果节点仍然无法访问：对于不可达节点上的所有 Pod 触发 API 发起的逐出操作。 默认情况下，节点控制器在将节点标记为 ​`Unknown` ​后等待 5 分钟提交第一个驱逐请求。

默认情况下，节点控制器每 5 秒检查一次节点状态，可以使用 ​`kube-controller-manager`​ 组件上的 ​`--node-monitor-period`​ 参数来配置周期。

### 逐出速率限制 

大部分情况下，节点控制器把逐出速率限制在每秒 ​`--node-eviction-rate`​ 个（默认为 0.1）。 这表示它每 10 秒钟内至多从一个节点驱逐 Pod。

当一个可用区域（Availability Zone）中的节点变为不健康时，节点的驱逐行为将发生改变。 节点控制器会同时检查可用区域中不健康（​`Ready` ​状况为 ​`Unknown` ​或 ​`False`​） 的节点的百分比：

*   如果不健康节点的比例超过 ​`--unhealthy-zone-threshold`​ （默认为 0.55）， 驱逐速率将会降低。
*   如果集群较小（意即小于等于 ​`--large-cluster-size-threshold`​ 个节点 - 默认为 50）， 驱逐操作将会停止。
*   否则驱逐速率将降为每秒 ​`--secondary-node-eviction-rate`​ 个（默认为 0.01）。

在逐个可用区域中实施这些策略的原因是， 当一个可用区域可能从控制面脱离时其它可用区域可能仍然保持连接。 如果你的集群没有跨越云服务商的多个可用区域，那（整个集群）就只有一个可用区域。

跨多个可用区域部署你的节点的一个关键原因是当某个可用区域整体出现故障时， 工作负载可以转移到健康的可用区域。 因此，如果一个可用区域中的所有节点都不健康时，节点控制器会以正常的速率 ​`--node-eviction-rate`​ 进行驱逐操作。 在所有的可用区域都不健康（也即集群中没有健康节点）的极端情况下， 节点控制器将假设控制面与节点间的连接出了某些问题，它将停止所有驱逐动作 （如果故障后部分节点重新连接，节点控制器会从剩下不健康或者不可达节点中驱逐 Pod）。

节点控制器还负责驱逐运行在拥有 ​`NoExecute` ​污点的节点上的 Pod， 除非这些 Pod 能够容忍此污点。 节点控制器还负责根据节点故障（例如节点不可访问或没有就绪） 为其添加污点。 这意味着调度器不会将 Pod 调度到不健康的节点上。

### 资源容量跟踪 

Node 对象会跟踪节点上资源的容量（例如可用内存和 CPU 数量）。 通过自注册机制生成的 Node 对象会在注册期间报告自身容量。 如果你手动添加了 Node， 你就需要在添加节点时手动设置节点容量。

Kubernetes 调度器 保证节点上有足够的资源供其上的所有 Pod 使用。 它会检查节点上所有容器的请求的总和不会超过节点的容量。 总的请求包括由 kubelet 启动的所有容器，但不包括由容器运行时直接启动的容器， 也不包括不受 ​`kubelet` ​控制的其他进程。

节点拓扑 
-----

FEATURE STATE: Kubernetes v1.18 \[beta\]

如果启用了 ​`TopologyManager` ​特性门控， ​`kubelet` ​可以在作出资源分配决策时使用拓扑提示。

节点体面关闭
------

FEATURE STATE: Kubernetes v1.21 \[beta\]

kubelet 会尝试检测节点系统关闭事件并终止在节点上运行的 Pods。

在节点终止期间，kubelet 保证 Pod 遵从常规的 Pod 终止流程。

体面节点关闭特性依赖于 systemd，因为它要利用 systemd 抑制器锁机制， 在给定的期限内延迟节点关闭。

体面节点关闭特性受 ​`GracefulNodeShutdown` ​特性门控控制， 在 1.21 版本中是默认启用的。

注意，默认情况下，下面描述的两个配置选项，shutdownGracePeriod 和 shutdownGracePeriodCriticalPods 都是被设置为 0 的，因此不会激活体面节点关闭功能。 要激活此功能特性，这两个 kubelet 配置选项要适当配置，并设置为非零值。

在体面关闭节点过程中，kubelet 分两个阶段来终止 Pod：

1.  终止在节点上运行的常规 Pod。
2.  终止在节点上运行的关键 Pod。

节点体面关闭的特性对应两个 ​`KubeletConfiguration` ​选项：

*   ​`shutdownGracePeriod`​：

*   指定节点应延迟关闭的总持续时间。此时间是 Pod 体面终止的时间总和，不区分常规 Pod 还是关键 Pod。

*   ​`shutdownGracePeriodCriticalPods`​：

*   在节点关闭期间指定用于终止关键 Pod 的持续时间。该值应小于 ​`shutdownGracePeriod`​。

例如，如果设置了 ​`shutdownGracePeriod=30s`​ 和 ​`shutdownGracePeriodCriticalPods=10s`​， 则 kubelet 将延迟 30 秒关闭节点。 在关闭期间，将保留前 20（30 - 10）秒用于体面终止常规 Pod， 而保留最后 10 秒用于终止关键 Pod。

> 当 Pod 在正常节点关闭期间被驱逐时，它们会被标记为已经失败（Failed）。 运行 ​`kubectl get pods`​ 时，被驱逐的 Pod 的状态显示为 ​`Shutdown`​。 并且 ​`kubectl describe pod`​ 表示 Pod 因节点关闭而被驱逐：
> 
> `Reason:         Terminated Message:        Pod was terminated in response to imminent node shutdown.`
> 
>         

### 基于 Pod 优先级的体面节点关闭 

FEATURE STATE: Kubernetes v1.23 \[alpha\]

为了在体面节点关闭期间提供更多的灵活性，尤其是处理关闭期间的 Pod 排序问题， 体面节点关闭机制能够关注 Pod 的 PriorityClass 设置，前提是你已经在集群中启用了此功能特性。 此功能特性允许集群管理员基于 Pod 的优先级类（Priority Class） 显式地定义体面节点关闭期间 Pod 的处理顺序。

前文所述的体面节点关闭特性能够分两个阶段关闭 Pod， 首先关闭的是非关键的 Pod，之后再处理关键 Pod。 如果需要显式地以更细粒度定义关闭期间 Pod 的处理顺序，需要一定的灵活度， 这时可以使用基于 Pod 优先级的体面关闭机制。

当体面节点关闭能够处理 Pod 优先级时，体面节点关闭的处理可以分为多个阶段， 每个阶段关闭特定优先级类的 Pod。kubelet 可以被配置为按确切的阶段处理 Pod， 且每个阶段可以独立设置关闭时间。

假设集群中存在以下自定义的 Pod 优先级类。

Pod 优先级类名称

Pod 优先级类数值

`custom-class-a`

100000

`custom-class-b`

10000

`custom-class-c`

1000

`regular/unset`

0

在 kubelet 配置中， ​`shutdownGracePeriodByPodPriority` ​可能看起来是这样：

Pod 优先级类数值

关闭期限

100000

10 秒

10000

180 秒

1000

120 秒

0

60 秒

对应的 kubelet 配置 YAML 将会是：

`shutdownGracePeriodByPodPriority:   - priority: 100000     shutdownGracePeriodSeconds: 10   - priority: 10000     shutdownGracePeriodSeconds: 180   - priority: 1000     shutdownGracePeriodSeconds: 120   - priority: 0     shutdownGracePeriodSeconds: 60`

上面的表格表明，所有 ​`priority` ​值大于等于 100000 的 Pod 会得到 10 秒钟期限停止， 所有 ​`priority` ​值介于 10000 和 100000 之间的 Pod 会得到 180 秒钟期限停止， 所有 ​`priority` ​值介于 1000 和 10000 之间的 Pod 会得到 120 秒钟期限停止， 所有其他 Pod 将获得 60 秒的时间停止。

用户不需要为所有的优先级类都设置数值。例如，你也可以使用下面这种配置：

Pod 优先级类数值

关闭期限

100000

300 秒

1000

120 秒

0

60 秒

在上面这个场景中，优先级类为 ​`custom-class-b`​ 的 Pod 会与优先级类为 ​`custom-class-c`​ 的 Pod 在关闭时按相同期限处理。

如果在特定的范围内不存在 Pod，则 kubelet 不会等待对应优先级范围的 Pod。 kubelet 会直接跳到下一个优先级数值范围进行处理。

如果此功能特性被启用，但没有提供配置数据，则不会出现排序操作。

使用此功能特性需要启用 ​`GracefulNodeShutdownBasedOnPodPriority` ​特性门控， 并将 kubelet 配置中的 ​`shutdownGracePeriodByPodPriority` ​设置为期望的配置， 其中包含 Pod 的优先级类数值以及对应的关闭期限。

kubelet 子系统中会生成 ​`graceful_shutdown_start_time_seconds` ​和 ​`graceful_shutdown_end_time_seconds` ​度量指标以便监视节点关闭行为。

交换内存管理
------

FEATURE STATE: Kubernetes v1.22 \[alpha\]

在 Kubernetes 1.22 之前，节点不支持使用交换内存，并且默认情况下， 如果在节点上检测到交换内存配置，kubelet 将无法启动。 在 1.22 以后，可以逐个节点地启用交换内存支持。

要在节点上启用交换内存，必须启用kubelet 的 ​`NodeSwap` ​特性门控， 同时使用 ​`--fail-swap-on`​ 命令行参数或者将 ​`failSwapOn` ​配置设置为 false。

用户还可以选择配置 ​`memorySwap.swapBehavior`​ 以指定节点使用交换内存的方式。例如:

`memorySwap:   swapBehavior: LimitedSwap`

可用的 ​`swapBehavior` ​的配置选项有：

*   ​`LimitedSwap`​：Kubernetes 工作负载的交换内存会受限制。 不受 Kubernetes 管理的节点上的工作负载仍然可以交换。
*   ​`UnlimitedSwap`​：Kubernetes 工作负载可以使用尽可能多的交换内存请求， 一直到达到系统限制为止。

如果启用了特性门控但是未指定 ​`memorySwap` ​的配置，默认情况下 kubelet 将使用 ​`LimitedSwap` ​设置。

​`LimitedSwap` ​这种设置的行为取决于节点运行的是 v1 还是 v2 的控制组（也就是 ​`cgroups`​）：

*   cgroupsv1: Kubernetes 工作负载可以使用内存和交换，上限为 Pod 的内存限制值（如果设置了的话）。
*   cgroupsv2: Kubernetes 工作负载不能使用交换内存。

##  2.  Kubernetes 控制面到节点通信
控制面到节点通信
--------

本文列举控制面节点（确切说是 API 服务器）和 Kubernetes 集群之间的通信路径。 目的是为了让用户能够自定义他们的安装，以实现对网络配置的加固，使得集群能够在不可信的网络上 （或者在一个云服务商完全公开的 IP 上）运行。

节点到控制面
------

Kubernetes 采用的是中心辐射型（Hub-and-Spoke）API 模式。 所有从集群（或所运行的 Pods）发出的 API 调用都终止于 API 服务器。 其它控制面组件都没有被设计为可暴露远程服务。 API 服务器被配置为在一个安全的 HTTPS 端口（通常为 443）上监听远程连接请求， 并启用一种或多种形式的客户端身份认证机制。 一种或多种客户端鉴权机制应该被启用， 特别是在允许使用匿名请求 或服务账号令牌的时候。

应该使用集群的公共根证书开通节点，这样它们就能够基于有效的客户端凭据安全地连接 API 服务器。 一种好的方法是以客户端证书的形式将客户端凭据提供给 kubelet。 请查看 kubelet TLS 启动引导 以了解如何自动提供 kubelet 客户端证书。

想要连接到 API 服务器的 Pod 可以使用服务账号安全地进行连接。 当 Pod 被实例化时，Kubernetes 自动把公共根证书和一个有效的持有者令牌注入到 Pod 里。 ​`kubernetes` ​服务（位于 ​`default` ​名字空间中）配置了一个虚拟 IP 地址，用于（通过 kube-proxy）转发 请求到 API 服务器的 HTTPS 末端。

控制面组件也通过安全端口与集群的 API 服务器通信。

这样，从集群节点和节点上运行的 Pod 到控制面的连接的缺省操作模式即是安全的， 能够在不可信的网络或公网上运行。

控制面到节点
------

从控制面（API 服务器）到节点有两种主要的通信路径。 第一种是从 API 服务器到集群中每个节点上运行的 kubelet 进程。 第二种是从 API 服务器通过它的代理功能连接到任何节点、Pod 或者服务。

### API 服务器到 kubelet

从 API 服务器到 kubelet 的连接用于：

*   获取 Pod 日志
*   挂接（通过 kubectl）到运行中的 Pod
*   提供 kubelet 的端口转发功能。

这些连接终止于 kubelet 的 HTTPS 末端。 默认情况下，API 服务器不检查 kubelet 的服务证书。这使得此类连接容易受到中间人攻击， 在非受信网络或公开网络上运行也是 不安全的。

为了对这个连接进行认证，使用 ​`--kubelet-certificate-authority`​ 标志给 API 服务器提供一个根证书包，用于 kubelet 的服务证书。

如果无法实现这点，又要求避免在非受信网络或公共网络上进行连接，可在 API 服务器和 kubelet 之间使用 SSH 隧道。

最后，应该启用 kubelet 用户认证和/或鉴权 来保护 kubelet API。

### API 服务器到节点、Pod 和服务

从 API 服务器到节点、Pod 或服务的连接默认为纯 HTTP 方式，因此既没有认证，也没有加密。 这些连接可通过给 API URL 中的节点、Pod 或服务名称添加前缀 ​`https:`​ 来运行在安全的 HTTPS 连接上。 不过这些连接既不会验证 HTTPS 末端提供的证书，也不会提供客户端证书。 因此，虽然连接是加密的，仍无法提供任何完整性保证。 这些连接 目前还不能安全地 在非受信网络或公共网络上运行。

### SSH 隧道 

Kubernetes 支持使用 SSH 隧道来保护从控制面到节点的通信路径。在这种配置下，API 服务器建立一个到集群中各节点的 SSH 隧道（连接到在 22 端口监听的 SSH 服务） 并通过这个隧道传输所有到 kubelet、节点、Pod 或服务的请求。 这一隧道保证通信不会被暴露到集群节点所运行的网络之外。

SSH 隧道目前已被废弃。除非你了解个中细节，否则不应使用。 Konnectivity 服务是对此通信通道的替代品。

### Konnectivity 服务 

FEATURE STATE: Kubernetes v1.18 \[beta\]

作为 SSH 隧道的替代方案，Konnectivity 服务提供 TCP 层的代理，以便支持从控制面到集群的通信。 Konnectivity 服务包含两个部分：Konnectivity 服务器和 Konnectivity 代理，分别运行在 控制面网络和节点网络中。Konnectivity 代理建立并维持到 Konnectivity 服务器的网络连接。 启用 Konnectivity 服务之后，所有控制面到节点的通信都通过这些连接传输。

##  3.  Kubernetes 控制器
控制器
---

在机器人技术和自动化领域，控制回路（Control Loop）是一个非终止回路，用于调节系统状态。

这是一个控制环的例子：房间里的温度自动调节器。

当你设置了温度，告诉了温度自动调节器你的期望状态（Desired State）。 房间的实际温度是当前状态（Current State）。 通过对设备的开关控制，温度自动调节器让其当前状态接近期望状态。

在 Kubernetes 中，控制器通过监控集群 的公共状态，并致力于将当前状态转变为期望的状态。

控制器模式
-----

一个控制器至少追踪一种类型的 Kubernetes 资源。这些 对象 有一个代表期望状态的 ​`spec` ​字段。 该资源的控制器负责确保其当前状态接近期望状态。

控制器可能会自行执行操作；在 Kubernetes 中更常见的是一个控制器会发送信息给 API 服务器，这会有副作用。 具体可参看后文的例子。

### 通过 API 服务器来控制

Job 控制器是一个 Kubernetes 内置控制器的例子。 内置控制器通过和集群 API 服务器交互来管理状态。

Job 是一种 Kubernetes 资源，它运行一个或者多个 Pod， 来执行一个任务然后停止。 （一旦被调度了，对 kubelet 来说 Pod 对象就会变成了期望状态的一部分）。

在集群中，当 Job 控制器拿到新任务时，它会保证一组 Node 节点上的 ​`kubelet` ​可以运行正确数量的 Pod 来完成工作。 Job 控制器不会自己运行任何的 Pod 或者容器。Job 控制器是通知 API 服务器来创建或者移除 Pod。 控制面中的其它组件 根据新的消息作出反应（调度并运行新 Pod）并且最终完成工作。

创建新 Job 后，所期望的状态就是完成这个 Job。Job 控制器会让 Job 的当前状态不断接近期望状态：创建为 Job 要完成工作所需要的 Pod，使 Job 的状态接近完成。

控制器也会更新配置对象。例如：一旦 Job 的工作完成了，Job 控制器会更新 Job 对象的状态为 ​`Finished`​。

（这有点像温度自动调节器关闭了一个灯，以此来告诉你房间的温度现在到你设定的值了）。

### 直接控制

相比 Job 控制器，有些控制器需要对集群外的一些东西进行修改。

例如，如果你使用一个控制回路来保证集群中有足够的 节点，那么控制器就需要当前集群外的 一些服务在需要时创建新节点。

和外部状态交互的控制器从 API 服务器获取到它想要的状态，然后直接和外部系统进行通信 并使当前状态更接近期望状态。

（实际上有一个控制器 可以水平地扩展集群中的节点。）

这里，很重要的一点是，控制器做出了一些变更以使得事物更接近你的期望状态， 之后将当前状态报告给集群的 API 服务器。 其他控制回路可以观测到所汇报的数据的这种变化并采取其各自的行动。

在温度计的例子中，如果房间很冷，那么某个控制器可能还会启动一个防冻加热器。 就 Kubernetes 集群而言，控制面间接地与 IP 地址管理工具、存储服务、云驱动 APIs 以及其他服务协作，通过扩展 Kubernetes 来实现这点。

期望状态与当前状态
---------

Kubernetes 采用了系统的云原生视图，并且可以处理持续的变化。

在任务执行时，集群随时都可能被修改，并且控制回路会自动修复故障。 这意味着很可能集群永远不会达到稳定状态。

只要集群中的控制器在运行并且进行有效的修改，整体状态的稳定与否是无关紧要的。

设计 
---

作为设计原则之一，Kubernetes 使用了很多控制器，每个控制器管理集群状态的一个特定方面。 最常见的一个特定的控制器使用一种类型的资源作为它的期望状态， 控制器管理控制另外一种类型的资源向它的期望状态演化。

使用简单的控制器而不是一组相互连接的单体控制回路是很有用的。 控制器会失败，所以 Kubernetes 的设计正是考虑到了这一点。

> 可以有多个控制器来创建或者更新相同类型的对象。 在后台，Kubernetes 控制器确保它们只关心与其控制资源相关联的资源。  
> 例如，你可以创建 Deployment 和 Job；它们都可以创建 Pod。 Job 控制器不会删除 Deployment 所创建的 Pod，因为有信息 （标签）让控制器可以区分这些 Pod。

运行控制器的方式
--------

Kubernetes 内置一组控制器，运行在 kube-controller-manager 内。 这些内置的控制器提供了重要的核心功能。

Deployment 控制器和 Job 控制器是 Kubernetes 内置控制器的典型例子。 Kubernetes 允许你运行一个稳定的控制平面，这样即使某些内置控制器失败了， 控制平面的其他部分会接替它们的工作。

你会遇到某些控制器运行在控制面之外，用以扩展 Kubernetes。 或者，如果你愿意，你也可以自己编写新控制器。 你可以以一组 Pod 来运行你的控制器，或者运行在 Kubernetes 之外。 最合适的方案取决于控制器所要执行的功能是什么。

##  4.  Kubernetes 云控制器管理器
云控制器管理器
-------

FEATURE STATE: Kubernetes v1.11 \[beta\]

使用云基础设施技术，你可以在公有云、私有云或者混合云环境中运行 Kubernetes。 Kubernetes 的信条是基于自动化的、API 驱动的基础设施，同时避免组件间紧密耦合。

组件 cloud-controller-manager 是指云控制器管理器， 云控制器管理器是指嵌入特定云的控制逻辑的 控制平面组件。 云控制器管理器使得你可以将你的集群连接到云提供商的 API 之上， 并将与该云平台交互的组件同与你的集群交互的组件分离开来。

通过分离 Kubernetes 和底层云基础设置之间的互操作性逻辑， 云控制器管理器组件使云提供商能够以不同于 Kubernetes 主项目的 步调发布新特征。

​`cloud-controller-manager`​ 组件是基于一种插件机制来构造的， 这种机制使得不同的云厂商都能将其平台与 Kubernetes 集成。

设计
--

![](https://atts.w3cschool.cn/attachments/image/20220429/1651198280484132.svg)  

云控制器管理器以一组多副本的进程集合的形式运行在控制面中，通常表现为 Pod 中的容器。每个 cloud-controller-manager 在同一进程中实现多个 控制器。

> 你也可以用 Kubernetes 插件 的形式而不是控制面中的一部分来运行云控制器管理器。

云控制器管理器的功能
----------

云控制器管理器中的控制器包括：

### 节点控制器 

节点控制器负责在云基础设施中创建了新服务器时为之 更新 节点（Node）对象。 节点控制器从云提供商获取当前租户中主机的信息。节点控制器执行以下功能：

1.  使用从云平台 API 获取的对应服务器的唯一标识符更新 Node 对象；
2.  利用特定云平台的信息为 Node 对象添加注解和标签，例如节点所在的 区域（Region）和所具有的资源（CPU、内存等等）；
3.  获取节点的网络地址和主机名；
4.  检查节点的健康状况。如果节点无响应，控制器通过云平台 API 查看该节点是否 已从云中禁用、删除或终止。如果节点已从云中删除，则控制器从 Kubernetes 集群 中删除 Node 对象。

某些云驱动实现中，这些任务被划分到一个节点控制器和一个节点生命周期控制器中。

### 路由控制器 

Route 控制器负责适当地配置云平台中的路由，以便 Kubernetes 集群中不同节点上的 容器之间可以相互通信。

取决于云驱动本身，路由控制器可能也会为 Pod 网络分配 IP 地址块。

### 服务控制器 

服务（Service）与受控的负载均衡器、 IP 地址、网络包过滤、目标健康检查等云基础设施组件集成。 服务控制器与云驱动的 API 交互，以配置负载均衡器和其他基础设施组件。 你所创建的 Service 资源会需要这些组件服务。

鉴权 
---

本节分别讲述云控制器管理器为了完成自身工作而产生的对各类 API 对象的访问需求。

### 节点控制器 

节点控制器只操作 Node 对象。它需要读取和修改 Node 对象的完全访问权限。

​`v1/Node`​:

*   Get
*   List
*   Create
*   Update
*   Patch
*   Watch
*   Delete

### 路由控制器

路由控制器会监听 Node 对象的创建事件，并据此配置路由设施。 它需要读取 Node 对象的 Get 权限。

​`v1/Node`​:

*   Get

### 服务控制器 

服务控制器监测 Service 对象的 Create、Update 和 Delete 事件，并配置 对应服务的 Endpoints 对象。 为了访问 Service 对象，它需要 List、Watch 访问权限；为了更新 Service 对象 它需要 Patch 和 Update 访问权限。 为了能够配置 Service 对应的 Endpoints 资源，它需要 Create、List、Get、Watch 和 Update 等访问权限。

​`v1/Service`​:

*   List
*   Get
*   Watch
*   Patch
*   Update

### 其他 

云控制器管理器的实现中，其核心部分需要创建 Event 对象的访问权限以及 创建 ServiceAccount 资源以保证操作安全性的权限。

​`v1/Event`​:

*   Create
*   Patch
*   Update

​`v1/ServiceAccount`​:

*   Create

用于云控制器管理器 RBAC 的 ClusterRole 如下例所示：

`apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata:   name: cloud-controller-manager rules: - apiGroups:   - ""   resources:   - events   verbs:   - create   - patch   - update - apiGroups:   - ""   resources:   - nodes   verbs:   - '*' - apiGroups:   - ""   resources:   - nodes/status   verbs:   - patch - apiGroups:   - ""   resources:   - services   verbs:   - list   - patch   - update   - watch - apiGroups:   - ""   resources:   - serviceaccounts   verbs:   - create - apiGroups:   - ""   resources:   - persistentvolumes   verbs:   - get   - list   - update   - watch - apiGroups:   - ""   resources:   - endpoints   verbs:   - create   - get   - list   - watch   - update`

##  5.  Kubernetes 垃圾收集
垃圾收集
----

垃圾收集是 Kubernetes 用于清理集群资源的各种机制的统称。 垃圾收集允许系统清理如下资源：

*   失败的 Pod
*   已完成的 Job
*   不再存在属主引用的对象
*   未使用的容器和容器镜像
*   动态制备的、StorageClass 回收策略为 Delete 的 PV 卷
*   阻滞或者过期的 CertificateSigningRequest (CSRs)
*   在以下情形中删除了的节点对象：

*   当集群使用云控制器管理器运行于云端时；
*   当集群使用类似于云控制器管理器的插件运行在本地环境中时。

*   节点租约对象

属主与依赖 
------

Kubernetes 中很多对象通过属主引用 链接到彼此。属主引用（Owner Reference）可以告诉控制面哪些对象依赖于其他对象。 Kubernetes 使用属主引用来为控制面以及其他 API 客户端在删除某对象时提供一个 清理关联资源的机会。在大多数场合，Kubernetes 都是自动管理属主引用的。

属主关系与某些资源所使用的的标签和选择算符 不同。例如，考虑一个创建 ​`EndpointSlice` ​对象的 Service 对象。Service 对象使用标签来允许控制面确定哪些 ​`EndpointSlice` ​对象被该 Service 使用。除了标签，每个被 Service 托管的 ​`EndpointSlice` ​对象还有一个属主引用属性。 属主引用可以帮助 Kubernetes 中的不同组件避免干预并非由它们控制的对象。

> 根据设计，系统不允许出现跨名字空间的属主引用。名字空间作用域的依赖对象可以指定集群作用域或者名字空间作用域的属主。 名字空间作用域的属主必须存在于依赖对象所在的同一名字空间。 如果属主位于不同名字空间，则属主引用被视为不存在，而当检查发现所有属主都已不存在时， 依赖对象会被删除。  
>   
> 集群作用域的依赖对象只能指定集群作用域的属主。 在 1.20 及更高版本中，如果一个集群作用域的依赖对象指定了某个名字空间作用域的类别作为其属主， 则该对象被视为拥有一个无法解析的属主引用，因而无法被垃圾收集处理。  
>   
> 在 1.20 及更高版本中，如果垃圾收集器检测到非法的跨名字空间 ​`ownerReference`​， 或者某集群作用域的依赖对象的 ​`ownerReference` ​引用某名字空间作用域的类别， 系统会生成一个警告事件，其原因为 ​`OwnerRefInvalidNamespace`​，​`involvedObject` ​设置为非法的依赖对象。你可以通过运行 ​`kubectl get events -A --field-selector=reason=OwnerRefInvalidNamespace`​ 来检查是否存在这类事件。

级联删除 
-----

Kubernetes 会检查并删除那些不再拥有属主引用的对象，例如在你删除了 ReplicaSet 之后留下来的 Pod。当你删除某个对象时，你可以控制 Kubernetes 是否要通过一个称作 级联删除（Cascading Deletion）的过程自动删除该对象的依赖对象。 级联删除有两种类型，分别如下：

*   前台级联删除
*   后台级联删除

你也可以使用 Kubernetes Finalizers 来控制垃圾收集机制如何以及何时删除包含属主引用的资源。

### 前台级联删除

在前台级联删除中，正在被你删除的对象首先进入 deletion in progress 状态。 在这种状态下，针对属主对象会发生以下事情：

*   Kubernetes API 服务器将对象的 ​`metadata.deletionTimestamp`​ 字段设置为对象被标记为要删除的时间点。
*   Kubernetes API 服务器也会将 ​`metadata.finalizers`​ 字段设置为 ​`foregroundDeletion`​。
*   在删除过程完成之前，通过 Kubernetes API 仍然可以看到该对象。

当属主对象进入删除过程中状态后，控制器删除其依赖对象。控制器在删除完所有依赖对象之后， 删除属主对象。这时，通过 Kubernetes API 就无法再看到该对象。

在前台级联删除过程中，唯一的可能阻止属主对象被删除的依赖对象是那些带有 ​`ownerReference.blockOwnerDeletion=true`​ 字段的对象。

### 后台级联删除

在后台级联删除过程中，Kubernetes 服务器立即删除属主对象，控制器在后台清理所有依赖对象。 默认情况下，Kubernetes 使用后台级联删除方案，除非你手动设置了要使用前台删除， 或者选择遗弃依赖对象。

### 被遗弃的依赖对象 

当 Kubernetes 删除某个属主对象时，被留下来的依赖对象被称作被遗弃的（Orphaned）对象。 默认情况下，Kubernetes 会删除依赖对象。

未使用容器和镜像的垃圾收集 
--------------

kubelet 会每五分钟对未使用的镜像执行一次垃圾收集， 每分钟对未使用的容器执行一次垃圾收集。 你应该避免使用外部的垃圾收集工具，因为外部工具可能会破坏 kubelet 的行为，移除应该保留的容器。

要配置对未使用容器和镜像的垃圾收集选项，可以使用一个 配置文件，基于 KubeletConfiguration 资源类型来调整与垃圾搜集相关的 kubelet 行为。

### 容器镜像生命期 

Kubernetes 通过其镜像管理器（Image Manager）来管理所有镜像的生命周期， 该管理器是 kubelet 的一部分，工作时与 cadvisor 协同。 kubelet 在作出垃圾收集决定时会考虑如下磁盘用量约束：

*   ​`HighThresholdPercent` ​
*   ​`LowThresholdPercent` ​

磁盘用量超出所配置的 ​`HighThresholdPercent` ​值时会触发垃圾收集， 垃圾收集器会基于镜像上次被使用的时间来按顺序删除它们，首先删除的是最老的镜像。 kubelet 会持续删除镜像，直到磁盘用量到达 ​`LowThresholdPercent` ​值为止。

### 容器垃圾收集 

kubelet 会基于如下变量对所有未使用的容器执行垃圾收集操作，这些变量都是你可以定义的：

*   ​`MinAge`​：kubelet 可以垃圾回收某个容器时该容器的最小年龄。设置为 ​`0`​ 表示禁止使用此规则。
*   ​`MaxPerPodContainer`​：每个 Pod 可以包含的已死亡的容器个数上限。设置为小于 ​`0`​ 的值表示禁止使用此规则。
*   ​`MaxContainers`​：集群中可以存在的已死亡的容器个数上限。设置为小于 ​`0`​ 的值意味着禁止应用此规则。

除以上变量之外，kubelet 还会垃圾收集除无标识的以及已删除的容器，通常从最老的容器开始。

当保持每个 Pod 的最大数量的容器（​`MaxPerPodContainer`​）会使得全局的已死亡容器个数超出上限 （​`MaxContainers`​）时，​`MaxPerPodContainer` ​和 ​`MaxContainers` ​之间可能会出现冲突。 在这种情况下，kubelet 会调整 ​`MaxPerPodContainer` ​来解决这一冲突。 最坏的情形是将 ​`MaxPerPodContainer` ​降格为 ​`1`​，并驱逐最老的容器。 此外，当隶属于某已被删除的 Pod 的容器的年龄超过 ​`MinAge` ​时，它们也会被删除。

> kubelet 仅会回收由它所管理的容器。

配置垃圾收集 
-------

你可以通过配置特定于管理资源的控制器来调整资源的垃圾收集行为。

##  6.  Kubernetes 容器运行时接口（CRI）
容器运行时接口（CRI）
------------

CRI 是一个插件接口，它使 kubelet 能够使用各种容器运行时，无需重新编译集群组件。

你需要在集群中的每个节点上都有一个可以正常工作的 容器运行时， 这样 kubelet 能启动 Pod 及其容器。

容器运行时接口（CRI）是 kubelet 和容器运行时之间通信的主要协议。

Kubernetes 容器运行时接口（CRI）定义了主要 gRPC 协议， 用于集群组件 kubelet 和 容器运行时。

API
---

FEATURE STATE: Kubernetes v1.23 \[stable\]

当通过 gRPC 连接到容器运行时时，kubelet 充当客户端。 运行时和镜像服务端点必须在容器运行时中可用，可以使用 命令行标志的 ​`--image-service-endpoint`​ 和 ​`--container-runtime-endpoint`​ 在 kubelet 中单独配置。

对 Kubernetes v1.23，kubelet 偏向于使用 CRI ​`v1`​ 版本。 如果容器运行时不支持 CRI 的 ​`v1`​ 版本，那么 kubelet 会尝试协商任何旧的其他支持版本。 如果 kubelet 无法协商支持的 CRI 版本，则 kubelet 放弃并且不会注册为节点。

升级 
---

升级 Kubernetes 时，kubelet 会尝试在组件重启时自动选择最新的 CRI 版本。 如果失败，则将如上所述进行回退。如果由于容器运行时已升级而需要 gRPC 重拨， 则容器运行时还必须支持最初选择的版本，否则重拨预计会失败。 这需要重新启动 kubelet。

#  6.  Kubernetes 容器

##  1.  Kubernetes 镜像
镜像
--

容器镜像（Image）所承载的是封装了应用程序及其所有软件依赖的二进制数据。 容器镜像是可执行的软件包，可以单独运行；该软件包对所处的运行时环境具有 良定（Well Defined）的假定。

你通常会创建应用的容器镜像并将其推送到某仓库（Registry），然后在 Pod 中引用它。

本页概要介绍容器镜像的概念。

镜像名称 
-----

容器镜像通常会被赋予 ​`pause`​、​`example/mycontainer`​ 或者 ​`kube-apiserver`​ 这类的名称。 镜像名称也可以包含所在仓库的主机名。例如：​`fictional.registry.example/imagename`​。 还可以包含仓库的端口号，例如：​`fictional.registry.example:10443/imagename`​。

如果你不指定仓库的主机名，Kubernetes 认为你在使用 Docker 公共仓库。

在镜像名称之后，你可以添加一个标签（Tag）（与使用 ​`docker` ​或 ​`podman` ​等命令时的方式相同）。 使用标签能让你辨识同一镜像序列中的不同版本。

镜像标签可以包含小写字母、大写字母、数字、下划线（​`_`​）、句点（​`.`​）和连字符（​`-`​）。 关于在镜像标签中何处可以使用分隔字符（​`_`​、​`-`​ 和 ​`.`​）还有一些额外的规则。 如果你不指定标签，Kubernetes 认为你想使用标签 ​`latest`​。

更新镜像 
-----

当你最初创建一个 Deployment、 StatefulSet、Pod 或者其他包含 Pod 模板的对象时，如果没有显式设定的话，Pod 中所有容器的默认镜像 拉取策略是 ​`IfNotPresent`​。这一策略会使得 kubelet 在镜像已经存在的情况下直接略过拉取镜像的操作。

### 镜像拉取策略 

容器的 ​`imagePullPolicy` ​和镜像的标签会影响 kubelet 尝试拉取（下载）指定的镜像。

以下列表包含了 ​`imagePullPolicy` ​可以设置的值，以及这些值的效果：

*   IfNotPresent
只有当镜像在本地不存在时才会拉取。*   Always
每当 kubelet 启动一个容器时，kubelet 会查询容器的镜像仓库， 将名称解析为一个镜像摘要。 如果 kubelet 有一个容器镜像，并且对应的摘要已在本地缓存，kubelet 就会使用其缓存的镜像； 否则，kubelet 就会使用解析后的摘要拉取镜像，并使用该镜像来启动容器。*   Never
Kubelet 不会尝试获取镜像。如果镜像已经以某种方式存在本地， kubelet 会尝试启动容器；否则，会启动失败。 

只要能够可靠地访问镜像仓库，底层镜像提供者的缓存语义甚至可以使 imagePullPolicy: Always 高效。 你的容器运行时可以注意到节点上已经存在的镜像层，这样就不需要再次下载。

> 在生产环境中部署容器时，你应该避免使用 ​`:latest`​ 标签，因为这使得正在运行的镜像的版本难以追踪，并且难以正确地回滚。  
>   
> 相反，应指定一个有意义的标签，如 ​`v1.42.0`​。

为了确保 Pod 总是使用相同版本的容器镜像，你可以指定镜像的摘要； 将 ​`<image-name>:<tag>`​ 替换为 ​`<image-name>@<digest>`​，例如 ​`image@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2`​。

当使用镜像标签时，如果镜像仓库修改了代码所对应的镜像标签，可能会出现新旧代码混杂在 Pod 中运行的情况。 镜像摘要唯一标识了镜像的特定版本，因此 Kubernetes 每次启动具有指定镜像名称和摘要的容器时，都会运行相同的代码。 通过摘要指定镜像可固定你运行的代码，这样镜像仓库的变化就不会导致版本的混杂。

有一些第三方的准入控制器 在创建 Pod（和 Pod 模板）时产生变更，这样运行的工作负载就是根据镜像摘要，而不是标签来定义的。 无论镜像仓库上的标签发生什么变化，你都想确保你所有的工作负载都运行相同的代码，那么指定镜像摘要会很有用。

### 默认镜像拉取策略 

当你（或控制器）向 API 服务器提交一个新的 Pod 时，你的集群会在满足特定条件时设置 ​`imagePullPolicy` ​字段：

*   如果你省略了 ​`imagePullPolicy` ​字段，并且容器镜像的标签是 ​`:latest`​， ​`imagePullPolicy` ​会自动设置为 ​`Always`​。
*   如果你省略了 ​`imagePullPolicy` ​字段，并且没有指定容器镜像的标签， ​`imagePullPolicy` ​会自动设置为 ​`Always`​。
*   如果你省略了 ​`imagePullPolicy` ​字段，并且为容器镜像指定了非 ​`:latest`​ 的标签， ​`imagePullPolicy` ​就会自动设置为 ​`IfNotPresent`​。

> 容器的 ​`imagePullPolicy` ​的值总是在对象初次 创建 时设置的，如果后来镜像的标签发生变化，则不会更新。  
>   
> 例如，如果你用一个 非 ​`:latest`​ 的镜像标签创建一个 Deployment， 并在随后更新该 Deployment 的镜像标签为 ​`:latest`​，则 ​`imagePullPolicy` ​字段 不会 变成 ​`Always`​。 你必须手动更改已经创建的资源的拉取策略。

### 必要的镜像拉取 

如果你想总是强制执行拉取，你可以使用下述的一中方式：

*   设置容器的 ​`imagePullPolicy` ​为 ​`Always`​。
*   省略 ​`imagePullPolicy`​，并使用 ​`:latest`​ 作为镜像标签； 当你提交 Pod 时，Kubernetes 会将策略设置为 ​`Always`​。
*   省略 ​`imagePullPolicy` ​和镜像的标签； 当你提交 Pod 时，Kubernetes 会将策略设置为 ​`Always`​。
*   启用准入控制器 AlwaysPullImages。

### ImagePullBackOff

当 kubelet 使用容器运行时创建 Pod 时，容器可能因为 ​`ImagePullBackOff` ​导致状态为 Waiting。

​`ImagePullBackOff` ​状态意味着容器无法启动， 因为 Kubernetes 无法拉取容器镜像（原因包括无效的镜像名称，或从私有仓库拉取而没有 ​`imagePullSecret`​）。 ​`BackOff` ​部分表示 Kubernetes 将继续尝试拉取镜像，并增加回退延迟。

Kubernetes 会增加每次尝试之间的延迟，直到达到编译限制，即 300 秒（5 分钟）。

带镜像索引的多架构镜像 
------------

除了提供二进制的镜像之外，容器仓库也可以提供 容器镜像索引。 镜像索引可以根据特定于体系结构版本的容器指向镜像的多个 镜像清单。 这背后的理念是让你可以为镜像命名（例如：​`pause`​、​`example/mycontainer`​、​`kube-apiserver`​） 的同时，允许不同的系统基于它们所使用的机器体系结构取回正确的二进制镜像。

Kubernetes 自身通常在命名容器镜像时添加后缀 ​`-$(ARCH)`​。 为了向前兼容，请在生成较老的镜像时也提供后缀。 这里的理念是为某镜像（如 ​`pause`​）生成针对所有平台都适用的清单时， 生成 ​`pause-amd64`​ 这类镜像，以便较老的配置文件或者将镜像后缀影编码到其中的 YAML 文件也能兼容。

使用私有仓库 
-------

从私有仓库读取镜像时可能需要密钥。 凭证可以用以下方式提供:

*   配置节点向私有仓库进行身份验证

*   所有 Pod 均可读取任何已配置的私有仓库
*   需要集群管理员配置节点

*   预拉镜像

*   所有 Pod 都可以使用节点上缓存的所有镜像
*   需要所有节点的 root 访问权限才能进行设置

*   在 Pod 中设置 ImagePullSecrets

*   只有提供自己密钥的 Pod 才能访问私有仓库

*   特定于厂商的扩展或者本地扩展

*   如果你在使用定制的节点配置，你（或者云平台提供商）可以实现让节点 向容器仓库认证的机制

### 配置 Node 对私有仓库认证

设置凭据的具体说明取决于你选择使用的容器运行时和仓库。

> Kubernetes 默认仅支持 Docker 配置中的 ​`auths` ​和 ​`HttpHeaders` ​部分， 不支持 Docker 凭据辅助程序（​`credHelpers` ​或 ​`credsStore`​）。

### config.json 说明

对于 ​`config.json`​ 的解释在原始 Docker 实现和 Kubernetes 的解释之间有所不同。 在 Docker 中，​`auths` ​键只能指定根 URL ，而 Kubernetes 允许 glob URLs 以及 前缀匹配的路径。这意味着，像这样的 ​`config.json`​ 是有效的：

`{     "auths": {         "*my-registry.io/images": {             "auth": "…"         }     } }`

    

使用以下语法匹配根 URL （​`*my-registry.io`​）：

`pattern:     { term }  term:     '*'         匹配任何无分隔符字符序列     '?'         匹配任意单个非分隔符     '[' [ '^' ] 字符范围                   字符集（必须非空）     c           匹配字符 c （c 不为 '*','?','\\','['）     '\\' c      匹配字符 c  字符范围:      c           匹配字符 c （c 不为 '\\','?','-',']'）     '\\' c      匹配字符 c     lo '-' hi   匹配字符范围在 lo 到 hi 之间字符`

现在镜像拉取操作会将每种有效模式的凭据都传递给 CRI 容器运行时。例如下面的容器镜像名称会匹配成功：

*   ​`my-registry.io/images` ​
*   ​`my-registry.io/images/my-image` ​
*   ​`my-registry.io/images/another-image` ​
*   ​`sub.my-registry.io/images/my-image` ​
*   ​`a.sub.my-registry.io/images/my-image`​

kubelet 为每个找到的凭证的镜像按顺序拉取。 这意味着在 ​`config.json`​ 中可能有多项：

`{     "auths": {         "my-registry.io/images": {             "auth": "…"         },         "my-registry.io/images/subpath": {             "auth": "…"         }     } }`

如果一个容器指定了要拉取的镜像 ​`my-registry.io/images/subpath/my-image`​， 并且其中一个失败，kubelet 将尝试从另一个身份验证源下载镜像。

### 提前拉取镜像 

> 该方法适用于你能够控制节点配置的场合。 如果你的云供应商负责管理节点并自动置换节点，这一方案无法可靠地工作。

默认情况下，​`kubelet` ​会尝试从指定的仓库拉取每个镜像。 但是，如果容器属性 ​`imagePullPolicy` ​设置为 ​`IfNotPresent` ​或者 ​`Never`​， 则会优先使用（对应 ​`IfNotPresent`​）或者一定使用（对应 ​`Never`​）本地镜像。

如果你希望使用提前拉取镜像的方法代替仓库认证，就必须保证集群中所有节点提前拉取的镜像是相同的。

这一方案可以用来提前载入指定的镜像以提高速度，或者作为向私有仓库执行身份认证的一种替代方案。

所有的 Pod 都可以使用节点上提前拉取的镜像。

### 在 Pod 上指定 ImagePullSecrets 

> 运行使用私有仓库中镜像的容器时，建议使用这种方法。

Kubernetes 支持在 Pod 中设置容器镜像仓库的密钥。

### 使用 Docker Config 创建 Secret 

你需要知道用于向仓库进行身份验证的用户名、密码和客户端电子邮件地址，以及它的主机名。 运行以下命令，注意替换适当的大写值：

`kubectl create secret docker-registry <name> --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL`

如果你已经有 Docker 凭据文件，则可以将凭据文件导入为 Kubernetes Secret， 而不是执行上面的命令。

如果你在使用多个私有容器仓库，这种技术将特别有用。 原因是 ​`kubectl create secret docker-registry`​ 创建的是仅适用于某个私有仓库的 Secret。

> Pod 只能引用位于自身所在名字空间中的 Secret，因此需要针对每个名字空间 重复执行上述过程。

### 在 Pod 中引用 ImagePullSecrets

现在，在创建 Pod 时，可以在 Pod 定义中增加 ​`imagePullSecrets` ​部分来引用该 Secret。

例如：

`cat <<EOF > pod.yaml apiVersion: v1 kind: Pod metadata:   name: foo   namespace: awesomeapps spec:   containers:     - name: foo       image: janedoe/awesomeapp:v1   imagePullSecrets:     - name: myregistrykey EOF  cat <<EOF >> ./kustomization.yaml resources: - pod.yaml EOF`

你需要对使用私有仓库的每个 Pod 执行以上操作。 不过，设置该字段的过程也可以通过为 服务账号 资源设置 ​`imagePullSecrets` ​来自动完成。

你也可以将此方法与节点级别的 ​`.docker/config.json`​ 配置结合使用。 来自不同来源的凭据会被合并。

使用案例 
-----

配置私有仓库有多种方案，以下是一些常用场景和建议的解决方案。

1.  集群运行非专有镜像（例如，开源镜像）。镜像不需要隐藏。

*   使用 Docker hub 上的公开镜像

*   无需配置
*   某些云厂商会自动为公开镜像提供高速缓存，以便提升可用性并缩短拉取镜像所需时间

7.  集群运行一些专有镜像，这些镜像需要对公司外部隐藏，对所有集群用户可见

*   使用托管的私有 Docker 仓库。

*   可以托管在 Docker Hub 或者其他地方
*   按照上面的描述，在每个节点上手动配置 .docker/config.json 文件

*   或者，在防火墙内运行一个组织内部的私有仓库，并开放读取权限

*   不需要配置 Kubenretes

*   使用控制镜像访问的托管容器镜像仓库服务

*   与手动配置节点相比，这种方案能更好地处理集群自动扩缩容

*   或者，在不方便更改节点配置的集群中，使用 imagePullSecrets

13.  集群使用专有镜像，且有些镜像需要更严格的访问控制

*   确保 AlwaysPullImages 准入控制器被启用。否则，所有 Pod 都可以使用所有镜像。
*   确保将敏感数据存储在 Secret 资源中，而不是将其打包在镜像里

19.  集群是多租户的并且每个租户需要自己的私有仓库

*   确保 AlwaysPullImages 准入控制器。否则，所有租户的所有的 Pod 都可以使用所有镜像。
*   为私有仓库启用鉴权
*   为每个租户生成访问仓库的凭据，放置在 Secret 中，并将 Secrert 发布到各租户的命名空间下。
*   租户将 Secret 添加到每个名字空间中的 imagePullSecrets

如果你需要访问多个仓库，可以为每个仓库创建一个 Secret。 ​`kubelet` ​将所有 ​`imagePullSecrets` ​合并为一个虚拟的 ​`.docker/config.json`​ 文件。

##  2.  Kubernetes 容器环境
容器环境 
-----

Kubernetes 的容器环境给容器提供了几个重要的资源：

*   文件系统，其中包含一个镜像 和一个或多个的卷
*   容器自身的信息
*   集群中其他对象的信息

### 容器信息

容器的 hostname 是它所运行在的 pod 的名称。它可以通过 ​`hostname` ​命令或者调用 libc 中的 gethostname 函数来获取。

Pod 名称和命名空间可以通过 下行 API 转换为环境变量。

Pod 定义中的用户所定义的环境变量也可在容器中使用，就像在 container 镜像中静态指定的任何环境变量一样。

### 集群信息

创建容器时正在运行的所有服务都可用作该容器的环境变量。 这里的服务仅限于新容器的 Pod 所在的名字空间中的服务，以及 Kubernetes 控制面的服务。

对于名为 foo 的服务，当映射到名为 bar 的容器时，以下变量是被定义了的：

`FOO_SERVICE_HOST=<the host the service is running on> FOO_SERVICE_PORT=<the port the service is running on>`

服务具有专用的 IP 地址。如果启用了 DNS 插件， 可以在容器中通过 DNS 来访问服务。

##  3.  Kubernetes 容器运行时类（Runtime Class）
容器运行时类（Runtime Class）
---------------------

FEATURE STATE: Kubernetes v1.20 \[stable\]

本页面描述了 RuntimeClass 资源和运行时的选择机制。

RuntimeClass 是一个用于选择容器运行时配置的特性，容器运行时配置用于运行 Pod 中的容器。

动机 
---

你可以在不同的 Pod 设置不同的 RuntimeClass，以提供性能与安全性之间的平衡。 例如，如果你的部分工作负载需要高级别的信息安全保证，你可以决定在调度这些 Pod 时尽量使它们在使用硬件虚拟化的容器运行时中运行。 这样，你将从这些不同运行时所提供的额外隔离中获益，代价是一些额外的开销。

你还可以使用 RuntimeClass 运行具有相同容器运行时但具有不同设置的 Pod。

设置 
---

1.  在节点上配置 CRI 的实现（取决于所选用的运行时）
2.  创建相应的 RuntimeClass 资源

### 1. 在节点上配置 CRI 实现

RuntimeClass 的配置依赖于 运行时接口（CRI）的实现。 根据你使用的 CRI 实现，查阅相关的文档（下方）来了解如何配置。

> RuntimeClass 假设集群中的节点配置是同构的（换言之，所有的节点在容器运行时方面的配置是相同的）。 如果需要支持异构节点，配置方法请参阅下面的 调度。

所有这些配置都具有相应的 ​`handler` ​名，并被 RuntimeClass 引用。 handler 必须是有效的 DNS 标签名。

### 2. 创建相应的 RuntimeClass 资源

在上面步骤 1 中，每个配置都需要有一个用于标识配置的 ​`handler`​。 针对每个 handler 需要创建一个 RuntimeClass 对象。

RuntimeClass 资源当前只有两个重要的字段：RuntimeClass 名 (​`metadata.name`​) 和 handler (​`handler`​)。 对象定义如下所示：

`apiVersion: node.k8s.io/v1  # RuntimeClass 定义于 node.k8s.io API 组 kind: RuntimeClass metadata:   name: myclass  # 用来引用 RuntimeClass 的名字   # RuntimeClass 是一个集群层面的资源 handler: myconfiguration  # 对应的 CRI 配置的名称`

> 建议将 RuntimeClass 写操作（create、update、patch 和 delete）限定于集群管理员使用。 通常这是默认配置。

使用说明 
-----

一旦完成集群中 RuntimeClasses 的配置，使用起来非常方便。 在 Pod spec 中指定 ​`runtimeClassName` ​即可。例如:

`apiVersion: v1 kind: Pod metadata:   name: mypod spec:   runtimeClassName: myclass   # ...`

这一设置会告诉 kubelet 使用所指的 RuntimeClass 来运行该 pod。 如果所指的 RuntimeClass 不存在或者 CRI 无法运行相应的 handler， 那么 pod 将会进入 ​`Failed` ​终止阶段。 你可以查看相应的事件， 获取执行过程中的错误信息。

如果未指定 ​`runtimeClassName` ​，则将使用默认的 RuntimeHandler，相当于禁用 RuntimeClass 功能特性。

### CRI 配置

#### dockershim

Dockershim 自 Kubernetes v1.20 起已弃用，并将在 v1.24 中删除。

为 dockershim 设置 RuntimeClass 时，必须将运行时处理程序设置为 ​`docker`​。 Dockershim 不支持自定义的可配置的运行时处理程序。

#### containerd

通过 containerd 的 ​`/etc/containerd/config.toml`​ 配置文件来配置运行时 handler。 handler 需要配置在 runtimes 块中：

`[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.${HANDLER_NAME}]`

更详细信息，请查阅 containerd [CRI 插件配置指南](https://github.com/containerd/cri/blob/master/docs/config.md)

#### cri-o

通过 cri-o 的 ​`/etc/crio/crio.conf`​ 配置文件来配置运行时 handler。 handler 需要配置在 [crio.runtime](https://github.com/cri-o/cri-o/blob/main/docs/crio.conf.5.md target=) 表 下面：

`[crio.runtime.runtimes.${HANDLER_NAME}]   runtime_path = "${PATH_TO_BINARY}"`

更详细信息，请查阅 CRI-O [配置文档](https://github.com/cri-o/cri-o/blob/main/docs/crio.conf.5.md)。

调度 
---

FEATURE STATE: Kubernetes v1.16 \[beta\]

通过为 RuntimeClass 指定 ​`scheduling` ​字段， 你可以通过设置约束，确保运行该 RuntimeClass 的 Pod 被调度到支持该 RuntimeClass 的节点上。 如果未设置 ​`scheduling`​，则假定所有节点均支持此 RuntimeClass 。

为了确保 pod 会被调度到支持指定运行时的 node 上，每个 node 需要设置一个通用的 label 用于被 ​`runtimeclass.scheduling.nodeSelector`​ 挑选。在 admission 阶段，RuntimeClass 的 nodeSelector 将会与 pod 的 nodeSelector 合并，取二者的交集。如果有冲突，pod 将会被拒绝。

如果 node 需要阻止某些需要特定 RuntimeClass 的 pod，可以在 ​`tolerations` ​中指定。 与 ​`nodeSelector` ​一样，tolerations 也在 admission 阶段与 pod 的 tolerations 合并，取二者的并集。

### Pod 开销 

FEATURE STATE: Kubernetes v1.18 \[beta\]

你可以指定与运行 Pod 相关的 开销 资源。声明开销即允许集群（包括调度器）在决策 Pod 和资源时将其考虑在内。 若要使用 Pod 开销特性，你必须确保 PodOverhead 特性门控 处于启用状态（默认为启用状态）。

Pod 开销通过 RuntimeClass 的 ​`overhead` ​字段定义。 通过使用这些字段，你可以指定使用该 RuntimeClass 运行 Pod 时的开销并确保 Kubernetes 将这些开销计算在内。

##  4.  Kubernetes 容器生命周期回调
概述
--

类似于许多具有生命周期回调组件的编程语言框架，例如 Angular、Kubernetes 为容器提供了生命周期回调。 回调使容器能够了解其管理生命周期中的事件，并在执行相应的生命周期回调时运行在处理程序中实现的代码。

容器回调
----

有两个回调暴露给容器：

​`PostStart` ​

这个回调在容器被创建之后立即被执行。 但是，不能保证回调会在容器入口点（ENTRYPOINT）之前执行。 没有参数传递给处理程序。

​`PreStop` ​

在容器因 API 请求或者管理事件（诸如存活态探针、启动探针失败、资源抢占、资源竞争等） 而被终止之前，此回调会被调用。 如果容器已经处于已终止或者已完成状态，则对 preStop 回调的调用将失败。 在用来停止容器的 TERM 信号被发出之前，回调必须执行结束。 Pod 的终止宽限周期在 ​`PreStop` ​回调被执行之前即开始计数，所以无论 回调函数的执行结果如何，容器最终都会在 Pod 的终止宽限期内被终止。 没有参数会被传递给处理程序。

### 回调处理程序的实现 

容器可以通过实现和注册该回调的处理程序来访问该回调。 针对容器，有两种类型的回调处理程序可供实现：

*   Exec - 在容器的 cgroups 和名称空间中执行特定的命令（例如 ​`pre-stop.sh`​）。 命令所消耗的资源计入容器的资源消耗。
*   HTTP - 对容器上的特定端点执行 HTTP 请求。

### 回调处理程序执行

当调用容器生命周期管理回调时，Kubernetes 管理系统根据回调动作执行其处理程序， ​`httpGet` ​和 ​`tcpSocket` ​在kubelet 进程执行，而 ​`exec` ​则由容器内执行 。

回调处理程序调用在包含容器的 Pod 上下文中是同步的。 这意味着对于 ​`PostStart` ​回调，容器入口点和回调异步触发。 但是，如果回调运行或挂起的时间太长，则容器无法达到 ​`running` ​状态。

​`PreStop` ​回调并不会与停止容器的信号处理程序异步执行；回调必须在 可以发送信号之前完成执行。 如果 ​`PreStop` ​回调在执行期间停滞不前，Pod 的阶段会变成 ​`Terminating` ​并且一直处于该状态，直到其 ​`terminationGracePeriodSeconds` ​耗尽为止， 这时 Pod 会被杀死。 这一宽限期是针对 ​`PreStop` ​回调的执行时间及容器正常停止时间的总和而言的。 例如，如果 ​`terminationGracePeriodSeconds` ​是 60，回调函数花了 55 秒钟 完成执行，而容器在收到信号之后花了 10 秒钟来正常结束，那么容器会在其 能够正常结束之前即被杀死，因为 ​`terminationGracePeriodSeconds` ​的值 小于后面两件事情所花费的总时间（55+10）。

如果 ​`PostStart` ​或 ​`PreStop` ​回调失败，它会杀死容器。

用户应该使他们的回调处理程序尽可能的轻量级。 但也需要考虑长时间运行的命令也很有用的情况，比如在停止容器之前保存状态。

### 回调递送保证

回调的递送应该是 至少一次，这意味着对于任何给定的事件， 例如 ​`PostStart` ​或 ​`PreStop`​，回调可以被调用多次。 如何正确处理被多次调用的情况，是回调实现所要考虑的问题。

通常情况下，只会进行单次递送。 例如，如果 HTTP 回调接收器宕机，无法接收流量，则不会尝试重新发送。 然而，偶尔也会发生重复递送的可能。 例如，如果 kubelet 在发送回调的过程中重新启动，回调可能会在 kubelet 恢复后重新发送。

### 调试回调处理程序

回调处理程序的日志不会在 Pod 事件中公开。 如果处理程序由于某种原因失败，它将播放一个事件。 对于 ​`PostStart`​，这是 ​`FailedPostStartHook` ​事件，对于 ​`PreStop`​，这是 ​`FailedPreStopHook` ​事件。 要自己生成失败的 ​`FailedPreStopHook` ​事件，请修改 [lifecycle-events.yaml](https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/lifecycle-events.yaml) 文件将 postStart 命令更改为 ”badcommand“ 并应用它。 以下是通过运行 ​`kubectl describe pod lifecycle-demo`​ 后你看到的一些结果事件的示例输出：

`Events:   Type     Reason               Age              From               Message   ----     ------               ----             ----               -------   Normal   Scheduled            7s               default-scheduler  Successfully assigned default/lifecycle-demo to ip-XXX-XXX-XX-XX.us-east-2...   Normal   Pulled               6s               kubelet            Successfully pulled image "nginx" in 229.604315ms   Normal   Pulling              4s (x2 over 6s)  kubelet            Pulling image "nginx"   Normal   Created              4s (x2 over 5s)  kubelet            Created container lifecycle-demo-container   Normal   Started              4s (x2 over 5s)  kubelet            Started container lifecycle-demo-container   Warning  FailedPostStartHook  4s (x2 over 5s)  kubelet            Exec lifecycle hook ([badcommand]) for Container "lifecycle-demo-container" in Pod "lifecycle-demo_default(30229739-9651-4e5a-9a32-a8f1688862db)" failed - error: command 'badcommand' exited with 126: , message: "OCI runtime exec failed: exec failed: container_linux.go:380: starting container process caused: exec: \"badcommand\": executable file not found in $PATH: unknown\r\n"   Normal   Killing              4s (x2 over 5s)  kubelet            FailedPostStartHook   Normal   Pulled               4s               kubelet            Successfully pulled image "nginx" in 215.66395ms   Warning  BackOff              2s (x2 over 3s)  kubelet            Back-off restarting failed container`

#  7.  Kubernetes Pods

##  1.  Kubernetes Pod的生命周期
Pod 的生命周期
---------

本页面讲述 Pod 的生命周期。 Pod 遵循一个预定义的生命周期，起始于 ​`Pending` ​阶段，如果至少 其中有一个主要容器正常启动，则进入 ​`Running`​，之后取决于 Pod 中是否有容器以 失败状态结束而进入 Succeeded 或者 Failed 阶段。

在 Pod 运行期间，​`kubelet` ​能够重启容器以处理一些失效场景。 在 Pod 内部，Kubernetes 跟踪不同容器的状态 并确定使 Pod 重新变得健康所需要采取的动作。

在 Kubernetes API 中，Pod 包含规约部分和实际状态部分。 Pod 对象的状态包含了一组 Pod 状况（Conditions）。 如果应用需要的话，你也可以向其中注入自定义的就绪性信息。

Pod 在其生命周期中只会被调度一次。 一旦 Pod 被调度（分派）到某个节点，Pod 会一直在该节点运行，直到 Pod 停止或者 被终止。

Pod 生命期 
--------

和一个个独立的应用容器一样，Pod 也被认为是相对临时性（而不是长期存在）的实体。 Pod 会被创建、赋予一个唯一的 ID（UID）， 并被调度到节点，并在终止（根据重启策略）或删除之前一直运行在该节点。

如果一个节点死掉了，调度到该节点 的 Pod 也被计划在给定超时期限结束后删除。

Pod 自身不具有自愈能力。如果 Pod 被调度到某节点 而该节点之后失效，Pod 会被删除；类似地，Pod 无法在因节点资源 耗尽或者节点维护而被驱逐期间继续存活。Kubernetes 使用一种高级抽象 来管理这些相对而言可随时丢弃的 Pod 实例，称作 控制器。

任何给定的 Pod （由 UID 定义）从不会被“重新调度（rescheduled）”到不同的节点； 相反，这一 Pod 可以被一个新的、几乎完全相同的 Pod 替换掉。 如果需要，新 Pod 的名字可以不变，但是其 UID 会不同。

如果某物声称其生命期与某 Pod 相同，例如存储卷， 这就意味着该对象在此 Pod （UID 亦相同）存在期间也一直存在。 如果 Pod 因为任何原因被删除，甚至某完全相同的替代 Pod 被创建时， 这个相关的对象（例如这里的卷）也会被删除并重建。

![](https://atts.w3cschool.cn/attachments/image/20220429/1651202755538007.svg)  

*   Pod 结构图例
一个包含多个容器的 Pod 中包含一个用来拉取文件的程序和一个 Web 服务器， 均使用持久卷作为容器间共享的存储。

Pod 阶段 
-------

Pod 的 ​`status` ​字段是一个 [PodStatus](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/ target=) 对象，其中包含一个 ​`phase` ​字段。

Pod 的阶段（Phase）是 Pod 在其生命周期中所处位置的简单宏观概述。 该阶段并不是对容器或 Pod 状态的综合汇总，也不是为了成为完整的状态机。

Pod 阶段的数量和含义是严格定义的。 除了本文档中列举的内容外，不应该再假定 Pod 有其他的 ​`phase` ​值。

下面是 ​`phase` ​可能的值：

取值

描述

`Pending`（悬决）

Pod 已被 Kubernetes 系统接受，但有一个或者多个容器尚未创建亦未运行。此阶段包括等待 Pod 被调度的时间和通过网络下载镜像的时间。

`Running`（运行中）

Pod 已经绑定到了某个节点，Pod 中所有的容器都已被创建。至少有一个容器仍在运行，或者正处于启动或重启状态。

`Succeeded`（成功）

Pod 中的所有容器都已成功终止，并且不会再重启。

`Failed`（失败）

Pod 中的所有容器都已终止，并且至少有一个容器是因为失败终止。也就是说，容器以非 0 状态退出或者被系统终止。

`Unknown`（未知）

因为某些原因无法取得 Pod 的状态。这种情况通常是因为与 Pod 所在主机通信失败。

如果某节点死掉或者与集群中其他节点失联，Kubernetes 会实施一种策略，将失去的节点上运行的所有 Pod 的 ​`phase` ​设置为 ​`Failed`​。

容器状态 
-----

Kubernetes 会跟踪 Pod 中每个容器的状态，就像它跟踪 Pod 总体上的阶段一样。 你可以使用容器生命周期回调 来在容器生命周期中的特定时间点触发事件。

一旦调度器将 Pod 分派给某个节点，kubelet 就通过 容器运行时 开始为 Pod 创建容器。 容器的状态有三种：​`Waiting`​（等待）、​`Running`​（运行中）和 ​`Terminated`​（已终止）。

要检查 Pod 中容器的状态，你可以使用 ​`kubectl describe pod <pod 名称>`​。 其输出中包含 Pod 中每个容器的状态。

每种状态都有特定的含义：

### Waiting （等待） 

如果容器并不处在 ​`Running` ​或 ​`Terminated` ​状态之一，它就处在 ​`Waiting` ​状态。 处于 ​`Waiting` ​状态的容器仍在运行它完成启动所需要的操作：例如，从某个容器镜像 仓库拉取容器镜像，或者向容器应用 Secret 数据等等。 当你使用 ​`kubectl` ​来查询包含 ​`Waiting` ​状态的容器的 Pod 时，你也会看到一个 Reason 字段，其中给出了容器处于等待状态的原因。

### Running（运行中） 

​`Running` ​状态表明容器正在执行状态并且没有问题发生。 如果配置了 ​`postStart` ​回调，那么该回调已经执行且已完成。 如果你使用 ​`kubectl` ​来查询包含 ​`Running` ​状态的容器的 Pod 时，你也会看到 关于容器进入 ​`Running` ​状态的信息。

### Terminated（已终止） 

处于 ​`Terminated` ​状态的容器已经开始执行并且或者正常结束或者因为某些原因失败。 如果你使用 ​`kubectl` ​来查询包含 ​`Terminated` ​状态的容器的 Pod 时，你会看到 容器进入此状态的原因、退出代码以及容器执行期间的起止时间。

如果容器配置了 ​`preStop` ​回调，则该回调会在容器进入 ​`Terminated` ​状态之前执行。

容器重启策略
------

Pod 的 ​`spec` ​中包含一个 ​`restartPolicy` ​字段，其可能取值包括 Always、OnFailure 和 Never。默认值是 Always。

​`restartPolicy` ​适用于 Pod 中的所有容器。​`restartPolicy` ​仅针对同一节点上 ​`kubelet` ​的容器重启动作。当 Pod 中的容器退出时，​`kubelet` ​会按指数回退 方式计算重启的延迟（10s、20s、40s、...），其最长延迟为 5 分钟。 一旦某容器执行了 10 分钟并且没有出现问题，​`kubelet` ​对该容器的重启回退计时器执行 重置操作。

Pod 状况 
-------

Pod 有一个 PodStatus 对象，其中包含一个 [PodConditions](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/ target=) 数组。Pod 可能通过也可能未通过其中的一些状况测试。

*   ​`PodScheduled`​：Pod 已经被调度到某节点；
*   ​`ContainersReady`​：Pod 中所有容器都已就绪；
*   ​`Initialized`​：所有的 Init 容器 都已成功完成；
*   ​`Ready`​：Pod 可以为请求提供服务，并且应该被添加到对应服务的负载均衡池中。

字段名称

描述

`type`

Pod 状况的名称

`status`

表明该状况是否适用，可能的取值有 "`True`", "`False`" 或 "`Unknown`"

`lastProbeTime`

上次探测 Pod 状况时的时间戳

`lastTransitionTime`

Pod 上次从一种状态转换到另一种状态时的时间戳

`reason`

机器可读的、驼峰编码（UpperCamelCase）的文字，表述上次状况变化的原因

`message`

人类可读的消息，给出上次状态转换的详细信息

### Pod 就绪态 

FEATURE STATE: Kubernetes v1.14 \[stable\]

你的应用可以向 PodStatus 中注入额外的反馈或者信号：Pod Readiness（Pod 就绪态）。 要使用这一特性，可以设置 Pod 规约中的 ​`readinessGates` ​列表，为 kubelet 提供一组额外的状况供其评估 Pod 就绪态时使用。

就绪态门控基于 Pod 的 ​`status.conditions`​ 字段的当前值来做决定。 如果 Kubernetes 无法在 ​`status.conditions`​ 字段中找到某状况，则该状况的 状态值默认为 "​`False`​"。

这里是一个例子：

`kind: Pod ... spec:   readinessGates:     - conditionType: "www.example.com/feature-1" status:   conditions:     - type: Ready                              # 内置的 Pod 状况       status: "False"       lastProbeTime: null       lastTransitionTime: 2018-01-01T00:00:00Z     - type: "www.example.com/feature-1"        # 额外的 Pod 状况       status: "False"       lastProbeTime: null       lastTransitionTime: 2018-01-01T00:00:00Z   containerStatuses:     - containerID: docker://abcd...       ready: true ...`

你所添加的 Pod 状况名称必须满足 Kubernetes 标签键名格式。

### Pod 就绪态的状态

命令 ​`kubectl patch`​ 不支持修改对象的状态。 如果需要设置 Pod 的 ​`status.conditions`​，应用或者 Operators 需要使用 ​`PATCH` ​操作。 你可以使用 Kubernetes 客户端库 之一来编写代码，针对 Pod 就绪态设置定制的 Pod 状况。

对于使用定制状况的 Pod 而言，只有当下面的陈述都适用时，该 Pod 才会被评估为就绪：

*   Pod 中所有容器都已就绪；
*   readinessGates 中的所有状况都为 True 值。

当 Pod 的容器都已就绪，但至少一个定制状况没有取值或者取值为 ​`False`​， ​`kubelet` ​将 Pod 的状况设置为 ​`ContainersReady`​。

容器探针 
-----

probe 是由 kubelet 对容器执行的定期诊断。 要执行诊断，kubelet 既可以在容器内执行代码，也可以发出一个网络请求。

### 检查机制 

使用探针来检查容器有四种不同的方法。 每个探针都必须准确定义为这四种机制中的一种：

*   exec
在容器内执行指定命令。如果命令退出时返回码为 0 则认为诊断成功。*   grpc
使用 gRPC 执行一个远程过程调用。 目标应该实现 gRPC健康检查。 如果响应的状态是 "SERVING"，则认为诊断成功。 gRPC 探针是一个 alpha 特性，只有在你启用了 "GRPCContainerProbe" 特性门控时才能使用。*   httpGet
对容器的 IP 地址上指定端口和路径执行 HTTP GET 请求。如果响应的状态码大于等于 200 且小于 400，则诊断被认为是成功的。*   tcpSocket
对容器的 IP 地址上的指定端口执行 TCP 检查。如果端口打开，则诊断被认为是成功的。 如果远程系统（容器）在打开连接后立即将其关闭，这算作是健康的。

### 探测结果 

每次探测都将获得以下三种结果之一：

*   Success（成功）
容器通过了诊断。*   Failure（失败）
容器未通过诊断。*   Unknown（未知）
诊断失败，因此不会采取任何行动。

### 探测类型 

针对运行中的容器，​`kubelet` ​可以选择是否执行以下三种探针，以及如何针对探测结果作出反应：

*   livenessProbe
指示容器是否正在运行。如果存活态探测失败，则 kubelet 会杀死容器， 并且容器将根据其重启策略决定未来。如果容器不提供存活探针， 则默认状态为 Success。*   readinessProbe
指示容器是否准备好为请求提供服务。如果就绪态探测失败， 端点控制器将从与 Pod 匹配的所有服务的端点列表中删除该 Pod 的 IP 地址。 初始延迟之前的就绪态的状态值默认为 Failure。 如果容器不提供就绪态探针，则默认状态为 Success。*   startupProbe
指示容器中的应用是否已经启动。如果提供了启动探针，则所有其他探针都会被 禁用，直到此探针成功为止。如果启动探测失败，kubelet 将杀死容器，而容器依其 重启策略进行重启。 如果容器没有提供启动探测，则默认状态为 Success。

### 何时该使用存活态探针? 

FEATURE STATE: Kubernetes v1.0 \[stable\]

如果容器中的进程能够在遇到问题或不健康的情况下自行崩溃，则不一定需要存活态探针; ​`kubelet` ​将根据 Pod 的​`restartPolicy` ​自动执行修复操作。

如果你希望容器在探测失败时被杀死并重新启动，那么请指定一个存活态探针， 并指定​`restartPolicy` ​为 "​`Always`​" 或 "​`OnFailure`​"。

### 何时该使用就绪态探针? 

FEATURE STATE: Kubernetes v1.0 \[stable\]

如果要仅在探测成功时才开始向 Pod 发送请求流量，请指定就绪态探针。 在这种情况下，就绪态探针可能与存活态探针相同，但是规约中的就绪态探针的存在意味着 Pod 将在启动阶段不接收任何数据，并且只有在探针探测成功后才开始接收数据。

如果你希望容器能够自行进入维护状态，也可以指定一个就绪态探针，检查某个特定于 就绪态的因此不同于存活态探测的端点。

如果你的应用程序对后端服务有严格的依赖性，你可以同时实现存活态和就绪态探针。 当应用程序本身是健康的，存活态探针检测通过后，就绪态探针会额外检查每个所需的后端服务是否可用。 这可以帮助你避免将流量导向只能返回错误信息的 Pod。

如果你的容器需要在启动期间加载大型数据、配置文件或执行迁移，你可以使用 启动探针。 然而，如果你想区分已经失败的应用和仍在处理其启动数据的应用，你可能更倾向于使用就绪探针。

> 请注意，如果你只是想在 Pod 被删除时能够排空请求，则不一定需要使用就绪态探针； 在删除 Pod 时，Pod 会自动将自身置于未就绪状态，无论就绪态探针是否存在。 等待 Pod 中的容器停止期间，Pod 会一直处于未就绪状态。

### 何时该使用启动探针？ 

FEATURE STATE: Kubernetes v1.18 \[beta\]

对于所包含的容器需要较长时间才能启动就绪的 Pod 而言，启动探针是有用的。 你不再需要配置一个较长的存活态探测时间间隔，只需要设置另一个独立的配置选定， 对启动期间的容器执行探测，从而允许使用远远超出存活态时间间隔所允许的时长。

如果你的容器启动时间通常超出 ​`initialDelaySeconds + failureThreshold × periodSeconds`​ 总值，你应该设置一个启动探测，对存活态探针所使用的同一端点执行检查。 ​`periodSeconds` ​的默认值是 10 秒。你应该将其 ​`failureThreshold`​ 设置得足够高， 以便容器有充足的时间完成启动，并且避免更改存活态探针所使用的默认值。 这一设置有助于减少死锁状况的发生。

Pod 的终止 
--------

由于 Pod 所代表的是在集群中节点上运行的进程，当不再需要这些进程时允许其体面地 终止是很重要的。一般不应武断地使用 ​`KILL` ​信号终止它们，导致这些进程没有机会 完成清理操作。

设计的目标是令你能够请求删除进程，并且知道进程何时被终止，同时也能够确保删除 操作终将完成。当你请求删除某个 Pod 时，集群会记录并跟踪 Pod 的体面终止周期， 而不是直接强制地杀死 Pod。在存在强制关闭设施的前提下， kubelet 会尝试体面地终止 Pod。

通常情况下，容器运行时会发送一个 TERM 信号到每个容器中的主进程。 很多容器运行时都能够注意到容器镜像中 ​`STOPSIGNAL` ​的值，并发送该信号而不是 TERM。 一旦超出了体面终止限期，容器运行时会向所有剩余进程发送 KILL 信号，之后 Pod 就会被从 API 服务器 上移除。如果 ​`kubelet` ​或者容器运行时的管理服务在等待进程终止期间被重启， 集群会从头开始重试，赋予 Pod 完整的体面终止限期。

下面是一个例子：

1.  你使用 ​`kubectl` ​工具手动删除某个特定的 Pod，而该 Pod 的体面终止限期是默认值（30 秒）。
2.  API 服务器中的 Pod 对象被更新，记录涵盖体面终止限期在内 Pod 的最终死期，超出所计算时间点则认为 Pod 已死（dead）。 如果你使用 ​`kubectl describe`​ 来查验你正在删除的 Pod，该 Pod 会显示为 "Terminating" （正在终止）。 在 Pod 运行所在的节点上：​`kubelet` ​一旦看到 Pod 被标记为正在终止（已经设置了体面终止限期），​`kubelet` ​即开始本地的 Pod 关闭过程。

1.  如果 Pod 中的容器之一定义了 ​`preStop` ​回调， ​`kubelet` ​开始在容器内运行该回调逻辑。如果超出体面终止限期时，​`preStop` ​回调逻辑 仍在运行，​`kubelet` ​会请求给予该 Pod 的宽限期一次性增加 2 秒钟。

> 如果 ​`preStop` ​回调所需要的时间长于默认的体面终止限期，你必须修改 ​`terminationGracePeriodSeconds` ​属性值来使其正常工作。

3.  ​`kubelet` ​接下来触发容器运行时发送 TERM 信号给每个容器中的进程 1。

> Pod 中的容器会在不同时刻收到 TERM 信号，接收顺序也是不确定的。 如果关闭的顺序很重要，可以考虑使用 ​preStop ​回调逻辑来协调。

4.  与此同时，​`kubelet` ​启动体面关闭逻辑，控制面会将 Pod 从对应的端点列表（以及端点切片列表， 如果启用了的话）中移除，过滤条件是 Pod 被对应的 服务以某 选择算符选定。 ReplicaSets和其他工作负载资源 不再将关闭进程中的 Pod 视为合法的、能够提供服务的副本。关闭动作很慢的 Pod 也无法继续处理请求数据，因为负载均衡器（例如服务代理）已经在终止宽限期开始的时候 将其从端点列表中移除。
5.  超出终止宽限期限时，​`kubelet` ​会触发强制关闭过程。容器运行时会向 Pod 中所有容器内 仍在运行的进程发送 ​`SIGKILL` ​信号。 ​`kubelet` ​也会清理隐藏的 ​`pause` ​容器，如果容器运行时使用了这种容器的话。
6.  ​`kubelet` ​触发强制从 API 服务器上删除 Pod 对象的逻辑，并将体面终止限期设置为 0 （这意味着马上删除）。
7.  API 服务器删除 Pod 的 API 对象，从任何客户端都无法再看到该对象。

### 强制终止 Pod

> 对于某些工作负载及其 Pod 而言，强制删除很可能会带来某种破坏。

默认情况下，所有的删除操作都会附有 30 秒钟的宽限期限。 ​`kubectl delete`​ 命令支持 ​`--grace-period=<seconds>`​ 选项，允许你重载默认值， 设定自己希望的期限值。

将宽限期限强制设置为 ​`0`​ 意味着立即从 API 服务器删除 Pod。 如果 Pod 仍然运行于某节点上，强制删除操作会触发 ​`kubelet` ​立即执行清理操作。

> 你必须在设置 ​`--grace-period=0`​ 的同时额外设置 ​`--force`​ 参数才能发起强制删除请求。

执行强制删除操作时，API 服务器不再等待来自 ​`kubelet` ​的、关于 Pod 已经在原来运行的节点上终止执行的确认消息。 API 服务器直接删除 Pod 对象，这样新的与之同名的 Pod 即可以被创建。 在节点侧，被设置为立即终止的 Pod 仍然会在被强行杀死之前获得一点点的宽限时间。

### 失效 Pod 的垃圾收集 

对于已失败的 Pod 而言，对应的 API 对象仍然会保留在集群的 API 服务器上，直到 用户或者控制器进程显式地 将其删除。

控制面组件会在 Pod 个数超出所配置的阈值 （根据 ​`kube-controller-manager`​ 的 ​`terminated-pod-gc-threshold`​ 设置）时 删除已终止的 Pod（阶段值为 ​`Succeeded` ​或 ​`Failed`​）。 这一行为会避免随着时间演进不断创建和终止 Pod 而引起的资源泄露问题。

##  2.  Kubernetes Init容器
Init 容器
-------

本页提供了 Init 容器的概览。Init 容器是一种特殊容器，在 Pod 内的应用容器启动之前运行。Init 容器可以包括一些应用镜像中不存在的实用工具和安装脚本。

你可以在 Pod 的规约中与用来描述应用容器的 ​`containers` ​数组平行的位置指定 Init 容器。

理解 Init 容器
----------

每个 Pod 中可以包含多个容器， 应用运行在这些容器里面，同时 Pod 也可以有一个或多个先于应用容器启动的 Init 容器。

Init 容器与普通的容器非常像，除了如下两点：

*   它们总是运行到完成。
*   每个都必须在下一个启动之前成功完成。

如果 Pod 的 Init 容器失败，kubelet 会不断地重启该 Init 容器直到该容器成功为止。 然而，如果 Pod 对应的 ​`restartPolicy` ​值为 "Never"，并且 Pod 的 Init 容器失败， 则 Kubernetes 会将整个 Pod 状态设置为失败。

为 Pod 设置 Init 容器需要在 Pod 规约 中添加 ​`initContainers` ​字段， 该字段以 Container 类型对象数组的形式组织，和应用的 ​`containers` ​数组同级相邻。 参阅 API 参考的容器章节了解详情。

Init 容器的状态在 ​`status.initContainerStatuses`​ 字段中以容器状态数组的格式返回 （类似 ​`status.containerStatuses`​ 字段）。

### 与普通容器的不同之处 

Init 容器支持应用容器的全部字段和特性，包括资源限制、数据卷和安全设置。 然而，Init 容器对资源请求和限制的处理稍有不同，在下面资源节有说明。

同时 Init 容器不支持 ​`lifecycle`​、​`livenessProbe`​、​`readinessProbe` ​和 ​`startupProbe`​， 因为它们必须在 Pod 就绪之前运行完成。

如果为一个 Pod 指定了多个 Init 容器，这些容器会按顺序逐个运行。 每个 Init 容器必须运行成功，下一个才能够运行。当所有的 Init 容器运行完成时， Kubernetes 才会为 Pod 初始化应用容器并像平常一样运行。

使用 Init 容器
----------

因为 Init 容器具有与应用容器分离的单独镜像，其启动相关代码具有如下优势：

*   Init 容器可以包含一些安装过程中应用容器中不存在的实用工具或个性化代码。 例如，没有必要仅为了在安装过程中使用类似 ​`sed`​、​`awk`​、​`python` ​或 ​`dig` ​这样的工具而去 ​`FROM` ​一个镜像来生成一个新的镜像。
*   Init 容器可以安全地运行这些工具，避免这些工具导致应用镜像的安全性降低。
*   应用镜像的创建者和部署者可以各自独立工作，而没有必要联合构建一个单独的应用镜像。
*   Init 容器能以不同于 Pod 内应用容器的文件系统视图运行。因此，Init 容器可以访问 应用容器不能访问的 Secret 的权限。
*   由于 Init 容器必须在应用容器启动之前运行完成，因此 Init 容器 提供了一种机制来阻塞或延迟应用容器的启动，直到满足了一组先决条件。 一旦前置条件满足，Pod 内的所有的应用容器会并行启动。

### 示例 

下面是一些如何使用 Init 容器的想法：

*   等待一个 Service 完成创建，通过类似如下 shell 命令：

`for i in {1..100}; do sleep 1; if dig myservice; then exit 0; fi; done; exit 1`

*   注册这个 Pod 到远程服务器，通过在命令中调用 API，类似如下： 

`curl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register \   -d 'instance=$(<POD_NAME>)&ip=$(<POD_IP>)'`

*   在启动应用容器之前等一段时间，使用类似命令：

`sleep 60`

*   克隆 Git 仓库到卷中。
*   将配置值放到配置文件中，运行模板工具为主应用容器动态地生成配置文件。 例如，在配置文件中存放 POD\_IP 值，并使用 Jinja 生成主应用配置文件。

### 使用 Init 容器的情况

下面的例子定义了一个具有 2 个 Init 容器的简单 Pod。 第一个等待 ​`myservice` ​启动， 第二个等待 ​`mydb` ​启动。 一旦这两个 Init容器 都启动完成，Pod 将启动 ​`spec` ​节中的应用容器。

`apiVersion: v1 kind: Pod metadata:   name: myapp-pod   labels:     app: myapp spec:   containers:   - name: myapp-container     image: busybox:1.28     command: ['sh', '-c', 'echo The app is running! && sleep 3600']   initContainers:   - name: init-myservice     image: busybox:1.28     command: ['sh', '-c', "until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done"]   - name: init-mydb     image: busybox:1.28     command: ['sh', '-c', "until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done"]`

你通过运行下面的命令启动 Pod：

`kubectl apply -f myapp.yaml`

输出类似于：

`pod/myapp-pod created`

使用下面的命令检查其状态：

`kubectl get -f myapp.yaml`

输出类似于：

`NAME        READY     STATUS     RESTARTS   AGE myapp-pod   0/1       Init:0/2   0          6m`

或者查看更多详细信息：

`kubectl describe -f myapp.yaml`

输出类似于：

`Name:          myapp-pod Namespace:     default [...] Labels:        app=myapp Status:        Pending [...] Init Containers:   init-myservice: [...]     State:         Running [...]   init-mydb: [...]     State:         Waiting       Reason:      PodInitializing     Ready:         False [...] Containers:   myapp-container: [...]     State:         Waiting       Reason:      PodInitializing     Ready:         False [...] Events:   FirstSeen    LastSeen    Count    From                      SubObjectPath                           Type          Reason        Message   ---------    --------    -----    ----                      -------------                           --------      ------        -------   16s          16s         1        {default-scheduler }                                              Normal        Scheduled     Successfully assigned myapp-pod to 172.17.4.201   16s          16s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulling       pulling image "busybox"   13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulled        Successfully pulled image "busybox"   13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Created       Created container with docker id 5ced34a04634; Security:[seccomp=unconfined]   13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Started       Started container with docker id 5ced34a04634`

如需查看 Pod 内 Init 容器的日志，请执行：

`kubectl logs myapp-pod -c init-myservice # 查看第一个 Init 容器 kubectl logs myapp-pod -c init-mydb      # 查看第二个 Init 容器`

在这一刻，Init 容器将会等待至发现名称为 ​`mydb` ​和 ​`myservice` ​的 Service。

如下为创建这些 Service 的配置文件：

`--- apiVersion: v1 kind: Service metadata:   name: myservice spec:   ports:   - protocol: TCP     port: 80     targetPort: 9376 --- apiVersion: v1 kind: Service metadata:   name: mydb spec:   ports:   - protocol: TCP     port: 80     targetPort: 9377`

创建 ​`mydb` ​和 ​`myservice` ​服务的命令：

`kubectl create -f services.yaml`

输出类似于：

`service "myservice" created service "mydb" created`

这样你将能看到这些 Init 容器执行完毕，随后 ​`my-app`​ 的 Pod 进入 ​`Running` ​状态：

`kubectl get -f myapp.yaml`

输出类似于：

`NAME        READY     STATUS    RESTARTS   AGE myapp-pod   1/1       Running   0          9m`

具体行为
----

在 Pod 启动过程中，每个 Init 容器会在网络和数据卷初始化之后按顺序启动。 kubelet 运行依据 Init 容器在 Pod 规约中的出现顺序依次运行之。

每个 Init 容器成功退出后才会启动下一个 Init 容器。 如果某容器因为容器运行时的原因无法启动，或以错误状态退出，kubelet 会根据 Pod 的 ​`restartPolicy` ​策略进行重试。 然而，如果 Pod 的 ​`restartPolicy` ​设置为 "Always"，Init 容器失败时会使用 ​`restartPolicy` ​的 "OnFailure" 策略。

在所有的 Init 容器没有成功之前，Pod 将不会变成 ​`Ready` ​状态。 Init 容器的端口将不会在 Service 中进行聚集。正在初始化中的 Pod 处于 ​`Pending` ​状态， 但会将状况 ​`Initializing` ​设置为 false。

如果 Pod 重启，所有 Init 容器必须重新执行。

对 Init 容器规约的修改仅限于容器的 ​`image` ​字段。 更改 Init 容器的 ​`image` ​字段，等同于重启该 Pod。

因为 Init 容器可能会被重启、重试或者重新执行，所以 Init 容器的代码应该是幂等的。 特别地，基于 ​`emptyDirs` ​写文件的代码，应该对输出文件可能已经存在做好准备。

Init 容器具有应用容器的所有字段。然而 Kubernetes 禁止使用 ​`readinessProbe`​， 因为 Init 容器不能定义不同于完成态（Completion）的就绪态（Readiness）。 Kubernetes 会在校验时强制执行此检查。

在 Pod 上使用 ​`activeDeadlineSeconds` ​和在容器上使用 ​`livenessProbe` ​可以避免 Init 容器一直重复失败。 ​`activeDeadlineSeconds` ​时间包含了 Init 容器启动的时间。 但建议仅在团队将其应用程序部署为 Job 时才使用 ​`activeDeadlineSeconds`​， 因为 ​`activeDeadlineSeconds` ​在 Init 容器结束后仍有效果。 如果你设置了 ​`activeDeadlineSeconds`​，已经在正常运行的 Pod 会被杀死。

在 Pod 中的每个应用容器和 Init 容器的名称必须唯一； 与任何其它容器共享同一个名称，会在校验时抛出错误。

### 资源

在给定的 Init 容器执行顺序下，资源使用适用于如下规则：

*   所有 Init 容器上定义的任何特定资源的 limit 或 request 的最大值，作为 Pod 有效初始 request/limit。 如果任何资源没有指定资源限制，这被视为最高限制。
*   Pod 对资源的 有效 limit/request 是如下两者的较大者：

*   所有应用容器对某个资源的 limit/request 之和
*   对某个资源的有效初始 limit/request

*   基于有效 limit/request 完成调度，这意味着 Init 容器能够为初始化过程预留资源， 这些资源在 Pod 生命周期过程中并没有被使用。
*   Pod 的 有效 QoS 层 ，与 Init 容器和应用容器的一样。

配额和限制适用于有效 Pod 的请求和限制值。 Pod 级别的 cgroups 是基于有效 Pod 的请求和限制值，和调度器相同。

### Pod 重启的原因 

Pod 重启会导致 Init 容器重新执行，主要有如下几个原因：

*   Pod 的基础设施容器 (译者注：如 ​`pause` ​容器) 被重启。这种情况不多见， 必须由具备 root 权限访问节点的人员来完成。
*   当 ​`restartPolicy` ​设置为 "​`Always`​"，Pod 中所有容器会终止而强制重启。 由于垃圾收集机制的原因，Init 容器的完成记录将会丢失。

当 Init 容器的镜像发生改变或者 Init 容器的完成记录因为垃圾收集等原因被丢失时， Pod 不会被重启。这一行为适用于 Kubernetes v1.20 及更新版本。如果你在使用较早 版本的 Kubernetes，可查阅你所使用的版本对应的文档。

##  3.  Kubernetes Pod拓扑分布约束
Pod 拓扑分布约束
----------

FEATURE STATE: Kubernetes v1.19 \[stable\]

你可以使用 拓扑分布约束（Topology Spread Constraints） 来控制 Pods 在集群内故障域 之间的分布，例如区域（Region）、可用区（Zone）、节点和其他用户自定义拓扑域。 这样做有助于实现高可用并提升资源利用率。

先决条件 
-----

### 节点标签 

拓扑分布约束依赖于节点标签来标识每个节点所在的拓扑域。 例如，某节点可能具有标签：​`node=node1,zone=us-east-1a,region=us-east-1` ​

假设你拥有具有以下标签的一个 4 节点集群：

`NAME    STATUS   ROLES    AGE     VERSION   LABELS node1   Ready    <none>   4m26s   v1.16.0   node=node1,zone=zoneA node2   Ready    <none>   3m58s   v1.16.0   node=node2,zone=zoneA node3   Ready    <none>   3m17s   v1.16.0   node=node3,zone=zoneB node4   Ready    <none>   2m43s   v1.16.0   node=node4,zone=zoneB`

那么，从逻辑上看集群如下：

  ![](https://atts.w3cschool.cn/attachments/image/20220429/1651214186740469.png)  

你可以复用在大多数集群上自动创建和填充的常用标签， 而不是手动添加标签。

Pod 的分布约束 
----------

### API

​`pod.spec.topologySpreadConstraints`​ 字段定义如下所示：

`apiVersion: v1 kind: Pod metadata:   name: mypod spec:   topologySpreadConstraints:     - maxSkew: <integer>       topologyKey: <string>       whenUnsatisfiable: <string>       labelSelector: <object>`

你可以定义一个或多个 ​`topologySpreadConstraint`​ 来指示 kube-scheduler 如何根据与现有的 Pod 的关联关系将每个传入的 Pod 部署到集群中。字段包括：

*   maxSkew 描述 Pod 分布不均的程度。这是给定拓扑类型中任意两个拓扑域中 匹配的 pod 之间的最大允许差值。它必须大于零。取决于 ​`whenUnsatisfiable` ​的 取值，其语义会有不同。

*   当 ​`whenUnsatisfiable` ​等于 "DoNotSchedule" 时，​`maxSkew` ​是目标拓扑域 中匹配的 Pod 数与全局最小值之间可存在的差异。
*   当 ​`whenUnsatisfiable` ​等于 "ScheduleAnyway" 时，调度器会更为偏向能够降低 偏差值的拓扑域。

*   topologyKey 是节点标签的键。如果两个节点使用此键标记并且具有相同的标签值， 则调度器会将这两个节点视为处于同一拓扑域中。调度器试图在每个拓扑域中放置数量 均衡的 Pod。
*   whenUnsatisfiable 指示如果 Pod 不满足分布约束时如何处理：

*   ​`DoNotSchedule`​（默认）告诉调度器不要调度。
*   ​`ScheduleAnyway` ​告诉调度器仍然继续调度，只是根据如何能将偏差最小化来对 节点进行排序。

*   labelSelector 用于查找匹配的 pod。匹配此标签的 Pod 将被统计，以确定相应 拓扑域中 Pod 的数量。

当 Pod 定义了不止一个 ​`topologySpreadConstraint`​，这些约束之间是逻辑与的关系。 kube-scheduler 会为新的 Pod 寻找一个能够满足所有约束的节点。

你可以执行 ​`kubectl explain Pod.spec.topologySpreadConstraints`​ 命令以 了解关于 topologySpreadConstraints 的更多信息。

### 例子：单个 TopologySpreadConstraint

假设你拥有一个 4 节点集群，其中标记为 ​`foo:bar`​ 的 3 个 Pod 分别位于 node1、node2 和 node3 中：

![](https://atts.w3cschool.cn/attachments/image/20220429/1651214227197110.png)  

如果希望新来的 Pod 均匀分布在现有的可用区域，则可以按如下设置其规约：

`kind: Pod apiVersion: v1 metadata:   name: mypod   labels:     foo: bar spec:   topologySpreadConstraints:   - maxSkew: 1     topologyKey: zone     whenUnsatisfiable: DoNotSchedule     labelSelector:       matchLabels:         foo: bar   containers:   - name: pause     image: k8s.gcr.io/pause:3.1`

​`topologyKey: zone`​ 意味着均匀分布将只应用于存在标签键值对为 "zone:<any value>" 的节点。 ​`whenUnsatisfiable: DoNotSchedule`​ 告诉调度器如果新的 Pod 不满足约束， 则让它保持悬决状态。

如果调度器将新的 Pod 放入 "zoneA"，Pods 分布将变为 \[3, 1\]，因此实际的偏差 为 2（3 - 1）。这违反了 ​`maxSkew: 1`​ 的约定。此示例中，新 Pod 只能放置在 "zoneB" 上：

![](https://atts.w3cschool.cn/attachments/image/20220429/1651214242600816.png)  

或者

![](https://atts.w3cschool.cn/attachments/image/20220429/1651214261240538.png)

你可以调整 Pod 规约以满足各种要求：

*   将 ​`maxSkew` ​更改为更大的值，比如 "2"，这样新的 Pod 也可以放在 "zoneA" 上。
*   将 ​`topologyKey` ​更改为 "node"，以便将 Pod 均匀分布在节点上而不是区域中。 在上面的例子中，如果 ​`maxSkew` ​保持为 "1"，那么传入的 Pod 只能放在 "node4" 上。
*   将 ​`whenUnsatisfiable: DoNotSchedule`​ 更改为 ​`whenUnsatisfiable: ScheduleAnyway`​， 以确保新的 Pod 始终可以被调度（假设满足其他的调度 API）。 但是，最好将其放置在匹配 Pod 数量较少的拓扑域中。 （请注意，这一优先判定会与其他内部调度优先级（如资源使用率等）排序准则一起进行标准化。）

### 例子：多个 TopologySpreadConstraints

下面的例子建立在前面例子的基础上。假设你拥有一个 4 节点集群，其中 3 个标记为 ​`foo:bar`​ 的 Pod 分别位于 node1、node2 和 node3 上：

![](https://atts.w3cschool.cn/attachments/image/20220429/1651214728456296.png)  

可以使用 2 个 TopologySpreadConstraint 来控制 Pod 在 区域和节点两个维度上的分布：

`kind: Pod apiVersion: v1 metadata:   name: mypod   labels:     foo: bar spec:   topologySpreadConstraints:   - maxSkew: 1     topologyKey: zone     whenUnsatisfiable: DoNotSchedule     labelSelector:       matchLabels:         foo: bar   - maxSkew: 1     topologyKey: node     whenUnsatisfiable: DoNotSchedule     labelSelector:       matchLabels:         foo: bar   containers:   - name: pause     image: k8s.gcr.io/pause:3.1`

在这种情况下，为了匹配第一个约束，新的 Pod 只能放置在 "zoneB" 中；而在第二个约束中， 新的 Pod 只能放置在 "node4" 上。最后两个约束的结果加在一起，唯一可行的选择是放置 在 "node4" 上。

多个约束之间可能存在冲突。假设有一个跨越 2 个区域的 3 节点集群：

![](https://atts.w3cschool.cn/attachments/image/20220429/1651214747235932.png)  

如果对集群应用 "two-constraints.yaml"，会发现 "mypod" 处于 ​`Pending` ​状态。 这是因为：为了满足第一个约束，"mypod" 只能放在 "zoneB" 中，而第二个约束要求 "mypod" 只能放在 "node2" 上。Pod 调度无法满足两种约束。

为了克服这种情况，你可以增加 ​`maxSkew` ​或修改其中一个约束，让其使用 ​`whenUnsatisfiable: ScheduleAnyway`​。

### 节点亲和性与节点选择器的相互作用 

如果 Pod 定义了 ​`spec.nodeSelector`​ 或 ​`spec.affinity.nodeAffinity`​， 调度器将在偏差计算中跳过不匹配的节点。

### 示例：TopologySpreadConstraints 与 NodeAffinity

假设你有一个跨越 zoneA 到 zoneC 的 5 节点集群：

![](https://atts.w3cschool.cn/attachments/image/20220429/1651214762408272.png)  

![](https://atts.w3cschool.cn/attachments/image/20220429/1651214786255141.png)  

而且你知道 "zoneC" 必须被排除在外。在这种情况下，可以按如下方式编写 YAML， 以便将 "mypod" 放置在 "zoneB" 上，而不是 "zoneC" 上。同样，​`spec.nodeSelector`​ 也要一样处理。

`kind: Pod apiVersion: v1 metadata:   name: mypod   labels:     foo: bar spec:   topologySpreadConstraints:   - maxSkew: 1     topologyKey: zone     whenUnsatisfiable: DoNotSchedule     labelSelector:       matchLabels:         foo: bar   affinity:     nodeAffinity:       requiredDuringSchedulingIgnoredDuringExecution:         nodeSelectorTerms:         - matchExpressions:           - key: zone             operator: NotIn             values:             - zoneC   containers:   - name: pause     image: k8s.gcr.io/pause:3.1`

调度器不会预先知道集群拥有的所有区域和其他拓扑域。拓扑域由集群中存在的节点确定。 在自动伸缩的集群中，如果一个节点池（或节点组）的节点数量为零， 而用户正期望其扩容时，可能会导致调度出现问题。 因为在这种情况下，调度器不会考虑这些拓扑域信息，因为它们是空的，没有节点。

### 其他值得注意的语义 

这里有一些值得注意的隐式约定：

*   只有与新的 Pod 具有相同命名空间的 Pod 才能作为匹配候选者。
*   调度器会忽略没有 topologySpreadConstraints\[\*\].topologyKey 的节点。这意味着：

1.  位于这些节点上的 Pod 不影响 maxSkew 的计算。 在上面的例子中，假设 "node1" 没有标签 "zone"，那么 2 个 Pod 将被忽略， 因此传入的 Pod 将被调度到 "zoneA" 中。
2.  新的 Pod 没有机会被调度到这类节点上。 在上面的例子中，假设一个带有标签 {zone-typo: zoneC} 的 "node5" 加入到集群， 它将由于没有标签键 "zone" 而被忽略。

*   注意，如果新 Pod 的 topologySpreadConstraints\[\*\].labelSelector 与自身的 标签不匹配，将会发生什么。 在上面的例子中，如果移除新 Pod 上的标签，Pod 仍然可以调度到 "zoneB"，因为约束仍然满足。 然而，在调度之后，集群的不平衡程度保持不变。zoneA 仍然有 2 个带有 {foo:bar} 标签的 Pod， zoneB 有 1 个带有 {foo:bar} 标签的 Pod。 因此，如果这不是你所期望的，建议工作负载的 topologySpreadConstraints\[\*\].labelSelector 与其自身的标签匹配。

### 集群级别的默认约束 

为集群设置默认的拓扑分布约束也是可能的。默认拓扑分布约束在且仅在以下条件满足 时才会应用到 Pod 上：

*   Pod 没有在其 ​`.spec.topologySpreadConstraints`​ 设置任何约束；
*   Pod 隶属于某个服务、副本控制器、ReplicaSet 或 StatefulSet。

你可以在 调度方案（Scheduling Profile） 中将默认约束作为 ​`PodTopologySpread` ​插件参数的一部分来设置。 约束的设置采用如前所述的 API，只是 ​`labelSelector` ​必须为空。 选择算符是根据 Pod 所属的服务、副本控制器、ReplicaSet 或 StatefulSet 来设置的。

配置的示例可能看起来像下面这个样子：

`apiVersion: kubescheduler.config.k8s.io/v1beta3 kind: KubeSchedulerConfiguration  profiles:   - schedulerName: default-scheduler     pluginConfig:       - name: PodTopologySpread         args:           defaultConstraints:             - maxSkew: 1               topologyKey: topology.kubernetes.io/zone               whenUnsatisfiable: ScheduleAnyway           defaultingType: List`

> 默认调度约束所生成的评分可能与 SelectorSpread 插件 所生成的评分有冲突。 建议你在为 ​`PodTopologySpread` ​设置默认约束是禁用调度方案中的该插件。

### 内部默认约束 

FEATURE STATE: Kubernetes v1.20 \[beta\]

当你使用了默认启用的 ​`DefaultPodTopologySpread`​ 特性门控时，原来的 ​`SelectorSpread` ​插件会被禁用。 kube-scheduler 会使用下面的默认拓扑约束作为 ​`PodTopologySpread` ​插件的 配置：

`defaultConstraints:   - maxSkew: 3     topologyKey: "kubernetes.io/hostname"     whenUnsatisfiable: ScheduleAnyway   - maxSkew: 5     topologyKey: "topology.kubernetes.io/zone"     whenUnsatisfiable: ScheduleAnyway`

此外，原来用于提供等同行为的 ​`SelectorSpread` ​插件也会被禁用。

> 对于分布约束中所指定的拓扑键而言，​`PodTopologySpread` ​插件不会为不包含这些主键的节点评分。 这可能导致在使用默认拓扑约束时，其行为与原来的 ​`SelectorSpread` ​插件的默认行为不同，  
>   
> 如果你的节点不会 同时 设置 ​`kubernetes.io/hostname`​ 和 ​`topology.kubernetes.io/zone`​ 标签，你应该定义自己的约束而不是使用 Kubernetes 的默认约束。

如果你不想为集群使用默认的 Pod 分布约束，你可以通过设置 ​`defaultingType` ​参数为 ​`List` ​并将 ​`PodTopologySpread` ​插件配置中的 ​`defaultConstraints` ​参数置空来禁用默认 Pod 分布约束。

`apiVersion: kubescheduler.config.k8s.io/v1beta3 kind: KubeSchedulerConfiguration  profiles:   - schedulerName: default-scheduler     pluginConfig:       - name: PodTopologySpread         args:           defaultConstraints: []           defaultingType: List`

    

与 PodAffinity/PodAntiAffinity 相比较
---------------------------------

在 Kubernetes 中，与“亲和性”相关的指令控制 Pod 的调度方式（更密集或更分散）。

*   对于 ​`PodAffinity`​，你可以尝试将任意数量的 Pod 集中到符合条件的拓扑域中。
*   对于 ​`PodAntiAffinity`​，只能将一个 Pod 调度到某个拓扑域中。

要实现更细粒度的控制，你可以设置拓扑分布约束来将 Pod 分布到不同的拓扑域下， 从而实现高可用性或节省成本。这也有助于工作负载的滚动更新和平稳地扩展副本规模。

已知局限性
-----

*   当 Pod 被移除时，无法保证约束仍被满足。例如，缩减某 Deployment 的规模时， Pod 的分布可能不再均衡。 你可以使用 Descheduler 来重新实现 Pod 分布的均衡。
*   具有污点的节点上匹配的 Pods 也会被统计。 参考 [Issue 80921](https://github.com/kubernetes/kubernetes/issues/80921)。

##  4.  Kubernetes 干扰（Disruptions）
干扰（Disruptions）
---------------

本指南针对的是希望构建高可用性应用程序的应用所有者，他们有必要了解可能发生在 Pod 上的干扰类型。

文档同样适用于想要执行自动化集群操作（例如升级和自动扩展集群）的集群管理员。

自愿干扰和非自愿干扰 
-----------

Pod 不会消失，除非有人（用户或控制器）将其销毁，或者出现了不可避免的硬件或软件系统错误。

我们把这些不可避免的情况称为应用的非自愿干扰（Involuntary Disruptions）。例如：

*   节点下层物理机的硬件故障
*   集群管理员错误地删除虚拟机（实例）
*   云提供商或虚拟机管理程序中的故障导致的虚拟机消失
*   内核错误
*   节点由于集群网络隔离从集群中消失
*   由于节点资源不足导致 pod 被驱逐。

除了资源不足的情况，大多数用户应该都熟悉这些情况；它们不是特定于 Kubernetes 的。

我们称其他情况为自愿干扰（Voluntary Disruptions）。 包括由应用程序所有者发起的操作和由集群管理员发起的操作。典型的应用程序所有者的操 作包括：

*   删除 Deployment 或其他管理 Pod 的控制器
*   更新了 Deployment 的 Pod 模板导致 Pod 重启
*   直接删除 Pod（例如，因为误操作）

集群管理员操作包括：

*   排空（drain）节点进行修复或升级。
*   从集群中排空节点以缩小集群（了解集群自动扩缩）。
*   从节点中移除一个 Pod，以允许其他 Pod 使用该节点。

这些操作可能由集群管理员直接执行，也可能由集群管理员所使用的自动化工具执行，或者由集群托管提供商自动执行。

咨询集群管理员或联系云提供商，或者查询发布文档，以确定是否为集群启用了任何资源干扰源。 如果没有启用，可以不用创建 Pod Disruption Budgets（Pod 干扰预算）

> 并非所有的自愿干扰都会受到 Pod 干扰预算的限制。 例如，删除 Deployment 或 Pod 的删除操作就会跳过 Pod 干扰预算检查。

处理干扰
----

以下是减轻非自愿干扰的一些方法：

*   确保 Pod 在请求中给出所需资源。
*   如果需要更高的可用性，请复制应用程序。
*   为了在运行复制应用程序时获得更高的可用性，请跨机架（使用 反亲和性 或跨区域（如果使用多区域集群）扩展应用程序。

自愿干扰的频率各不相同。在一个基本的 Kubernetes 集群中，没有自愿干扰（只有用户触发的干扰）。 然而，集群管理员或托管提供商可能运行一些可能导致自愿干扰的额外服务。例如，节点软 更新可能导致自愿干扰。另外，集群（节点）自动缩放的某些 实现可能导致碎片整理和紧缩节点的自愿干扰。集群 管理员或托管提供商应该已经记录了各级别的自愿干扰（如果有的话）。 有些配置选项，例如在 pod spec 中 使用 PriorityClasses 也会产生自愿（和非自愿）的干扰。

Kubernetes 提供特性来满足在出现频繁自愿干扰的同时运行高可用的应用程序。我们称这些特性为 干扰预算（Disruption Budget）。

干扰预算 
-----

FEATURE STATE: Kubernetes v1.21 \[stable\]

即使你会经常引入自愿性干扰，Kubernetes 也能够支持你运行高度可用的应用。

应用程序所有者可以为每个应用程序创建 ​`PodDisruptionBudget` ​对象（PDB）。 PDB 将限制在同一时间因自愿干扰导致的复制应用程序中宕机的 pod 数量。 例如，基于票选机制的应用程序希望确保运行的副本数永远不会低于仲裁所需的数量。 Web 前端可能希望确保提供负载的副本数量永远不会低于总数的某个百分比。

集群管理员和托管提供商应该使用遵循 PodDisruptionBudgets 的接口 （通过调用Eviction API）， 而不是直接删除 Pod 或 Deployment。

例如，​`kubectl drain`​ 命令可以用来标记某个节点即将停止服务。 运行 ​`kubectl drain`​ 命令时，工具会尝试驱逐机器上的所有 Pod。 ​`kubectl` ​所提交的驱逐请求可能会暂时被拒绝，所以该工具会定时重试失败的请求， 直到所有的 Pod 都被终止，或者达到配置的超时时间。

PDB 指定应用程序可以容忍的副本数量（相当于应该有多少副本）。 例如，具有 ​`.spec.replicas: 5`​ 的 Deployment 在任何时间都应该有 5 个 Pod。 如果 PDB 允许其在某一时刻有 4 个副本，那么驱逐 API 将允许同一时刻仅有一个而不是两个 Pod 自愿干扰。

使用标签选择器来指定构成应用程序的一组 Pod，这与应用程序的控制器（Deployment，StatefulSet 等） 选择 Pod 的逻辑一样。

Pod 控制器的 ​`.spec.replicas`​ 计算“预期的” Pod 数量。 根据 Pod 对象的 ​`.metadata.ownerReferences`​ 字段来发现控制器。

PDB 无法防止非自愿干扰； 但它们确实计入预算。

由于应用程序的滚动升级而被删除或不可用的 Pod 确实会计入干扰预算， 但是控制器（如 Deployment 和 StatefulSet）在进行滚动升级时不受 PDB 的限制。应用程序更新期间的故障处理方式是在对应的工作负载资源的 ​`spec` ​中配置的。

当使用驱逐 API 驱逐 Pod 时，Pod 会被体面地 终止，期间会 参考 [PodSpec](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/ target=) 中的 ​`terminationGracePeriodSeconds` ​配置值。

PDB 例子 
-------

假设集群有 3 个节点，​`node-1`​ 到 ​`node-3`​。集群上运行了一些应用。 其中一个应用有 3 个副本，分别是 ​`pod-a`​，​`pod-b`​ 和 ​`pod-c`​。 另外，还有一个不带 PDB 的无关 pod ​`pod-x`​ 也同样显示出来。 最初，所有的 Pod 分布如下：

node-1

node-2

node-3

pod-a _available_

pod-b _available_

pod-c _available_

pod-x _available_

3 个 Pod 都是 deployment 的一部分，并且共同拥有同一个 PDB，要求 3 个 Pod 中至少有 2 个 Pod 始终处于可用状态。

例如，假设集群管理员想要重启系统，升级内核版本来修复内核中的权限。 集群管理员首先使用 ​`kubectl drain`​ 命令尝试排空 ​`node-1`​ 节点。 命令尝试驱逐 ​`pod-a`​ 和 ​`pod-x`​。操作立即就成功了。 两个 Pod 同时进入 ​`terminating` ​状态。这时的集群处于下面的状态：

node-1 _draining_

node-2

node-3

pod-a _terminating_

pod-b _available_

pod-c _available_

pod-x _terminating_

Deployment 控制器观察到其中一个 Pod 正在终止，因此它创建了一个替代 Pod ​`pod-d`​。 由于 ​`node-1`​ 被封锁（cordon），​`pod-d`​ 落在另一个节点上。 同样其他控制器也创建了 ​`pod-y`​ 作为 ​`pod-x`​ 的替代品。

（注意：对于 StatefulSet 来说，​`pod-a`​（也称为 ​`pod-0`​）需要在替换 Pod 创建之前完全终止， 替代它的也称为 ​`pod-0`​，但是具有不同的 UID。除此之外，此示例也适用于 StatefulSet。）

当前集群的状态如下：

node-1 _draining_

node-2

node-3

pod-a _terminating_

pod-b _available_

pod-c _available_

pod-x _terminating_

pod-d _starting_

pod-y

在某一时刻，Pod 被终止，集群如下所示：

node-1 _drained_

node-2

node-3

pod-b _available_

pod-c _available_

pod-d _starting_

pod-y

此时，如果一个急躁的集群管理员试图排空（drain）​`node-2`​ 或 ​`node-3`​，drain 命令将被阻塞， 因为对于 Deployment 来说只有 2 个可用的 Pod，并且它的 PDB 至少需要 2 个。 经过一段时间，​`pod-d`​ 变得可用。

集群状态如下所示：

node-1 _drained_

node-2

node-3

pod-b _available_

pod-c _available_

pod-d _available_

pod-y

现在，集群管理员试图排空（drain）​`node-2`​。 drain 命令将尝试按照某种顺序驱逐两个 Pod，假设先是 ​`pod-b`​，然后是 ​`pod-d`​。 命令成功驱逐 ​`pod-b`​，但是当它尝试驱逐 ​`pod-d`​时将被拒绝，因为对于 Deployment 来说只剩一个可用的 Pod 了。

Deployment 创建 ​`pod-b`​ 的替代 Pod ​`pod-e`​。 因为集群中没有足够的资源来调度 ​`pod-e`​，drain 命令再次阻塞。集群最终将是下面这种状态：

node-1 _drained_

node-2

node-3

_no node_

pod-b _terminating_

pod-c _available_

pod-e _pending_

pod-d _available_

pod-y

此时，集群管理员需要增加一个节点到集群中以继续升级操作。

可以看到 Kubernetes 如何改变干扰发生的速率，根据：

*   应用程序需要多少个副本
*   优雅关闭应用实例需要多长时间
*   启动应用新实例需要多长时间
*   控制器的类型
*   集群的资源能力

分离集群所有者和应用所有者角色
---------------

通常，将集群管理者和应用所有者视为彼此了解有限的独立角色是很有用的。这种责任分离在下面这些场景下是有意义的：

*   当有许多应用程序团队共用一个 Kubernetes 集群，并且有自然的专业角色
*   当第三方工具或服务用于集群自动化管理

Pod 干扰预算通过在角色之间提供接口来支持这种分离。

如果你的组织中没有这样的责任分离，则可能不需要使用 Pod 干扰预算。

如何在集群上执行干扰性操作
-------------

如果你是集群管理员，并且需要对集群中的所有节点执行干扰操作，例如节点或系统软件升级，则可以使用以下选项

*   接受升级期间的停机时间。
*   故障转移到另一个完整的副本集群。

*   没有停机时间，但是对于重复的节点和人工协调成本可能是昂贵的。

*   编写可容忍干扰的应用程序和使用 PDB。

*   不停机。
*   最小的资源重复。
*   允许更多的集群管理自动化。
*   编写可容忍干扰的应用程序是棘手的，但对于支持容忍自愿干扰所做的工作，和支持自动扩缩和容忍非 自愿干扰所做工作相比，有大量的重叠

##  5.  Kubernetes 临时容器
临时容器
----

FEATURE STATE: Kubernetes v1.23 \[beta\]

本页面概述了临时容器：一种特殊的容器，该容器在现有 Pod 中临时运行，以便完成用户发起的操作，例如故障排查。 你会使用临时容器来检查服务，而不是用它来构建应用程序。

了解临时容器 
-------

Pod 是 Kubernetes 应用程序的基本构建块。 由于 Pod 是一次性且可替换的，因此一旦 Pod 创建，就无法将容器加入到 Pod 中。 取而代之的是，通常使用 Deployment 以受控的方式来删除并替换 Pod。

有时有必要检查现有 Pod 的状态。例如，对于难以复现的故障进行排查。 在这些场景中，可以在现有 Pod 中运行临时容器来检查其状态并运行任意命令。

### 什么是临时容器？ 

临时容器与其他容器的不同之处在于，它们缺少对资源或执行的保证，并且永远不会自动重启， 因此不适用于构建应用程序。 临时容器使用与常规容器相同的 ​`ContainerSpec` ​节来描述，但许多字段是不兼容和不允许的。

*   临时容器没有端口配置，因此像 ​`ports`​，​`livenessProbe`​，​`readinessProbe` ​这样的字段是不允许的。
*   Pod 资源分配是不可变的，因此 ​`resources` ​配置是不允许的。
*   有关允许字段的完整列表，请参见 [EphemeralContainer 参考文档](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/ target=)。

临时容器是使用 API 中的一种特殊的 ​`ephemeralcontainers`​ 处理器进行创建的， 而不是直接添加到 ​`pod.spec`​ 段，因此无法使用 ​`kubectl edit`​ 来添加一个临时容器。

与常规容器一样，将临时容器添加到 Pod 后，将不能更改或删除临时容器。

临时容器的用途 
--------

当由于容器崩溃或容器镜像不包含调试工具而导致 ​`kubectl exec`​ 无用时， 临时容器对于交互式故障排查很有用。

尤其是，[Distroless 镜像](https://github.com/GoogleContainerTools/distroless) 允许用户部署最小的容器镜像，从而减少攻击面并减少故障和漏洞的暴露。 由于 distroless 镜像不包含 Shell 或任何的调试工具，因此很难单独使用 ​`kubectl exec`​ 命令进行故障排查。

使用临时容器时，启用 进程名字空间共享 很有帮助，可以查看其他容器中的进程。

#  8.  Kubernetes 工作负载资源

##  1.  Kubernetes Deployments
Deployments
-----------

一个 Deployment 为 Pod 和 ReplicaSet 提供声明式的更新能力。

你负责描述 Deployment 中的 目标状态，而 Deployment 控制器（Controller） 以受控速率更改实际状态， 使其变为期望状态。你可以定义 Deployment 以创建新的 ReplicaSet，或删除现有 Deployment， 并通过新的 Deployment 收养其资源。

> 不要管理 Deployment 所拥有的 ReplicaSet 。 如果存在下面未覆盖的使用场景，请考虑在 Kubernetes 仓库中提出 Issue。

用例
--

以下是 Deployments 的典型用例：

*   创建 Deployment 以将 ReplicaSet 上线。 ReplicaSet 在后台创建 Pods。 检查 ReplicaSet 的上线状态，查看其是否成功。
*   通过更新 Deployment 的 PodTemplateSpec，声明 Pod 的新状态 。 新的 ReplicaSet 会被创建，Deployment 以受控速率将 Pod 从旧 ReplicaSet 迁移到新 ReplicaSet。 每个新的 ReplicaSet 都会更新 Deployment 的修订版本。
*   如果 Deployment 的当前状态不稳定，回滚到较早的 Deployment 版本。 每次回滚都会更新 Deployment 的修订版本。
*   扩大 Deployment 规模以承担更多负载。
*   暂停 Deployment 以应用对 PodTemplateSpec 所作的多项修改， 然后恢复其执行以启动新的上线版本。
*   使用 Deployment 状态 来判定上线过程是否出现停滞。
*   清理较旧的不再需要的 ReplicaSet 。

创建 Deployment 
--------------

下面是一个 Deployment 示例。其中创建了一个 ReplicaSet，负责启动三个 ​`nginx`​ Pods：

`apiVersion: apps/v1 kind: Deployment metadata:   name: nginx-deployment   labels:     app: nginx spec:   replicas: 3   selector:     matchLabels:       app: nginx   template:     metadata:       labels:         app: nginx     spec:       containers:       - name: nginx         image: nginx:1.14.2         ports:         - containerPort: 80`

在该例中：

*   创建名为 ​`nginx-deployment`​（由 ​`.metadata.name`​ 字段标明）的 Deployment。
*   该 Deployment 创建三个（由 ​`replicas` ​字段标明）Pod 副本。
*   ​`selector` ​字段定义 Deployment 如何查找要管理的 Pods。 在这里，你选择在 Pod 模板中定义的标签（​`app: nginx`​）。 不过，更复杂的选择规则是也可能的，只要 Pod 模板本身满足所给规则即可。

> ​`spec.selector.matchLabels`​ 字段是 ​`{key,value}`​ 键值对映射。 在 ​`matchLabels` ​映射中的每个 ​`{key,value}`​ 映射等效于 ​`matchExpressions` ​中的一个元素， 即其 ​`key` ​字段是 “key”，​`operator` ​为 “In”，​`values` ​数组仅包含 “value”。 在 ​`matchLabels` ​和 ​`matchExpressions` ​中给出的所有条件都必须满足才能匹配。

*   ​`template` ​字段包含以下子字段：

*   Pod 被使用 ​`.metadata.labels`​ 字段打上 ​`app: nginx`​ 标签。
*   Pod 模板规约（即 ​`.template.spec`​ 字段）指示 Pods 运行一个 ​`nginx` ​容器， 该容器运行版本为 1.14.2 的 ​`nginx` [​Docker Hub](https://hub.docker.com/)镜像。
*   创建一个容器并使用 ​`.spec.template.spec.containers[0].name`​ 字段将其命名为 ​`nginx`​。

开始之前，请确保的 Kubernetes 集群已启动并运行。 按照以下步骤创建上述 Deployment ：

1.  通过运行以下命令创建 Deployment ：

`kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml`

3.  运行 ​`kubectl get deployments`​ 检查 Deployment 是否已创建。 如果仍在创建 Deployment，则输出类似于：

`NAME               READY   UP-TO-DATE   AVAILABLE   AGE nginx-deployment   0/3     0            0           1s`

在检查集群中的 Deployment 时，所显示的字段有：

*   ​`NAME` ​列出了集群中 Deployment 的名称。
*   ​`READY` ​显示应用程序的可用的“副本”数。显示的模式是“就绪个数/期望个数”。
*   ​`UP-TO-DATE`​ 显示为了达到期望状态已经更新的副本数。
*   ​`AVAILABLE` ​显示应用可供用户使用的副本数。
*   ​`AGE` ​显示应用程序运行的时间。

请注意期望副本数是根据 ​`.spec.replicas`​ 字段设置 3。

8.  要查看 Deployment 上线状态，运行 kubectl rollout status deployment/nginx-deployment。

输出类似于：

`Waiting for rollout to finish: 2 out of 3 new replicas have been updated... deployment "nginx-deployment" successfully rolled out`

11.  几秒钟后再次运行 ​`kubectl get deployments`​。输出类似于：

`NAME               READY   UP-TO-DATE   AVAILABLE   AGE nginx-deployment   3/3     3            3           18s`

注意 Deployment 已创建全部三个副本，并且所有副本都是最新的（它们包含最新的 Pod 模板） 并且可用。

14.  要查看 Deployment 创建的 ReplicaSet（​`rs`​），运行 ​`kubectl get rs`​。 输出类似于：

`NAME                          DESIRED   CURRENT   READY   AGE nginx-deployment-75675f5897   3         3         3       18s`

ReplicaSet 输出中包含以下字段：

*   ​`NAME` ​列出名字空间中 ReplicaSet 的名称；
*   ​`DESIRED` ​显示应用的期望副本个数，即在创建 Deployment 时所定义的值。 此为期望状态；
*   ​`CURRENT` ​显示当前运行状态中的副本个数；
*   ​`READY` ​显示应用中有多少副本可以为用户提供服务；
*   ​`AGE` ​显示应用已经运行的时间长度。

注意 ReplicaSet 的名称始终被格式化为​`[Deployment名称]-[随机字符串]`​。 其中的随机字符串是使用 ​`pod-template-hash`​ 作为种子随机生成的。

19.  要查看每个 Pod 自动生成的标签，运行 ​`kubectl get pods --show-labels`​。返回以下输出：

`NAME                                READY     STATUS    RESTARTS   AGE       LABELS nginx-deployment-75675f5897-7ci7o   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453 nginx-deployment-75675f5897-kzszj   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453 nginx-deployment-75675f5897-qqcnn   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453`

所创建的 ReplicaSet 确保总是存在三个 ​`nginx` ​Pod。

> 你必须在 Deployment 中指定适当的选择算符和 Pod 模板标签（在本例中为 ​`app: nginx`​）。 标签或者选择算符不要与其他控制器（包括其他 Deployment 和 StatefulSet）重叠。 Kubernetes 不会阻止你这样做，但是如果多个控制器具有重叠的选择算符， 它们可能会发生冲突执行难以预料的操作。

### Pod-template-hash 标签 

> 不要更改此标签。

Deployment 控制器将 ​`pod-template-hash`​ 标签添加到 Deployment 所创建或收留的每个 ReplicaSet 。

此标签可确保 Deployment 的子 ReplicaSets 不重叠。 标签是通过对 ReplicaSet 的 ​`PodTemplate` ​进行哈希处理。 所生成的哈希值被添加到 ReplicaSet 选择算符、Pod 模板标签，并存在于在 ReplicaSet 可能拥有的任何现有 Pod 中。

更新 Deployment 
--------------

> 仅当 Deployment Pod 模板（即 ​`.spec.template`​）发生改变时，例如模板的标签或容器镜像被更新， 才会触发 Deployment 上线。其他更新（如对 Deployment 执行扩缩容的操作）不会触发上线动作。

按照以下步骤更新 Deployment：

1.  先来更新 nginx Pod 以使用 ​`nginx:1.16.1`​ 镜像，而不是 ​`nginx:1.14.2`​ 镜像。

`kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1`

或者使用下面的命令：

`kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1`

输出类似于：

`deployment/nginx-deployment image updated`

或者，可以对 Deployment 执行 ​`edit` ​操作并将 ​`.spec.template.spec.containers[0].image`​ 从 ​`nginx:1.14.2`​ 更改至 ​`nginx:1.16.1`​。

`kubectl edit deployment/nginx-deployment`

输出类似于：

`deployment/nginx-deployment edited`

11.  要查看上线状态，运行：

`kubectl rollout status deployment/nginx-deployment`

输出类似于：

`Waiting for rollout to finish: 2 out of 3 new replicas have been updated...`

或者

`deployment "nginx-deployment" successfully rolled out`

获取关于已更新的 Deployment 的更多信息：

*   在上线成功后，可以通过运行 ​`kubectl get deployments`​ 来查看 Deployment： 输出类似于：

`NAME               READY   UP-TO-DATE   AVAILABLE   AGE nginx-deployment   3/3     3            3           36s`

*   运行 ​`kubectl get rs`​ 以查看 Deployment 通过创建新的 ReplicaSet 并将其扩容到 3 个副本并将旧 ReplicaSet 缩容到 0 个副本完成了 Pod 的更新操作：

`kubectl get rs`

输出类似于：

`NAME                          DESIRED   CURRENT   READY   AGE nginx-deployment-1564180365   3         3         3       6s nginx-deployment-2035384211   0         0         0       36s`

*   现在运行 ​`get pods`​ 应仅显示新的 Pods:

`kubectl get pods`

输出类似于：

`NAME                                READY     STATUS    RESTARTS   AGE nginx-deployment-1564180365-khku8   1/1       Running   0          14s nginx-deployment-1564180365-nacti   1/1       Running   0          14s nginx-deployment-1564180365-z9gth   1/1       Running   0          14s`

下次要更新这些 Pods 时，只需再次更新 Deployment Pod 模板即可。

Deployment 可确保在更新时仅关闭一定数量的 Pod。默认情况下，它确保至少所需 Pods 75% 处于运行状态（最大不可用比例为 25%）。

Deployment 还确保仅所创建 Pod 数量只可能比期望 Pods 数高一点点。 默认情况下，它可确保启动的 Pod 个数比期望个数最多多出 25%（最大峰值 25%）。

例如，如果仔细查看上述 Deployment ，将看到它首先创建了一个新的 Pod，然后删除了一些旧的 Pods， 并创建了新的 Pods。它不会杀死老 Pods，直到有足够的数量新的 Pods 已经出现。 在足够数量的旧 Pods 被杀死前并没有创建新 Pods。它确保至少 2 个 Pod 可用， 同时最多总共 4 个 Pod 可用。 当 Deployment 设置为 4 个副本时，Pod 的个数会介于 3 和 5 之间。

*   获取 Deployment 的更多信息

`kubectl describe deployments`

输出类似于：

`Name:                   nginx-deployment Namespace:              default CreationTimestamp:      Thu, 30 Nov 2017 10:56:25 +0000 Labels:                 app=nginx Annotations:            deployment.kubernetes.io/revision=2 Selector:               app=nginx Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType:           RollingUpdate MinReadySeconds:        0 RollingUpdateStrategy:  25% max unavailable, 25% max surge Pod Template:   Labels:  app=nginx    Containers:     nginx:       Image:        nginx:1.16.1       Port:         80/TCP       Environment:  <none>       Mounts:       <none>     Volumes:        <none>   Conditions:     Type           Status  Reason     ----           ------  ------     Available      True    MinimumReplicasAvailable     Progressing    True    NewReplicaSetAvailable   OldReplicaSets:  <none>   NewReplicaSet:   nginx-deployment-1564180365 (3/3 replicas created)   Events:     Type    Reason             Age   From                   Message     ----    ------             ----  ----                   -------     Normal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set nginx-deployment-2035384211 to 3     Normal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 1     Normal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 2     Normal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 2     Normal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 1     Normal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 3     Normal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 0`

可以看到，当第一次创建 Deployment 时，它创建了一个 ReplicaSet（​`nginx-deployment-2035384211`​） 并将其直接扩容至 3 个副本。更新 Deployment 时，它创建了一个新的 ReplicaSet （nginx-deployment-1564180365），并将其扩容为 1，等待其就绪；然后将旧 ReplicaSet 缩容到 2， 将新的 ReplicaSet 扩容到 2 以便至少有 3 个 Pod 可用且最多创建 4 个 Pod。 然后，它使用相同的滚动更新策略继续对新的 ReplicaSet 扩容并对旧的 ReplicaSet 缩容。 最后，你将有 3 个可用的副本在新的 ReplicaSet 中，旧 ReplicaSet 将缩容到 0。

> Kubernetes 在计算 ​`availableReplicas` ​数值时不考虑终止过程中的 Pod， ​`availableReplicas` ​的值一定介于 ​`replicas - maxUnavailable`​ 和 ​`replicas + maxSurge`​ 之间。 因此，你可能在上线期间看到 Pod 个数比预期的多，Deployment 所消耗的总的资源也大于 ​`replicas + maxSurge`​ 个 Pod 所用的资源，直到被终止的 Pod 所设置的 ​`terminationGracePeriodSeconds` ​到期为止。

### 翻转（多 Deployment 动态更新）

Deployment 控制器每次注意到新的 Deployment 时，都会创建一个 ReplicaSet 以启动所需的 Pods。 如果更新了 Deployment，则控制标签匹配 ​`.spec.selector`​ 但模板不匹配 ​`.spec.template`​ 的 Pods 的现有 ReplicaSet 被缩容。最终，新的 ReplicaSet 缩放为 ​`.spec.replicas`​ 个副本， 所有旧 ReplicaSets 缩放为 0 个副本。

当 Deployment 正在上线时被更新，Deployment 会针对更新创建一个新的 ReplicaSet 并开始对其扩容，之前正在被扩容的 ReplicaSet 会被翻转，添加到旧 ReplicaSets 列表 并开始缩容。

例如，假定你在创建一个 Deployment 以生成 ​`nginx:1.14.2`​ 的 5 个副本，但接下来 更新 Deployment 以创建 5 个 ​`nginx:1.16.1`​ 的副本，而此时只有 3 个​`nginx:1.14.2`​ 副本已创建。在这种情况下，Deployment 会立即开始杀死 3 个 ​`nginx:1.14.2`​ Pods， 并开始创建 ​`nginx:1.16.1`​ Pods。它不会等待 ​`nginx:1.14.2`​ 的 5 个副本都创建完成后才开始执行变更动作。

### 更改标签选择算符 

通常不鼓励更新标签选择算符。建议你提前规划选择算符。 在任何情况下，如果需要更新标签选择算符，请格外小心， 并确保自己了解这背后可能发生的所有事情。

> 在 API 版本 ​`apps/v1`​ 中，Deployment 标签选择算符在创建后是不可变的。

*   添加选择算符时要求使用新标签更新 Deployment 规约中的 Pod 模板标签，否则将返回验证错误。 此更改是非重叠的，也就是说新的选择算符不会选择使用旧选择算符所创建的 ReplicaSet 和 Pod， 这会导致创建新的 ReplicaSet 时所有旧 ReplicaSet 都会被孤立。
*   选择算符的更新如果更改了某个算符的键名，这会导致与添加算符时相同的行为。
*   删除选择算符的操作会删除从 Deployment 选择算符中删除现有算符。 此操作不需要更改 Pod 模板标签。现有 ReplicaSet 不会被孤立，也不会因此创建新的 ReplicaSet， 但请注意已删除的标签仍然存在于现有的 Pod 和 ReplicaSet 中。

回滚 Deployment
-------------

有时，你可能想要回滚 Deployment；例如，当 Deployment 不稳定时（例如进入反复崩溃状态）。 默认情况下，Deployment 的所有上线记录都保留在系统中，以便可以随时回滚 （你可以通过修改修订历史记录限制来更改这一约束）。

> Deployment 被触发上线时，系统就会创建 Deployment 的新的修订版本。 这意味着仅当 Deployment 的 Pod 模板（​`.spec.template`​）发生更改时，才会创建新修订版本 -- 例如，模板的标签或容器镜像发生变化。 其他更新，如 Deployment 的扩缩容操作不会创建 Deployment 修订版本。 这是为了方便同时执行手动缩放或自动缩放。 换言之，当你回滚到较早的修订版本时，只有 Deployment 的 Pod 模板部分会被回滚。

*   假设你在更新 Deployment 时犯了一个拼写错误，将镜像名称命名设置为 ​`nginx:1.161`​ 而不是 ​`nginx:1.16.1`​：

`kubectl set image deployment/nginx-deployment nginx=nginx:1.161 --record=true`

输出类似于：

`deployment/nginx-deployment image updated`

*   此上线进程会出现停滞。你可以通过检查上线状态来验证：

`kubectl rollout status deployment/nginx-deployment`

输出类似于：

`Waiting for rollout to finish: 1 out of 3 new replicas have been updated...`

*   按 Ctrl-C 停止上述上线状态观测。
*   你可以看到旧的副本有两个（​`nginx-deployment-1564180365`​ 和 ​`nginx-deployment-2035384211`​）， 新的副本有 1 个（​`nginx-deployment-3066724191`​）：

`kubectl get rs`

输出类似于：

`NAME                          DESIRED   CURRENT   READY   AGE nginx-deployment-1564180365   3         3         3       25s nginx-deployment-2035384211   0         0         0       36s nginx-deployment-3066724191   1         1         0       6s`

*   查看所创建的 Pod，你会注意到新 ReplicaSet 所创建的 1 个 Pod 卡顿在镜像拉取循环中。

`kubectl get pods`

输出类似于：

`NAME                                READY     STATUS             RESTARTS   AGE nginx-deployment-1564180365-70iae   1/1       Running            0          25s nginx-deployment-1564180365-jbqqo   1/1       Running            0          25s nginx-deployment-1564180365-hysrc   1/1       Running            0          25s nginx-deployment-3066724191-08mng   0/1       ImagePullBackOff   0          6s`

> Deployment 控制器自动停止有问题的上线过程，并停止对新的 ReplicaSet 扩容。 这行为取决于所指定的 rollingUpdate 参数（具体为 ​`maxUnavailable`​）。 默认情况下，Kubernetes 将此值设置为 25%。

*   获取 Deployment 描述信息：

`kubectl describe deployment`

输出类似于：

`Name:           nginx-deployment Namespace:      default CreationTimestamp:  Tue, 15 Mar 2016 14:48:04 -0700 Labels:         app=nginx Selector:       app=nginx Replicas:       3 desired | 1 updated | 4 total | 3 available | 1 unavailable StrategyType:       RollingUpdate MinReadySeconds:    0 RollingUpdateStrategy:  25% max unavailable, 25% max surge Pod Template:   Labels:  app=nginx   Containers:    nginx:     Image:        nginx:1.91     Port:         80/TCP     Host Port:    0/TCP     Environment:  <none>     Mounts:       <none>   Volumes:        <none> Conditions:   Type           Status  Reason   ----           ------  ------   Available      True    MinimumReplicasAvailable   Progressing    True    ReplicaSetUpdated OldReplicaSets:     nginx-deployment-1564180365 (3/3 replicas created) NewReplicaSet:      nginx-deployment-3066724191 (1/1 replicas created) Events:   FirstSeen LastSeen    Count   From                    SubobjectPath   Type        Reason              Message   --------- --------    -----   ----                    -------------   --------    ------              -------   1m        1m          1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-2035384211 to 3   22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 1   22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 2   22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 2   21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 1   21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 3   13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 0   13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-3066724191 to 1`

要解决此问题，需要回滚到以前稳定的 Deployment 版本。

### 检查 Deployment 上线历史 

按照如下步骤检查回滚历史：

1.  首先，检查 Deployment 修订历史：

`kubectl rollout history deployment/nginx-deployment`

输出类似于：

`deployments "nginx-deployment" REVISION    CHANGE-CAUSE 1           kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml 2           kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1 3           kubectl set image deployment/nginx-deployment nginx=nginx:1.161`

​`CHANGE-CAUSE`​ 的内容是从 Deployment 的 ​`kubernetes.io/change-cause`​ 注解复制过来的。 复制动作发生在修订版本创建时。你可以通过以下方式设置 ​`CHANGE-CAUSE`​ 消息：

*   使用 ​`kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause="image updated to 1.16.1"`​ 为 Deployment 添加注解。
*   手动编辑资源的清单。

7.  要查看修订历史的详细信息，运行：

`kubectl rollout history deployment/nginx-deployment --revision=2`

输出类似于：

`deployments "nginx-deployment" revision 2   Labels:       app=nginx           pod-template-hash=1159050644   Annotations:  kubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1   Containers:    nginx:     Image:      nginx:1.16.1     Port:       80/TCP      QoS Tier:         cpu:      BestEffort         memory:   BestEffort     Environment Variables:      <none>   No volumes.`

### 回滚到之前的修订版本 

按照下面给出的步骤将 Deployment 从当前版本回滚到以前的版本（即版本 2）。

1.  假定现在你已决定撤消当前上线并回滚到以前的修订版本：

`kubectl rollout undo deployment/nginx-deployment`

输出类似于：

`deployment.apps/nginx-deployment`

或者，你也可以通过使用 --to-revision 来回滚到特定修订版本：

`kubectl rollout undo deployment/nginx-deployment --to-revision=2`

输出类似于：

`deployment.apps/nginx-deployment`

与回滚相关的指令的更详细信息，请参考 [kubectl rollout](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands target=)。

现在，Deployment 正在回滚到以前的稳定版本。正如你所看到的，Deployment 控制器生成了回滚到修订版本 2 的 ​`DeploymentRollback` ​事件。

11.  检查回滚是否成功以及 Deployment 是否正在运行，运行：

`kubectl get deployment nginx-deployment`

输出类似于：

`NAME               READY   UP-TO-DATE   AVAILABLE   AGE nginx-deployment   3/3     3            3           30m`

15.  获取 Deployment 描述信息：

`kubectl describe deployment nginx-deployment`

输出类似于：

`Name:                   nginx-deployment Namespace:              default CreationTimestamp:      Sun, 02 Sep 2018 18:17:55 -0500 Labels:                 app=nginx Annotations:            deployment.kubernetes.io/revision=4                         kubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1 Selector:               app=nginx Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType:           RollingUpdate MinReadySeconds:        0 RollingUpdateStrategy:  25% max unavailable, 25% max surge Pod Template:   Labels:  app=nginx   Containers:    nginx:     Image:        nginx:1.16.1     Port:         80/TCP     Host Port:    0/TCP     Environment:  <none>     Mounts:       <none>   Volumes:        <none> Conditions:   Type           Status  Reason   ----           ------  ------   Available      True    MinimumReplicasAvailable   Progressing    True    NewReplicaSetAvailable OldReplicaSets:  <none> NewReplicaSet:   nginx-deployment-c4747d96c (3/3 replicas created) Events:   Type    Reason              Age   From                   Message   ----    ------              ----  ----                   -------   Normal  ScalingReplicaSet   12m   deployment-controller  Scaled up replica set nginx-deployment-75675f5897 to 3   Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 1   Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 2   Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 2   Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 1   Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 3   Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 0   Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-595696685f to 1   Normal  DeploymentRollback  15s   deployment-controller  Rolled back deployment "nginx-deployment" to revision 2   Normal  ScalingReplicaSet   15s   deployment-controller  Scaled down replica set nginx-deployment-595696685f to 0`

缩放 Deployment 
--------------

你可以使用如下指令缩放 Deployment：

`kubectl scale deployment/nginx-deployment --replicas=10`

输出类似于：

`deployment.apps/nginx-deployment scaled`

假设集群启用了Pod 的水平自动缩放， 你可以为 Deployment 设置自动缩放器，并基于现有 Pod 的 CPU 利用率选择要运行的 Pod 个数下限和上限。

`kubectl autoscale deployment/nginx-deployment --min=10 --max=15 --cpu-percent=80`

输出类似于：

`deployment.apps/nginx-deployment scaled`

### 比例缩放 

RollingUpdate 的 Deployment 支持同时运行应用程序的多个版本。 当自动缩放器缩放处于上线进程（仍在进行中或暂停）中的 RollingUpdate Deployment 时， Deployment 控制器会平衡现有的活跃状态的 ReplicaSets（含 Pods 的 ReplicaSets）中的额外副本， 以降低风险。这称为 比例缩放（Proportional Scaling）。

例如，你正在运行一个 10 个副本的 Deployment，其 maxSurge=3，maxUnavailable=2。

*   确保 Deployment 的这 10 个副本都在运行。

`kubectl get deploy`

输出类似于：

`NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE nginx-deployment     10        10        10           10          50s`

*   更新 Deployment 使用新镜像，碰巧该镜像无法从集群内部解析。

`kubectl set image deployment/nginx-deployment nginx=nginx:sometag`

输出类似于：

`deployment.apps/nginx-deployment image updated`

*   镜像更新使用 ReplicaSet ​`nginx-deployment-1989198191`​ 启动新的上线过程， 但由于上面提到的 ​`maxUnavailable` ​要求，该进程被阻塞了。检查上线状态：

`kubectl get rs`

输出类似于：

`NAME                          DESIRED   CURRENT   READY     AGE nginx-deployment-1989198191   5         5         0         9s nginx-deployment-618515232    8         8         8         1m`

*   然后，出现了新的 Deployment 扩缩请求。自动缩放器将 Deployment 副本增加到 15。 Deployment 控制器需要决定在何处添加 5 个新副本。如果未使用比例缩放，所有 5 个副本 都将添加到新的 ReplicaSet 中。使用比例缩放时，可以将额外的副本分布到所有 ReplicaSet。 较大比例的副本会被添加到拥有最多副本的 ReplicaSet，而较低比例的副本会进入到 副本较少的 ReplicaSet。所有剩下的副本都会添加到副本最多的 ReplicaSet。 具有零副本的 ReplicaSets 不会被扩容。

在上面的示例中，3 个副本被添加到旧 ReplicaSet 中，2 个副本被添加到新 ReplicaSet。 假定新的副本都很健康，上线过程最终应将所有副本迁移到新的 ReplicaSet 中。 要确认这一点，请运行：

`kubectl get deploy`

输出类似于：

`NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE nginx-deployment     15        18        7            8           7m`

上线状态确认了副本是如何被添加到每个 ReplicaSet 的。

`kubectl get rs`

输出类似于：

`NAME                          DESIRED   CURRENT   READY     AGE nginx-deployment-1989198191   7         7         0         7m nginx-deployment-618515232    11        11        11        7m`

暂停、恢复 Deployment 的上线过程 
-----------------------

在你更新一个 Deployment 的时候，或者计划更新它的时候， 你可以在触发一个或多个更新之前暂停 Deployment 的上线过程。 当你准备行应用这些变更时，你可以重新恢复 Deployment 上线过程。 这样做使得你能够在暂停和恢复执行之间应用多个修补程序，而不会触发不必要的上线操作。

*   例如，对于一个刚刚创建的 Deployment：

获取该 Deployment 信息：

`kubectl get deploy`

输出类似于：

`NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE nginx     3         3         3            3           1m`

获取上线状态：

`kubectl get rs`

输出类似于：

`NAME               DESIRED   CURRENT   READY     AGE nginx-2142116321   3         3         3         1m`

*   使用如下指令暂停上线：

`kubectl rollout pause deployment/nginx-deployment`

输出类似于：

`deployment.apps/nginx-deployment paused`

*   接下来更新 Deployment 镜像：

`kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1`

输出类似于：

`deployment.apps/nginx-deployment image updated`

*   注意没有新的上线被触发：

`kubectl rollout history deployment/nginx-deployment`

输出类似于：

`deployments "nginx" REVISION  CHANGE-CAUSE 1   <none>`

*   获取上线状态验证现有的 ReplicaSet 没有被更改：

`kubectl get rs`

输出类似于：

`NAME               DESIRED   CURRENT   READY     AGE nginx-2142116321   3         3         3         2m`

*   你可以根据需要执行很多更新操作，例如，可以要使用的资源：

`kubectl set resources deployment/nginx-deployment -c=nginx --limits=cpu=200m,memory=512Mi`

输出类似于：

`deployment.apps/nginx-deployment resource requirements updated`

暂停 Deployment 上线之前的初始状态将继续发挥作用，但新的更新在 Deployment 上线被暂停期间不会产生任何效果。

*   最终，恢复 Deployment 上线并观察新的 ReplicaSet 的创建过程，其中包含了所应用的所有更新：

`kubectl rollout resume deployment/nginx-deployment`

输出类似于这样：

`deployment.apps/nginx-deployment resumed`

*   观察上线的状态，直到完成。

`kubectl get rs -w`

输出类似于：

`NAME               DESIRED   CURRENT   READY     AGE nginx-2142116321   2         2         2         2m nginx-3926361531   2         2         0         6s nginx-3926361531   2         2         1         18s nginx-2142116321   1         2         2         2m nginx-2142116321   1         2         2         2m nginx-3926361531   3         2         1         18s nginx-3926361531   3         2         1         18s nginx-2142116321   1         1         1         2m nginx-3926361531   3         3         1         18s nginx-3926361531   3         3         2         19s nginx-2142116321   0         1         1         2m nginx-2142116321   0         1         1         2m nginx-2142116321   0         0         0         2m nginx-3926361531   3         3         3         20s`

*   获取最近上线的状态：

`kubectl get rs`

输出类似于：

`NAME               DESIRED   CURRENT   READY     AGE nginx-2142116321   0         0         0         2m nginx-3926361531   3         3         3         28s`

> 你不可以回滚处于暂停状态的 Deployment，除非先恢复其执行状态。

Deployment 状态
-------------

Deployment 的生命周期中会有许多状态。上线新的 ReplicaSet 期间可能处于 Progressing（进行中），可能是 Complete（已完成），也可能是 Failed（失败）以至于无法继续进行。

### 进行中的 Deployment 

执行下面的任务期间，Kubernetes 标记 Deployment 为 进行中（Progressing）：

*   Deployment 创建新的 ReplicaSet
*   Deployment 正在为其最新的 ReplicaSet 扩容
*   Deployment 正在为其旧有的 ReplicaSet(s) 缩容
*   新的 Pods 已经就绪或者可用（就绪至少持续了 MinReadySeconds 秒）。

当上线过程进入“Progressing”状态时，Deployment 控制器会向 Deployment 的​ `.status.conditions`​ 中添加包含下面属性的状况条目：

*   ​`type: Progressing` ​
*   ​`status: "True"` ​
*   ​`reason: NewReplicaSetCreated`​ | ​`reason: FoundNewReplicaSet`​ | ​`reason: ReplicaSetUpdated`​

你可以使用 ​`kubectl rollout status`​ 监视 Deployment 的进度。

### 完成的 Deployment 

当 Deployment 具有以下特征时，Kubernetes 将其标记为 完成（Complete）：

*   与 Deployment 关联的所有副本都已更新到指定的最新版本，这意味着之前请求的所有更新都已完成。
*   与 Deployment 关联的所有副本都可用。
*   未运行 Deployment 的旧副本。

当上线过程进入“Complete”状态时，Deployment 控制器会向 Deployment 的 ​`.status.conditions`​ 中添加包含下面属性的状况条目：

*   ​`type: Progressing` ​
*   ​`status: "True"` ​
*   ​`reason: NewReplicaSetAvailable`​

这一 ​`Progressing` ​状况的状态值会持续为 ​`"True"`​，直至新的上线动作被触发。 即使副本的可用状态发生变化（进而影响 ​`Available` ​状况），​`Progressing` ​状况的值也不会变化。

你可以使用 ​`kubectl rollout status`​ 检查 Deployment 是否已完成。 如果上线成功完成，​`kubectl rollout status`​ 返回退出代码 0。

`kubectl rollout status deployment/nginx-deployment`

输出类似于：

`Waiting for rollout to finish: 2 of 3 updated replicas are available... deployment "nginx-deployment" successfully rolled out`

从 ​`kubectl rollout`​ 命令获得的返回状态为 0（成功）：

`$ echo $?`

`0`

### 失败的 Deployment 

你的 Deployment 可能会在尝试部署其最新的 ReplicaSet 受挫，一直处于未完成状态。 造成此情况一些可能因素如下：

*   配额（Quota）不足
*   就绪探测（Readiness Probe）失败
*   镜像拉取错误
*   权限不足
*   限制范围（Limit Ranges）问题
*   应用程序运行时的配置错误

检测此状况的一种方法是在 Deployment 规约中指定截止时间参数： （\[​`.spec.progressDeadlineSeconds`​\]（#progress-deadline-seconds））。 ​`.spec.progressDeadlineSeconds`​ 给出的是一个秒数值，Deployment 控制器在（通过 Deployment 状态） 标示 Deployment 进展停滞之前，需要等待所给的时长。

以下 ​`kubectl` ​命令设置规约中的 ​`progressDeadlineSeconds`​，从而告知控制器 在 10 分钟后报告 Deployment 没有进展：

`kubectl patch deployment/nginx-deployment -p '{"spec":{"progressDeadlineSeconds":600}}'`

输出类似于：

`deployment.apps/nginx-deployment patched`

超过截止时间后，Deployment 控制器将添加具有以下属性的 Deployment 状况到 Deployment 的 ​`.status.conditions`​ 中：

*   Type=Progressing
*   Status=False
*   Reason=ProgressDeadlineExceeded

这一状况也可能会比较早地失败，因而其状态值被设置为 ​`"False"`​， 其原因为 ​`ReplicaSetCreateError`​。 一旦 Deployment 上线完成，就不再考虑其期限。

参考 [Kubernetes API Conventions](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md target=) 获取更多状态状况相关的信息。

> 除了报告 ​`Reason=ProgressDeadlineExceeded`​ 状态之外，Kubernetes 对已停止的 Deployment 不执行任何操作。更高级别的编排器可以利用这一设计并相应地采取行动。 例如，将 Deployment 回滚到其以前的版本。

> 如果你暂停了某个 Deployment 上线，Kubernetes 不再根据指定的截止时间检查 Deployment 进展。 你可以在上线过程中间安全地暂停 Deployment 再恢复其执行，这样做不会导致超出最后时限的问题。

Deployment 可能会出现瞬时性的错误，可能因为设置的超时时间过短， 也可能因为其他可认为是临时性的问题。例如，假定所遇到的问题是配额不足。 如果描述 Deployment，你将会注意到以下部分：

`kubectl describe deployment nginx-deployment`

输出类似于：

`<...> Conditions:   Type            Status  Reason   ----            ------  ------   Available       True    MinimumReplicasAvailable   Progressing     True    ReplicaSetUpdated   ReplicaFailure  True    FailedCreate <...>`

如果运行 ​`kubectl get deployment nginx-deployment -o yaml`​，Deployment 状态输出 将类似于这样：

`status:   availableReplicas: 2   conditions:   - lastTransitionTime: 2016-10-04T12:25:39Z     lastUpdateTime: 2016-10-04T12:25:39Z     message: Replica set "nginx-deployment-4262182780" is progressing.     reason: ReplicaSetUpdated     status: "True"     type: Progressing   - lastTransitionTime: 2016-10-04T12:25:42Z     lastUpdateTime: 2016-10-04T12:25:42Z     message: Deployment has minimum availability.     reason: MinimumReplicasAvailable     status: "True"     type: Available   - lastTransitionTime: 2016-10-04T12:25:39Z     lastUpdateTime: 2016-10-04T12:25:39Z     message: 'Error creating: pods "nginx-deployment-4262182780-" is forbidden: exceeded quota:       object-counts, requested: pods=1, used: pods=3, limited: pods=2'     reason: FailedCreate     status: "True"     type: ReplicaFailure   observedGeneration: 3   replicas: 2   unavailableReplicas: 2`

最终，一旦超过 Deployment 进度限期，Kubernetes 将更新状态和进度状况的原因：

`Conditions:   Type            Status  Reason   ----            ------  ------   Available       True    MinimumReplicasAvailable   Progressing     False   ProgressDeadlineExceeded   ReplicaFailure  True    FailedCreate`

可以通过缩容 Deployment 或者缩容其他运行状态的控制器，或者直接在命名空间中增加配额 来解决配额不足的问题。如果配额条件满足，Deployment 控制器完成了 Deployment 上线操作， Deployment 状态会更新为成功状况（​`Status=True`​ and ​`Reason=NewReplicaSetAvailable`​）。

`Conditions:   Type          Status  Reason   ----          ------  ------   Available     True    MinimumReplicasAvailable   Progressing   True    NewReplicaSetAvailable`

    

​`type: Available`​ 加上 ​`status: True`​ 意味着 Deployment 具有最低可用性。 最低可用性由 Deployment 策略中的参数指定。 ​`type: Progressing`​ 加上 ​`status: True`​ 表示 Deployment 处于上线过程中，并且正在运行， 或者已成功完成进度，最小所需新副本处于可用。 请参阅对应状况的 Reason 了解相关细节。 在我们的案例中 ​`reason: NewReplicaSetAvailable`​ 表示 Deployment 已完成。

你可以使用 ​`kubectl rollout status`​ 检查 Deployment 是否未能取得进展。 如果 Deployment 已超过进度限期，​`kubectl rollout status`​ 返回非零退出代码。

`kubectl rollout status deployment/nginx-deployment`

输出类似于：

`Waiting for rollout to finish: 2 out of 3 new replicas have been updated... error: deployment "nginx" exceeded its progress deadline`

​`kubectl rollout`​ 命令的退出状态为 1（表明发生了错误）：

`$ echo $?`

`1`

### 对失败 Deployment 的操作 

可应用于已完成的 Deployment 的所有操作也适用于失败的 Deployment。 你可以对其执行扩缩容、回滚到以前的修订版本等操作，或者在需要对 Deployment 的 Pod 模板应用多项调整时，将 Deployment 暂停。

清理策略
----

你可以在 Deployment 中设置 ​`.spec.revisionHistoryLimit`​ 字段以指定保留此 Deployment 的多少个旧有 ReplicaSet。其余的 ReplicaSet 将在后台被垃圾回收。 默认情况下，此值为 10。

> 显式将此字段设置为 0 将导致 Deployment 的所有历史记录被清空，因此 Deployment 将无法回滚。

金丝雀部署 
------

如果要使用 Deployment 向用户子集或服务器子集上线版本， 则可以遵循资源管理 所描述的金丝雀模式，创建多个 Deployment，每个版本一个。

编写 Deployment 规约 
-----------------

同其他 Kubernetes 配置一样， Deployment 需要 ​`apiVersion`​，​`kind` ​和 ​`metadata` ​字段。

Deployment 对象的名称必须是合法的 DNS 子域名。 Deployment 还需要 ​

##  2.  Kubernetes ReplicaSet
ReplicaSet
----------

ReplicaSet 的目的是维护一组在任何时候都处于运行状态的 Pod 副本的稳定集合。 因此，它通常用来保证给定数量的、完全相同的 Pod 的可用性。

ReplicaSet 的工作原理
----------------

ReplicaSet 是通过一组字段来定义的，包括一个用来识别可获得的 Pod 的集合的选择算符、一个用来标明应该维护的副本个数的数值、一个用来指定应该创建新 Pod 以满足副本个数条件时要使用的 Pod 模板等等。 每个 ReplicaSet 都通过根据需要创建和 删除 Pod 以使得副本个数达到期望值， 进而实现其存在价值。当 ReplicaSet 需要创建新的 Pod 时，会使用所提供的 Pod 模板。

ReplicaSet 通过 Pod 上的 metadata.ownerReferences 字段连接到附属 Pod，该字段给出当前对象的属主资源。 ReplicaSet 所获得的 Pod 都在其 ownerReferences 字段中包含了属主 ReplicaSet 的标识信息。正是通过这一连接，ReplicaSet 知道它所维护的 Pod 集合的状态， 并据此计划其操作行为。

ReplicaSet 使用其选择算符来辨识要获得的 Pod 集合。如果某个 Pod 没有 OwnerReference 或者其 OwnerReference 不是一个 控制器，且其匹配到 某 ReplicaSet 的选择算符，则该 Pod 立即被此 ReplicaSet 获得。

何时使用 ReplicaSet
---------------

ReplicaSet 确保任何时间都有指定数量的 Pod 副本在运行。 然而，Deployment 是一个更高级的概念，它管理 ReplicaSet，并向 Pod 提供声明式的更新以及许多其他有用的功能。 因此，我们建议使用 Deployment 而不是直接使用 ReplicaSet，除非 你需要自定义更新业务流程或根本不需要更新。

这实际上意味着，你可能永远不需要操作 ReplicaSet 对象：而是使用 Deployment，并在 spec 部分定义你的应用。

示例
--

`apiVersion: apps/v1 kind: ReplicaSet metadata:   name: frontend   labels:     app: guestbook     tier: frontend spec:   # modify replicas according to your case   replicas: 3   selector:     matchLabels:       tier: frontend   template:     metadata:       labels:         tier: frontend     spec:       containers:       - name: php-redis         image: gcr.io/google_samples/gb-frontend:v3`

将此清单保存到 ​`frontend.yaml`​ 中，并将其提交到 Kubernetes 集群， 应该就能创建 yaml 文件所定义的 ReplicaSet 及其管理的 Pod。

`kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml`

你可以看到当前被部署的 ReplicaSet：

`kubectl get rs`

并看到你所创建的前端：

`NAME       DESIRED   CURRENT   READY   AGE frontend   3         3         3       6s`

你也可以查看 ReplicaSet 的状态：

`kubectl describe rs/frontend`

你会看到类似如下的输出：

`Name:		frontend Namespace:	default Selector:	tier=frontend Labels:		app=guestbook 		tier=frontend Annotations:  kubectl.kubernetes.io/last-applied-configuration:                 {"apiVersion":"apps/v1","kind":"ReplicaSet","metadata":{"annotations":{},"labels":{"app":"guestbook","tier":"frontend"},"name":"frontend",... Replicas:	3 current / 3 desired Pods Status:	3 Running / 0 Waiting / 0 Succeeded / 0 Failed Pod Template:   Labels:       tier=frontend   Containers:    php-redis:     Image:      gcr.io/google_samples/gb-frontend:v3     Port:         <none>     Host Port:    <none>     Environment:  <none>     Mounts:             <none>   Volumes:              <none> Events:   Type    Reason            Age   From                   Message   ----    ------            ----  ----                   -------   Normal  SuccessfulCreate  117s  replicaset-controller  Created pod: frontend-wtsmm   Normal  SuccessfulCreate  116s  replicaset-controller  Created pod: frontend-b2zdv   Normal  SuccessfulCreate  116s  replicaset-controller  Created pod: frontend-vcmts`

最后可以查看启动了的 Pods：

`kubectl get pods`

你会看到类似如下的 Pod 信息：

`NAME             READY   STATUS    RESTARTS   AGE frontend-b2zdv   1/1     Running   0          6m36s frontend-vcmts   1/1     Running   0          6m36s frontend-wtsmm   1/1     Running   0          6m36s`

你也可以查看 Pods 的属主引用被设置为前端的 ReplicaSet。 要实现这点，可取回运行中的 Pods 之一的 YAML：

`kubectl get pods frontend-b2zdv -o yaml`

输出将类似这样，frontend ReplicaSet 的信息被设置在 metadata 的 ​`ownerReferences` ​字段中：

`apiVersion: v1 kind: Pod metadata:   creationTimestamp: "2020-02-12T07:06:16Z"   generateName: frontend-   labels:     tier: frontend   name: frontend-b2zdv   namespace: default   ownerReferences:   - apiVersion: apps/v1     blockOwnerDeletion: true     controller: true     kind: ReplicaSet     name: frontend     uid: f391f6db-bb9b-4c09-ae74-6a1f77f3d5cf ...`

非模板 Pod 的获得 
------------

尽管你完全可以直接创建裸的 Pods，强烈建议你确保这些裸的 Pods 并不包含可能与你 的某个 ReplicaSet 的选择算符相匹配的标签。原因在于 ReplicaSet 并不仅限于拥有 在其模板中设置的 Pods，它还可以像前面小节中所描述的那样获得其他 Pods。

`apiVersion: v1 kind: Pod metadata:   name: pod1   labels:     tier: frontend spec:   containers:   - name: hello1     image: gcr.io/google-samples/hello-app:2.0  ---  apiVersion: v1 kind: Pod metadata:   name: pod2   labels:     tier: frontend spec:   containers:   - name: hello2     image: gcr.io/google-samples/hello-app:1.0`

由于这些 Pod 没有控制器（Controller，或其他对象）作为其属主引用，并且 其标签与 frontend ReplicaSet 的选择算符匹配，它们会立即被该 ReplicaSet 获取。

假定你在 frontend ReplicaSet 已经被部署之后创建 Pods，并且你已经在 ReplicaSet 中设置了其初始的 Pod 副本数以满足其副本计数需要：

`kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml`

新的 Pods 会被该 ReplicaSet 获取，并立即被 ReplicaSet 终止，因为 它们的存在会使得 ReplicaSet 中 Pod 个数超出其期望值。

取回 Pods：

`kubectl get pods`

输出显示新的 Pods 或者已经被终止，或者处于终止过程中：

`NAME             READY   STATUS        RESTARTS   AGE frontend-b2zdv   1/1     Running       0          10m frontend-vcmts   1/1     Running       0          10m frontend-wtsmm   1/1     Running       0          10m pod1             0/1     Terminating   0          1s pod2             0/1     Terminating   0          1s`

如果你先行创建 Pods：

`kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml`

之后再创建 ReplicaSet：

`kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml`

你会看到 ReplicaSet 已经获得了该 Pods，并仅根据其规约创建新的 Pods，直到 新的 Pods 和原来的 Pods 的总数达到其预期个数。 这时取回 Pods：

`kubectl get pods`

将会生成下面的输出：

`NAME             READY   STATUS    RESTARTS   AGE frontend-hmmj2   1/1     Running   0          9s pod1             1/1     Running   0          36s pod2             1/1     Running   0          36s`

采用这种方式，一个 ReplicaSet 中可以包含异质的 Pods 集合。

编写 ReplicaSet 的 spec
--------------------

与所有其他 Kubernetes API 对象一样，ReplicaSet 也需要 ​`apiVersion`​、​`kind`​、和 ​`metadata` ​字段。 对于 ReplicaSets 而言，其 ​`kind` ​始终是 ReplicaSet。

ReplicaSet 对象的名称必须是合法的 DNS 子域名。

ReplicaSet 也需要 ​`.spec`​ 部分。

### Pod 模版

​`.spec.template`​ 是一个Pod 模版， 要求设置标签。在 ​`frontend.yaml`​ 示例中，我们指定了标签 ​`tier: frontend`​。 注意不要将标签与其他控制器的选择算符重叠，否则那些控制器会尝试收养此 Pod。

对于模板的重启策略 字段，​`.spec.template.spec.restartPolicy`​，唯一允许的取值是 ​`Always`​，这也是默认值.

### Pod 选择算符 

​`.spec.selector`​ 字段是一个标签选择算符。 如前文中所讨论的，这些是用来标识要被获取的 Pods 的标签。在签名的 ​`frontend.yaml`​ 示例中，选择算符为：

`matchLabels:   tier: frontend`

在 ReplicaSet 中，​`.spec.template.metadata.labels`​ 的值必须与 ​`spec.selector`​ 值 相匹配，否则该配置会被 API 拒绝。

> 对于设置了相同的 ​`.spec.selector`​，但 ​`.spec.template.metadata.labels`​ 和 ​`.spec.template.spec`​ 字段不同的 两个 ReplicaSet 而言，每个 ReplicaSet 都会忽略被另一个 ReplicaSet 所 创建的 Pods。

### Replicas

你可以通过设置 ​`.spec.replicas`​ 来指定要同时运行的 Pod 个数。 ReplicaSet 创建、删除 Pods 以与此值匹配。

如果你没有指定 ​`.spec.replicas`​, 那么默认值为 1。

使用 ReplicaSets
--------------

### 删除 ReplicaSet 和它的 Pod

要删除 ReplicaSet 和它的所有 Pod，使用 [kubectl delete](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands target=) 命令。 默认情况下，垃圾收集器 自动删除所有依赖的 Pod。

当使用 REST API 或 ​`client-go`​ 库时，你必须在删除选项中将 ​`propagationPolicy` ​设置为 ​`Background` ​或 ​`Foreground`​。例如：

`kubectl proxy --port=8080 curl -X DELETE  'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend' \    -d '{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Foreground"}' \    -H "Content-Type: application/json"`

### 只删除 ReplicaSet

你可以只删除 ReplicaSet 而不影响它的 Pods，方法是使用 [kubectl delete](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands target=) 命令并设置 ​`--cascade=orphan`​ 选项。

当使用 REST API 或 ​`client-go`​ 库时，你必须将 ​`propagationPolicy` ​设置为 ​`Orphan`​。 例如：

`kubectl proxy --port=8080 curl -X DELETE  'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend' \   -d '{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Orphan"}' \   -H "Content-Type: application/json"`

一旦删除了原来的 ReplicaSet，就可以创建一个新的来替换它。 由于新旧 ReplicaSet 的 ​`.spec.selector`​ 是相同的，新的 ReplicaSet 将接管老的 Pod。 但是，它不会努力使现有的 Pod 与新的、不同的 Pod 模板匹配。 若想要以可控的方式更新 Pod 的规约，可以使用 Deployment 资源，因为 ReplicaSet 并不直接支持滚动更新。

### 将 Pod 从 ReplicaSet 中隔离

可以通过改变标签来从 ReplicaSet 的目标集中移除 Pod。 这种技术可以用来从服务中去除 Pod，以便进行排错、数据恢复等。 以这种方式移除的 Pod 将被自动替换（假设副本的数量没有改变）。

### 缩放 RepliaSet 

通过更新 ​`.spec.replicas`​ 字段，ReplicaSet 可以被轻松的进行缩放。ReplicaSet 控制器能确保匹配标签选择器的数量的 Pod 是可用的和可操作的。

在降低集合规模时，ReplicaSet 控制器通过对可用的 Pods 进行排序来优先选择 要被删除的 Pods。其一般性算法如下：

1.  首先选择剔除悬决（Pending，且不可调度）的 Pods
2.  如果设置了 ​`controller.kubernetes.io/pod-deletion-cost`​ 注解，则注解值 较小的优先被裁减掉
3.  所处节点上副本个数较多的 Pod 优先于所处节点上副本较少者
4.  如果 Pod 的创建时间不同，最近创建的 Pod 优先于早前创建的 Pod 被裁减。 （当 ​`LogarithmicScaleDown` ​这一 特性门控 被启用时，创建时间是按整数幂级来分组的）。

如果以上比较结果都相同，则随机选择。

### Pod 删除开销 

FEATURE STATE: Kubernetes v1.22 \[beta\]

通过使用 ​`controller.kubernetes.io/pod-deletion-cost`​ 注解，用户可以对 ReplicaSet 缩容时要先删除哪些 Pods 设置偏好。

此注解要设置到 Pod 上，取值范围为 \[-2147483647, 2147483647\]。 所代表的的是删除同一 ReplicaSet 中其他 Pod 相比较而言的开销。 删除开销较小的 Pods 比删除开销较高的 Pods 更容易被删除。

Pods 如果未设置此注解，则隐含的设置值为 0。负值也是可接受的。 如果注解值非法，API 服务器会拒绝对应的 Pod。

此功能特性处于 Beta 阶段，默认被禁用。你可以通过为 kube-apiserver 和 kube-controller-manager 设置 特性门控 ​`PodDeletionCost` ​来启用此功能。

> 此机制实施时仅是尽力而为，并不能对 Pod 的删除顺序作出任何保证；  
> 用户应避免频繁更新注解值，例如根据某观测度量值来更新此注解值是应该避免的。 这样做会在 API 服务器上产生大量的 Pod 更新操作。

### 使用场景示例

同一应用的不同 Pods 可能其利用率是不同的。在对应用执行缩容操作时，可能 希望移除利用率较低的 Pods。为了避免频繁更新 Pods，应用应该在执行缩容 操作之前更新一次 ​`controller.kubernetes.io/pod-deletion-cost`​ 注解值 （将注解值设置为一个与其 Pod 利用率对应的值）。 如果应用自身控制器缩容操作时（例如 Spark 部署的驱动 Pod），这种机制 是可以起作用的。

### ReplicaSet 作为水平的 Pod 自动缩放器目标

ReplicaSet 也可以作为 水平的 Pod 缩放器 (HPA) 的目标。也就是说，ReplicaSet 可以被 HPA 自动缩放。 以下是 HPA 以我们在前一个示例中创建的副本集为目标的示例。

`apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata:   name: frontend-scaler spec:   scaleTargetRef:     kind: ReplicaSet     name: frontend   minReplicas: 3   maxReplicas: 10   targetCPUUtilizationPercentage: 50`

将这个列表保存到 ​`hpa-rs.yaml`​ 并提交到 Kubernetes 集群，就能创建它所定义的 HPA，进而就能根据复制的 Pod 的 CPU 利用率对目标 ReplicaSet进行自动缩放。

`kubectl apply -f https://k8s.io/examples/controllers/hpa-rs.yaml`

或者，可以使用 ​`kubectl autoscale`​ 命令完成相同的操作。 (而且它更简单！)

`kubectl autoscale rs frontend --max=10 --min=3 --cpu-percent=50`

ReplicaSet 的替代方案
----------------

### Deployment （推荐）

Deployment 是一个 可以拥有 ReplicaSet 并使用声明式方式在服务器端完成对 Pods 滚动更新的对象。 尽管 ReplicaSet 可以独立使用，目前它们的主要用途是提供给 Deployment 作为 编排 Pod 创建、删除和更新的一种机制。当使用 Deployment 时，你不必关心 如何管理它所创建的 ReplicaSet，Deployment 拥有并管理其 ReplicaSet。 因此，建议你在需要 ReplicaSet 时使用 Deployment。

### 裸 Pod

与用户直接创建 Pod 的情况不同，ReplicaSet 会替换那些由于某些原因被删除或被终止的 Pod，例如在节点故障或破坏性的节点维护（如内核升级）的情况下。 因为这个原因，我们建议你使用 ReplicaSet，即使应用程序只需要一个 Pod。 想像一下，ReplicaSet 类似于进程监视器，只不过它在多个节点上监视多个 Pod， 而不是在单个节点上监视单个进程。 ReplicaSet 将本地容器重启的任务委托给了节点上的某个代理（例如，Kubelet 或 Docker）去完成。

### Job

使用Job 代替ReplicaSet， 可以用于那些期望自行终止的 Pod。

### DaemonSet

对于管理那些提供主机级别功能（如主机监控和主机日志）的容器， 就要用 ​`DaemonSet` ​而不用 ReplicaSet。 这些 Pod 的寿命与主机寿命有关：这些 Pod 需要先于主机上的其他 Pod 运行， 并且在机器准备重新启动/关闭时安全地终止。

### ReplicationController

ReplicaSet 是 ReplicationController 的后继者。二者目的相同且行为类似，只是 ReplicationController 不支持 标签用户指南 中讨论的基于集合的选择算符需求。 因此，相比于 ReplicationController，应优先考虑 ReplicaSet。

##  3.  Kubernetes StatefulSets
StatefulSets
------------

StatefulSet 是用来管理有状态应用的工作负载 API 对象。

StatefulSet 用来管理某 Pod 集合的部署和扩缩， 并为这些 Pod 提供持久存储和持久标识符。

和 Deployment 类似， StatefulSet 管理基于相同容器规约的一组 Pod。但和 Deployment 不同的是， StatefulSet 为它们的每个 Pod 维护了一个有粘性的 ID。这些 Pod 是基于相同的规约来创建的， 但是不能相互替换：无论怎么调度，每个 Pod 都有一个永久不变的 ID。

如果希望使用存储卷为工作负载提供持久存储，可以使用 StatefulSet 作为解决方案的一部分。 尽管 StatefulSet 中的单个 Pod 仍可能出现故障， 但持久的 Pod 标识符使得将现有卷与替换已失败 Pod 的新 Pod 相匹配变得更加容易。

使用 StatefulSets
---------------

StatefulSets 对于需要满足以下一个或多个需求的应用程序很有价值：

*   稳定的、唯一的网络标识符。
*   稳定的、持久的存储。
*   有序的、优雅的部署和缩放。
*   有序的、自动的滚动更新。

在上面描述中，“稳定的”意味着 Pod 调度或重调度的整个过程是有持久性的。 如果应用程序不需要任何稳定的标识符或有序的部署、删除或伸缩，则应该使用 由一组无状态的副本控制器提供的工作负载来部署应用程序，比如 Deployment 或者 ReplicaSet 可能更适用于你的无状态应用部署需要。

限制 
---

*   给定 Pod 的存储必须由 PersistentVolume 驱动 基于所请求的 ​`storage class`​ 来提供，或者由管理员预先提供。
*   删除或者收缩 StatefulSet 并不会删除它关联的存储卷。 这样做是为了保证数据安全，它通常比自动清除 StatefulSet 所有相关的资源更有价值。
*   StatefulSet 当前需要无头服务 来负责 Pod 的网络标识。你需要负责创建此服务。
*   当删除 StatefulSets 时，StatefulSet 不提供任何终止 Pod 的保证。 为了实现 StatefulSet 中的 Pod 可以有序地且体面地终止，可以在删除之前将 StatefulSet 缩放为 0。
*   在默认 Pod 管理策略(​`OrderedReady`​) 时使用 滚动更新，可能进入需要人工干预 才能修复的损坏状态。

组件
--

下面的示例演示了 StatefulSet 的组件。

`apiVersion: v1 kind: Service metadata:   name: nginx   labels:     app: nginx spec:   ports:   - port: 80     name: web   clusterIP: None   selector:     app: nginx --- apiVersion: apps/v1 kind: StatefulSet metadata:   name: web spec:   selector:     matchLabels:       app: nginx # has to match .spec.template.metadata.labels   serviceName: "nginx"   replicas: 3 # by default is 1   template:     metadata:       labels:         app: nginx # has to match .spec.selector.matchLabels     spec:       terminationGracePeriodSeconds: 10       containers:       - name: nginx         image: k8s.gcr.io/nginx-slim:0.8         ports:         - containerPort: 80           name: web         volumeMounts:         - name: www           mountPath: /usr/share/nginx/html   volumeClaimTemplates:   - metadata:       name: www     spec:       accessModes: [ "ReadWriteOnce" ]       storageClassName: "my-storage-class"       resources:         requests:           storage: 1Gi`

上述例子中：

*   名为 ​`nginx` ​的 Headless Service 用来控制网络域名。
*   名为 ​`web` ​的 StatefulSet 有一个 Spec，它表明将在独立的 3 个 Pod 副本中启动 nginx 容器。
*   ​`volumeClaimTemplates` ​将通过 PersistentVolumes 驱动提供的 PersistentVolumes 来提供稳定的存储。

StatefulSet 的命名需要遵循DNS 子域名规范。

Pod选择算符
-------

你必须设置 StatefulSet 的 ​`.spec.selector`​ 字段，使之匹配其在 ​`.spec.template.metadata.labels`​ 中设置的标签。在 Kubernetes 1.8 版本之前， 被忽略 ​`.spec.selector`​ 字段会获得默认设置值。 在 1.8 和以后的版本中，未指定匹配的 Pod 选择器将在创建 StatefulSet 期间导致验证错误。

Pod 标识 
-------

StatefulSet Pod 具有唯一的标识，该标识包括顺序标识、稳定的网络标识和稳定的存储。 该标识和 Pod 是绑定的，不管它被调度在哪个节点上。

### 有序索引 

对于具有 N 个副本的 StatefulSet，StatefulSet 中的每个 Pod 将被分配一个整数序号， 从 0 到 N-1，该序号在 StatefulSet 上是唯一的。

### 稳定的网络 ID 

StatefulSet 中的每个 Pod 根据 StatefulSet 的名称和 Pod 的序号派生出它的主机名。 组合主机名的格式为​`$(StatefulSet 名称)-$(序号)`​。 上例将会创建三个名称分别为 ​`web-0、web-1、web-2`​ 的 Pod。 StatefulSet 可以使用 无头服务 控制它的 Pod 的网络域。管理域的这个服务的格式为： ​`$(服务名称).$(命名空间).svc.cluster.local`​，其中 ​`cluster.local`​ 是集群域。 一旦每个 Pod 创建成功，就会得到一个匹配的 DNS 子域，格式为： ​`$(pod 名称).$(所属服务的 DNS 域名)`​，其中所属服务由 StatefulSet 的 ​`serviceName` ​域来设定。

取决于集群域内部 DNS 的配置，有可能无法查询一个刚刚启动的 Pod 的 DNS 命名。 当集群内其他客户端在 Pod 创建完成前发出 Pod 主机名查询时，就会发生这种情况。 负缓存 (在 DNS 中较为常见) 意味着之前失败的查询结果会被记录和重用至少若干秒钟， 即使 Pod 已经正常运行了也是如此。

如果需要在 Pod 被创建之后及时发现它们，有以下选项：

*   直接查询 Kubernetes API（比如，利用 watch 机制）而不是依赖于 DNS 查询
*   缩短 Kubernetes DNS 驱动的缓存时长（通常这意味着修改 CoreDNS 的 ConfigMap，目前缓存时长为 30 秒）

正如限制中所述，你需要负责创建无头服务 以便为 Pod 提供网络标识。

下面给出一些选择集群域、服务名、StatefulSet 名、及其怎样影响 StatefulSet 的 Pod 上的 DNS 名称的示例：

集群域名

服务（名字空间/名字）

StatefulSet（名字空间/名字）

StatefulSet 域名

Pod DNS

Pod 主机名

cluster.local

default/nginx

default/web

nginx.default.svc.cluster.local

web-{0..N-1}.nginx.default.svc.cluster.local

web-{0..N-1}

cluster.local

foo/nginx

foo/web

nginx.foo.svc.cluster.local

web-{0..N-1}.nginx.foo.svc.cluster.local

web-{0..N-1}

kube.local

foo/nginx

foo/web

nginx.foo.svc.kube.local

web-{0..N-1}.nginx.foo.svc.kube.local

web-{0..N-1}

> 集群域会被设置为 ​`cluster.local`​，除非有其他配置。

### 稳定的存储

对于 StatefulSet 中定义的每个 VolumeClaimTemplate，每个 Pod 接收到一个 PersistentVolumeClaim。在上面的 nginx 示例中，每个 Pod 将会得到基于 StorageClass ​`my-storage-class`​ 提供的 1 Gib 的 PersistentVolume。 如果没有声明 StorageClass，就会使用默认的 StorageClass。 当一个 Pod 被调度（重新调度）到节点上时，它的 ​`volumeMounts` ​会挂载与其 PersistentVolumeClaims 相关联的 PersistentVolume。 请注意，当 Pod 或者 StatefulSet 被删除时，与 PersistentVolumeClaims 相关联的 PersistentVolume 并不会被删除。要删除它必须通过手动方式来完成。

### Pod 名称标签 

当 StatefulSet 控制器（Controller） 创建 Pod 时， 它会添加一个标签 ​`statefulset.kubernetes.io/pod-name`​，该标签值设置为 Pod 名称。 这个标签允许你给 StatefulSet 中的特定 Pod 绑定一个 Service。

部署和扩缩保证 
--------

*   对于包含 N 个 副本的 StatefulSet，当部署 Pod 时，它们是依次创建的，顺序为 ​`0..N-1`​。
*   当删除 Pod 时，它们是逆序终止的，顺序为 ​`N-1..0`​。
*   在将缩放操作应用到 Pod 之前，它前面的所有 Pod 必须是 Running 和 Ready 状态。
*   在 Pod 终止之前，所有的继任者必须完全关闭。

StatefulSet 不应将 ​`pod.Spec.TerminationGracePeriodSeconds`​ 设置为 0。 这种做法是不安全的，要强烈阻止。

在上面的 nginx 示例被创建后，会按照 web-0、web-1、web-2 的顺序部署三个 Pod。 在 web-0 进入 Running 和 Ready 状态前不会部署 web-1。在 web-1 进入 Running 和 Ready 状态前不会部署 web-2。 如果 web-1 已经处于 Running 和 Ready 状态，而 web-2 尚未部署，在此期间发生了 web-0 运行失败，那么 web-2 将不会被部署，要等到 web-0 部署完成并进入 Running 和 Ready 状态后，才会部署 web-2。

如果用户想将示例中的 StatefulSet 收缩为 ​`replicas=1`​，首先被终止的是 web-2。 在 web-2 没有被完全停止和删除前，web-1 不会被终止。 当 web-2 已被终止和删除、web-1 尚未被终止，如果在此期间发生 web-0 运行失败， 那么就不会终止 web-1，必须等到 web-0 进入 Running 和 Ready 状态后才会终止 web-1。

### Pod 管理策略

在 Kubernetes 1.7 及以后的版本中，StatefulSet 允许你放宽其排序保证， 同时通过它的 ​`.spec.podManagementPolicy`​ 域保持其唯一性和身份保证。

#### OrderedReady Pod 管理

​`OrderedReady` ​Pod 管理是 StatefulSet 的默认设置。它实现了 上面描述的功能。

#### 并行 Pod 管理 

​`Parallel` ​Pod 管理让 StatefulSet 控制器并行的启动或终止所有的 Pod， 启动或者终止其他 Pod 前，无需等待 Pod 进入 Running 和 ready 或者完全停止状态。 这个选项只会影响伸缩操作的行为，更新则不会被影响。

更新策略 
-----

StatefulSet 的 ​`.spec.updateStrategy`​ 字段让 你可以配置和禁用掉自动滚动更新 Pod 的容器、标签、资源请求或限制、以及注解。 有两个允许的值：

*   OnDelete

*   当 StatefulSet 的 ​`.spec.updateStrategy.type`​ 设置为 ​`OnDelete` ​时， 它的控制器将不会自动更新 StatefulSet 中的 Pod。 用户必须手动删除 Pod 以便让控制器创建新的 Pod，以此来对 StatefulSet 的 ​`.spec.template`​ 的变动作出反应。

*   RollingUpdate

*   ​`RollingUpdate` ​更新策略对 StatefulSet 中的 Pod 执行自动的滚动更新。这是默认的更新策略。

滚动更新
----

当 StatefulSet 的 ​`.spec.updateStrategy.type`​ 被设置为 ​`RollingUpdate` ​时， StatefulSet 控制器会删除和重建 StatefulSet 中的每个 Pod。 它将按照与 Pod 终止相同的顺序（从最大序号到最小序号）进行，每次更新一个 Pod。

Kubernetes 控制面会等到被更新的 Pod 进入 Running 和 Ready 状态，然后再更新其前身。 如果你设置了 ​`.spec.minReadySeconds`​（查看最短就绪秒数），控制面在 Pod 就绪后会额外等待一定的时间再执行下一步。

### 分区滚动更新 

通过声明 ​`.spec.updateStrategy.rollingUpdate.partition`​ 的方式，​`RollingUpdate` ​更新策略可以实现分区。 如果声明了一个分区，当 StatefulSet 的 ​`.spec.template`​ 被更新时， 所有序号大于等于该分区序号的 Pod 都会被更新。 所有序号小于该分区序号的 Pod 都不会被更新，并且，即使他们被删除也会依据之前的版本进行重建。 如果 StatefulSet 的 ​`.spec.updateStrategy.rollingUpdate.partition`​ 大于它的 ​`.spec.replicas`​，对它的 ​`.spec.template`​ 的更新将不会传递到它的 Pod。 在大多数情况下，你不需要使用分区，但如果你希望进行阶段更新、执行金丝雀或执行 分阶段上线，则这些分区会非常有用。

### 强制回滚

在默认 Pod 管理策略(​`OrderedReady`​) 下使用 滚动更新 ，可能进入需要人工干预才能修复的损坏状态。

如果更新后 Pod 模板配置进入无法运行或就绪的状态（例如，由于错误的二进制文件 或应用程序级配置错误），StatefulSet 将停止回滚并等待。

在这种状态下，仅将 Pod 模板还原为正确的配置是不够的。由于 已知问题，StatefulSet 将继续等待损坏状态的 Pod 准备就绪（永远不会发生），然后再尝试将其恢复为正常工作配置。

恢复模板后，还必须删除 StatefulSet 尝试使用错误的配置来运行的 Pod。这样， StatefulSet 才会开始使用被还原的模板来重新创建 Pod。

### 最短就绪秒数 

FEATURE STATE: Kubernetes v1.22 \[alpha\]

​`.spec.minReadySeconds`​ 是一个可选字段，用于指定新创建的 Pod 就绪（没有任何容器崩溃）后被认为可用的最小秒数。 默认值是 0（Pod 就绪时就被认为可用）。

请注意只有当你启用 ​`StatefulSetMinReadySeconds` ​特性门控时，该字段才会生效。

##  4.  Kubernetes DaemonSet
DaemonSet
---------

DaemonSet 确保全部（或者某些）节点上运行一个 Pod 的副本。 当有节点加入集群时， 也会为他们新增一个 Pod 。 当有节点从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。

DaemonSet 的一些典型用法：

*   在每个节点上运行集群守护进程
*   在每个节点上运行日志收集守护进程
*   在每个节点上运行监控守护进程

一种简单的用法是为每种类型的守护进程在所有的节点上都启动一个 DaemonSet。 一个稍微复杂的用法是为同一种守护进程部署多个 DaemonSet；每个具有不同的标志， 并且对不同硬件类型具有不同的内存、CPU 要求。

编写 DaemonSet Spec 
------------------

### 创建 DaemonSet 

你可以在 YAML 文件中描述 DaemonSet。 例如，下面的 daemonset.yaml 文件描述了一个运行 fluentd-elasticsearch Docker 镜像的 DaemonSet：

`apiVersion: apps/v1 kind: DaemonSet metadata:   name: fluentd-elasticsearch   namespace: kube-system   labels:     k8s-app: fluentd-logging spec:   selector:     matchLabels:       name: fluentd-elasticsearch   template:     metadata:       labels:         name: fluentd-elasticsearch     spec:       tolerations:       # 这些容忍度设置是为了让守护进程在控制平面节点上运行       # 如果你不希望控制平面节点运行 Pod，可以删除它们       - key: node-role.kubernetes.io/control-plane         operator: Exists         effect: NoSchedule       - key: node-role.kubernetes.io/master         operator: Exists         effect: NoSchedule       containers:       - name: fluentd-elasticsearch         image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2         resources:           limits:             memory: 200Mi           requests:             cpu: 100m             memory: 200Mi         volumeMounts:         - name: varlog           mountPath: /var/log         - name: varlibdockercontainers           mountPath: /var/lib/docker/containers           readOnly: true       terminationGracePeriodSeconds: 30       volumes:       - name: varlog         hostPath:           path: /var/log       - name: varlibdockercontainers         hostPath:           path: /var/lib/docker/containers`

基于 YAML 文件创建 DaemonSet：

`kubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml`

### 必需字段 

和所有其他 Kubernetes 配置一样，DaemonSet 需要 ​`apiVersion`​、​`kind` ​和 ​`metadata` ​字段。 有关配置文件的基本信息，参见 部署应用、 配置容器和 使用 kubectl 进行对象管理 文档。

DaemonSet 对象的名称必须是一个合法的 DNS 子域名。

DaemonSet 也需要一个 ​`[.spec](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md target=)`​ 配置段。

### Pod 模板 

​`.spec`​ 中唯一必需的字段是 ​`.spec.template`​。

​`.spec.template`​ 是一个 Pod 模板。 除了它是嵌套的，因而不具有 ​`apiVersion` ​或 ​`kind` ​字段之外，它与 Pod 具有相同的 schema。

除了 Pod 必需字段外，在 DaemonSet 中的 Pod 模板必须指定合理的标签（查看 Pod 选择算符）。

在 DaemonSet 中的 Pod 模板必须具有一个值为 ​`Always` ​的 ​`RestartPolicy`​。 当该值未指定时，默认是 ​`Always`​。

### Pod 选择算符 

​`.spec.selector`​ 字段表示 Pod 选择算符，它与 Job 的 ​`.spec.selector`​ 的作用是相同的。

你必须指定与 ​`.spec.template`​ 的标签匹配的 Pod 选择算符。 此外，一旦创建了 DaemonSet，它的 ​`.spec.selector`​ 就不能修改。 修改 Pod 选择算符可能导致 Pod 意外悬浮，并且这对用户来说是费解的。

​`spec.selector`​ 是一个对象，如下两个字段组成：

*   ​`matchLabels` ​- 与 ReplicationController 的 ​`.spec.selector`​ 的作用相同。
*   ​`matchExpressions` ​- 允许构建更加复杂的选择器，可以通过指定 key、value 列表以及将 key 和 value 列表关联起来的 operator。

当上述两个字段都指定时，结果会按逻辑与（AND）操作处理。

​`.spec.selector`​ 必须与 ​`.spec.template.metadata.labels`​ 相匹配。 如果配置中这两个字段不匹配，则会被 API 拒绝。

### 仅在某些节点上运行 Pod 

如果指定了 ​`.spec.template.spec.nodeSelector`​，DaemonSet 控制器将在能够与 Node 选择算符 匹配的节点上创建 Pod。 类似这种情况，可以指定 ​`.spec.template.spec.affinity`​，之后 DaemonSet 控制器 将在能够与节点亲和性 匹配的节点上创建 Pod。 如果根本就没有指定，则 DaemonSet Controller 将在所有节点上创建 Pod。

Daemon Pods 是如何被调度的 
--------------------

### 通过默认调度器调度 

FEATURE STATE: Kubernetes 1.17 \[stable\]

DaemonSet 确保所有符合条件的节点都运行该 Pod 的一个副本。 通常，运行 Pod 的节点由 Kubernetes 调度器选择。 不过，DaemonSet Pods 由 DaemonSet 控制器创建和调度。这就带来了以下问题：

*   Pod 行为的不一致性：正常 Pod 在被创建后等待调度时处于 ​`Pending` ​状态， DaemonSet Pods 创建后不会处于 ​`Pending` ​状态下。这使用户感到困惑。
*   Pod 抢占 由默认调度器处理。启用抢占后，DaemonSet 控制器将在不考虑 Pod 优先级和抢占 的情况下制定调度决策。

​`ScheduleDaemonSetPods` ​允许您使用默认调度器而不是 DaemonSet 控制器来调度 DaemonSets， 方法是将 ​`NodeAffinity` ​条件而不是 ​`.spec.nodeName`​ 条件添加到 DaemonSet Pods。 默认调度器接下来将 Pod 绑定到目标主机。 如果 DaemonSet Pod 的节点亲和性配置已存在，则被替换 （原始的节点亲和性配置在选择目标主机之前被考虑）。 DaemonSet 控制器仅在创建或修改 DaemonSet Pod 时执行这些操作， 并且不会更改 DaemonSet 的 ​`spec.template`​。

`nodeAffinity:   requiredDuringSchedulingIgnoredDuringExecution:     nodeSelectorTerms:     - matchFields:       - key: metadata.name         operator: In         values:         - target-host-name`
    

此外，系统会自动添加 ​`node.kubernetes.io/unschedulable：NoSchedule`​ 容忍度到 DaemonSet Pods。在调度 DaemonSet Pod 时，默认调度器会忽略 ​`unschedulable` ​节点。

### 污点和容忍度 

尽管 Daemon Pods 遵循污点和容忍度 规则，根据相关特性，控制器会自动将以下容忍度添加到 DaemonSet Pod：

容忍度键名

效果

版本

描述

`node.kubernetes.io/not-ready`

NoExecute

1.13+

当出现类似网络断开的情况导致节点问题时，DaemonSet Pod 不会被逐出。

`node.kubernetes.io/unreachable`

NoExecute

1.13+

当出现类似于网络断开的情况导致节点问题时，DaemonSet Pod 不会被逐出。

`node.kubernetes.io/disk-pressure`

NoSchedule

1.8+

DaemonSet Pod 被默认调度器调度时能够容忍磁盘压力属性。

`node.kubernetes.io/memory-pressure`

NoSchedule

1.8+

DaemonSet Pod 被默认调度器调度时能够容忍内存压力属性。

`node.kubernetes.io/unschedulable`

NoSchedule

1.12+

DaemonSet Pod 能够容忍默认调度器所设置的 `unschedulable` 属性.

`node.kubernetes.io/network-unavailable`

NoSchedule

1.12+

DaemonSet 在使用宿主网络时，能够容忍默认调度器所设置的 `network-unavailable` 属性。

与 Daemon Pods 通信 
-----------------

与 DaemonSet 中的 Pod 进行通信的几种可能模式如下：

*   推送（Push）：配置 DaemonSet 中的 Pod，将更新发送到另一个服务，例如统计数据库。 这些服务没有客户端。
*   NodeIP 和已知端口：DaemonSet 中的 Pod 可以使用 ​`hostPort`​，从而可以通过节点 IP 访问到 Pod。客户端能通过某种方法获取节点 IP 列表，并且基于此也可以获取到相应的端口。
*   DNS：创建具有相同 Pod 选择算符的 无头服务， 通过使用 ​`endpoints` ​资源或从 DNS 中检索到多个 A 记录来发现 DaemonSet。
*   Service：创建具有相同 Pod 选择算符的服务，并使用该服务随机访问到某个节点上的 守护进程（没有办法访问到特定节点）。

更新 DaemonSet 
-------------

如果节点的标签被修改，DaemonSet 将立刻向新匹配上的节点添加 Pod， 同时删除不匹配的节点上的 Pod。

你可以修改 DaemonSet 创建的 Pod。不过并非 Pod 的所有字段都可更新。 下次当某节点（即使具有相同的名称）被创建时，DaemonSet 控制器还会使用最初的模板。

您可以删除一个 DaemonSet。如果使用 ​`kubectl` ​并指定 ​`--cascade=orphan`​ 选项， 则 Pod 将被保留在节点上。接下来如果创建使用相同选择算符的新 DaemonSet， 新的 DaemonSet 会收养已有的 Pod。 如果有 Pod 需要被替换，DaemonSet 会根据其 ​`updateStrategy` ​来替换。

你可以对 DaemonSet 执行滚动更新操作。

DaemonSet 的替代方案 
----------------

### init 脚本

直接在节点上启动守护进程（例如使用 ​`init`​、​`upstartd` ​或 ​`systemd`​）的做法当然是可行的。 不过，基于 DaemonSet 来运行这些进程有如下一些好处：

*   像所运行的其他应用一样，DaemonSet 具备为守护进程提供监控和日志管理的能力。
*   为守护进程和应用所使用的配置语言和工具（如 Pod 模板、​`kubectl`​）是相同的。
*   在资源受限的容器中运行守护进程能够增加守护进程和应用容器的隔离性。 然而，这一点也可以通过在容器中运行守护进程但却不在 Pod 中运行之来实现。 例如，直接基于 Docker 启动。

### 裸 Pod 

直接创建 Pod并指定其运行在特定的节点上也是可以的。 然而，DaemonSet 能够替换由于任何原因（例如节点失败、例行节点维护、内核升级） 而被删除或终止的 Pod。 由于这个原因，你应该使用 DaemonSet 而不是单独创建 Pod。

### 静态 Pod 

通过在一个指定的、受 ​`kubelet` ​监视的目录下编写文件来创建 Pod 也是可行的。 这类 Pod 被称为静态 Pod。 不像 DaemonSet，静态 Pod 不受 ​`kubectl` ​和其它 Kubernetes API 客户端管理。 静态 Pod 不依赖于 API 服务器，这使得它们在启动引导新集群的情况下非常有用。 此外，静态 Pod 在将来可能会被废弃。

### Deployments

DaemonSet 与 Deployments 非常类似， 它们都能创建 Pod，并且 Pod 中的进程都不希望被终止（例如，Web 服务器、存储服务器）。

建议为无状态的服务使用 Deployments，比如前端服务。 对这些服务而言，对副本的数量进行扩缩容、平滑升级，比精确控制 Pod 运行在某个主机上要重要得多。 当需要 Pod 副本总是运行在全部或特定主机上，并且当该 DaemonSet 提供了节点级别的功能（允许其他 Pod 在该特定节点上正确运行）时， 应该使用 DaemonSet。

例如，网络插件通常包含一个以 DaemonSet 运行的组件。 这个 DaemonSet 组件确保它所在的节点的集群网络正常工作。

##  5.  Kubernetes Jobs
Jobs
----

Job 会创建一个或者多个 Pods，并将继续重试 Pods 的执行，直到指定数量的 Pods 成功终止。 随着 Pods 成功结束，Job 跟踪记录成功完成的 Pods 个数。 当数量达到指定的成功个数阈值时，任务（即 Job）结束。 删除 Job 的操作会清除所创建的全部 Pods。 挂起 Job 的操作会删除 Job 的所有活跃 Pod，直到 Job 被再次恢复执行。

一种简单的使用场景下，你会创建一个 Job 对象以便以一种可靠的方式运行某 Pod 直到完成。 当第一个 Pod 失败或者被删除（比如因为节点硬件失效或者重启）时，Job 对象会启动一个新的 Pod。

你也可以使用 Job 以并行的方式运行多个 Pod。

运行示例 Job 
---------

下面是一个 Job 配置示例。它负责计算 π 到小数点后 2000 位，并将结果打印出来。 此计算大约需要 10 秒钟完成。

`apiVersion: batch/v1 kind: Job metadata:   name: pi spec:   template:     spec:       containers:       - name: pi         image: perl         command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]       restartPolicy: Never   backoffLimit: 4`

你可以使用下面的命令来运行此示例：

`kubectl apply -f https://kubernetes.io/examples/controllers/job.yaml`

输出类似于：

`job.batch/pi created`

使用 ​`kubectl` ​来检查 Job 的状态：

`kubectl describe jobs/pi`

输出类似于：

`Name:           pi Namespace:      default Selector:       controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c Labels:         controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c                 job-name=pi Annotations:    kubectl.kubernetes.io/last-applied-configuration:                   {"apiVersion":"batch/v1","kind":"Job","metadata":{"annotations":{},"name":"pi","namespace":"default"},"spec":{"backoffLimit":4,"template":... Parallelism:    1 Completions:    1 Start Time:     Mon, 02 Dec 2019 15:20:11 +0200 Completed At:   Mon, 02 Dec 2019 15:21:16 +0200 Duration:       65s Pods Statuses:  0 Running / 1 Succeeded / 0 Failed Pod Template:   Labels:  controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c            job-name=pi   Containers:    pi:     Image:      perl     Port:       <none>     Host Port:  <none>     Command:       perl       -Mbignum=bpi       -wle       print bpi(2000)     Environment:  <none>     Mounts:       <none>   Volumes:        <none> Events:   Type    Reason            Age   From            Message   ----    ------            ----  ----            -------   Normal  SuccessfulCreate  14m   job-controller  Created pod: pi-5rwd7`

要查看 Job 对应的已完成的 Pods，可以执行 ​`kubectl get pods`​。

要以机器可读的方式列举隶属于某 Job 的全部 Pods，你可以使用类似下面这条命令：

`pods=$(kubectl get pods --selector=job-name=pi --output=jsonpath='{.items[*].metadata.name}') echo $pods`

输出类似于：

`pi-5rwd7`

这里，选择算符与 Job 的选择算符相同。​`--output=jsonpath`​ 选项给出了一个表达式， 用来从返回的列表中提取每个 Pod 的 name 字段。

查看其中一个 Pod 的标准输出：

`kubectl logs $pods`

输出类似于：

`3.1415926535 #后面为Π的无穷数`

编写 Job 规约 
----------

与 Kubernetes 中其他资源的配置类似，Job 也需要 ​`apiVersion`​、​`kind` ​和 ​`metadata` ​字段。 Job 的名字必须是合法的 DNS 子域名。

Job 配置还需要一个​`[.spec](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md target=)`​ 节。

### Pod 模版

Job 的 ​`.spec`​ 中只有 ​`.spec.template`​ 是必需的字段。

字段 ​`.spec.template`​ 的值是一个 Pod 模版。 其定义规范与 Pod 完全相同，只是其中不再需要 ​`apiVersion` ​或 ​`kind` ​字段。

除了作为 Pod 所必需的字段之外，Job 中的 Pod 模版必需设置合适的标签 （参见Pod 选择算符）和合适的重启策略。

Job 中 Pod 的 ​`RestartPolicy` ​只能设置为 ​`Never` ​或 ​`OnFailure` ​之一。

### Pod 选择算符 

字段 ​`.spec.selector`​ 是可选的。在绝大多数场合，你都不需要为其赋值。

### Job 的并行执行

适合以 Job 形式来运行的任务主要有三种：

1.  非并行 Job：

*   通常只启动一个 Pod，除非该 Pod 失败。
*   当 Pod 成功终止时，立即视 Job 为完成状态。

3.  具有 确定完成计数 的并行 Job：

*   ​`.spec.completions`​ 字段设置为非 0 的正数值。
*   Job 用来代表整个任务，当成功的 Pod 个数达到 ​`.spec.completions`​ 时，Job 被视为完成。
*   当使用 ​`.spec.completionMode="Indexed"`​ 时，每个 Pod 都会获得一个不同的 索引值，介于 0 和 ​`.spec.completions-1`​ 之间。

5.  带 工作队列 的并行 Job：

*   不设置 ​`spec.completions`​，默认值为 ​`.spec.parallelism`​。
*   多个 Pod 之间必须相互协调，或者借助外部服务确定每个 Pod 要处理哪个工作条目。 例如，任一 Pod 都可以从工作队列中取走最多 N 个工作条目。
*   每个 Pod 都可以独立确定是否其它 Pod 都已完成，进而确定 Job 是否完成。
*   当 Job 中 任何 Pod 成功终止，不再创建新 Pod。
*   一旦至少 1 个 Pod 成功完成，并且所有 Pod 都已终止，即可宣告 Job 成功完成。
*   一旦任何 Pod 成功退出，任何其它 Pod 都不应再对此任务执行任何操作或生成任何输出。 所有 Pod 都应启动退出过程。

对于 非并行 的 Job，你可以不设置 ​`spec.completions`​ 和 ​`spec.parallelism`​。 这两个属性都不设置时，均取默认值 1。

对于 确定完成计数 类型的 Job，你应该设置 ​`.spec.completions`​ 为所需要的完成个数。 你可以设置 ​`.spec.parallelism`​，也可以不设置。其默认值为 1。

对于一个 工作队列 Job，你不可以设置 ​`.spec.completions`​，但要将​`.spec.parallelism`​ 设置为一个非负整数。

#### 控制并行性 

并行性请求（​`.spec.parallelism`​）可以设置为任何非负整数。 如果未设置，则默认为 1。 如果设置为 0，则 Job 相当于启动之后便被暂停，直到此值被增加。

实际并行性（在任意时刻运行状态的 Pods 个数）可能比并行性请求略大或略小， 原因如下：

*   对于 确定完成计数 Job，实际上并行执行的 Pods 个数不会超出剩余的完成数。 如果 ​`.spec.parallelism`​ 值较高，会被忽略。
*   对于 工作队列 Job，有任何 Job 成功结束之后，不会有新的 Pod 启动。 不过，剩下的 Pods 允许执行完毕。
*   如果 Job 控制器 没有来得及作出响应，或者
*   如果 Job 控制器因为任何原因（例如，缺少 ​`ResourceQuota` ​或者没有权限）无法创建 Pods。 Pods 个数可能比请求的数目小。
*   Job 控制器可能会因为之前同一 Job 中 Pod 失效次数过多而压制新 Pod 的创建。
*   当 Pod 处于体面终止进程中，需要一定时间才能停止。

### 完成模式 

FEATURE STATE: Kubernetes v1.22 \[beta\]

带有 确定完成计数 的 Job，即 ​`.spec.completions`​ 不为 null 的 Job， 都可以在其 ​`.spec.completionMode`​ 中设置完成模式：

*   ​`NonIndexed`​（默认值）：当成功完成的 Pod 个数达到 ​`.spec.completions`​ 所 设值时认为 Job 已经完成。换言之，每个 Job 完成事件都是独立无关且同质的。 要注意的是，当 ​`.spec.completions`​ 取值为 null 时，Job 被隐式处理为 ​`NonIndexed`​。
*   ​`Indexed`​：Job 的 Pod 会获得对应的完成索引，取值为 0 到 ​`.spec.completions-1`​。 该索引可以通过三种方式获取：

*   Pod 注解 ​`batch.kubernetes.io/job-completion-index`​。
*   作为 Pod 主机名的一部分，遵循模式 ​`$(job-name)-$(index)`​。 当你同时使用带索引的 Job（Indexed Job）与 服务（Service）， Job 中的 Pods 可以通过 DNS 使用确切的主机名互相寻址。
*   对于容器化的任务，在环境变量 ​`JOB_COMPLETION_INDEX`​ 中。

当每个索引都对应一个完成完成的 Pod 时，Job 被认为是已完成的。需要注意的是，对同一索引值可能被启动的 Pod 不止一个，尽管这种情况很少发生。 这时，只有一个会被记入完成计数中。

处理 Pod 和容器失效
------------

Pod 中的容器可能因为多种不同原因失效，例如因为其中的进程退出时返回值非零， 或者容器因为超出内存约束而被杀死等等。 如果发生这类事件，并且 ​`.spec.template.spec.restartPolicy = "OnFailure"`​， Pod 则继续留在当前节点，但容器会被重新运行。 因此，你的程序需要能够处理在本地被重启的情况，或者要设置 ​`.spec.template.spec.restartPolicy = "Never"`​。

整个 Pod 也可能会失败，且原因各不相同。 例如，当 Pod 启动时，节点失效（被升级、被重启、被删除等）或者其中的容器失败而 ​`.spec.template.spec.restartPolicy = "Never"`​。 当 Pod 失败时，Job 控制器会启动一个新的 Pod。 这意味着，你的应用需要处理在一个新 Pod 中被重启的情况。 尤其是应用需要处理之前运行所产生的临时文件、锁、不完整的输出等问题。

注意，即使你将 ​`.spec.parallelism`​ 设置为 1，且将 ​`.spec.completions`​ 设置为 1，并且 ​`.spec.template.spec.restartPolicy`​ 设置为 "Never"，同一程序仍然有可能被启动两次。

如果你确实将 ​`.spec.parallelism`​ 和 ​`.spec.completions`​ 都设置为比 1 大的值， 那就有可能同时出现多个 Pod 运行的情况。 为此，你的 Pod 也必须能够处理并发性问题。

### Pod 回退失效策略

在有些情形下，你可能希望 Job 在经历若干次重试之后直接进入失败状态，因为这很 可能意味着遇到了配置错误。 为了实现这点，可以将 ​`.spec.backoffLimit`​ 设置为视 Job 为失败之前的重试次数。 失效回退的限制值默认为 6。 与 Job 相关的失效的 Pod 会被 Job 控制器重建，回退重试时间将会按指数增长 （从 10 秒、20 秒到 40 秒）最多至 6 分钟。 当 Job 的 Pod 被删除时，或者 Pod 成功时没有其它 Pod 处于失败状态，失效回退的次数也会被重置（为 0）。

> 如果你的 Job 的 ​`restartPolicy` ​被设置为 "OnFailure"，就要注意运行该 Job 的 Pod 会在 Job 到达失效回退次数上限时自动被终止。 这会使得调试 Job 中可执行文件的工作变得非常棘手。 我们建议在调试 Job 时将 ​`restartPolicy` ​设置为 "Never"， 或者使用日志系统来确保失效 Jobs 的输出不会意外遗失。

Job 终止与清理
---------

Job 完成时不会再创建新的 Pod，不过已有的 Pod 通常也不会被删除。 保留这些 Pod 使得你可以查看已完成的 Pod 的日志输出，以便检查错误、警告 或者其它诊断性输出。 Job 完成时 Job 对象也一样被保留下来，这样你就可以查看它的状态。 在查看了 Job 状态之后删除老的 Job 的操作留给了用户自己。 你可以使用 ​`kubectl` ​来删除 Job（例如，​`kubectl delete jobs/pi`​ 或者 ​`kubectl delete -f ./job.yaml`​）。 当使用 ​`kubectl` ​来删除 Job 时，该 Job 所创建的 Pods 也会被删除。

默认情况下，Job 会持续运行，除非某个 Pod 失败（​`restartPolicy=Never`​） 或者某个容器出错退出（​`restartPolicy=OnFailure`​）。 这时，Job 基于前述的 ​`spec.backoffLimit`​ 来决定是否以及如何重试。 一旦重试次数到达 ​`.spec.backoffLimit`​ 所设的上限，Job 会被标记为失败， 其中运行的 Pods 都会被终止。

终止 Job 的另一种方式是设置一个活跃期限。 你可以为 Job 的 ​`.spec.activeDeadlineSeconds`​ 设置一个秒数值。 该值适用于 Job 的整个生命期，无论 Job 创建了多少个 Pod。 一旦 Job 运行时间达到 ​`activeDeadlineSeconds` ​秒，其所有运行中的 Pod 都会被终止，并且 Job 的状态更新为 ​`type: Failed`​ 及 ​`reason: DeadlineExceeded`​。

注意 Job 的 ​`.spec.activeDeadlineSeconds`​ 优先级高于其 ​`.spec.backoffLimit`​ 设置。 因此，如果一个 Job 正在重试一个或多个失效的 Pod，该 Job 一旦到达 ​`activeDeadlineSeconds` ​所设的时限即不再部署额外的 Pod，即使其重试次数还未 达到 ​`backoffLimit` ​所设的限制。

例如：

`apiVersion: batch/v1 kind: Job metadata:   name: pi-with-timeout spec:   backoffLimit: 5   activeDeadlineSeconds: 100   template:     spec:       containers:       - name: pi         image: perl         command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]       restartPolicy: Never`

注意 Job 规约和 Job 中的 Pod 模版规约 都有 ​`activeDeadlineSeconds` ​字段。 请确保你在合适的层次设置正确的字段。

还要注意的是，​`restartPolicy` ​对应的是 Pod，而不是 Job 本身： 一旦 Job 状态变为 ​`type: Failed`​，就不会再发生 Job 重启的动作。 换言之，由 ​`.spec.activeDeadlineSeconds`​ 和 ​`.spec.backoffLimit`​ 所触发的 Job 终结机制 都会导致 Job 永久性的失败，而这类状态都需要手工干预才能解决。

自动清理完成的 Job 
------------

完成的 Job 通常不需要留存在系统中。在系统中一直保留它们会给 API 服务器带来额外的压力。 如果 Job 由某种更高级别的控制器来管理，例如 CronJobs， 则 Job 可以被 CronJob 基于特定的根据容量裁定的清理策略清理掉。

### 已完成 Job 的 TTL 机制 

FEATURE STATE: Kubernetes v1.23 \[stable\]

自动清理已完成 Job （状态为 ​`Complete` ​或 ​`Failed`​）的另一种方式是使用由 TTL 控制器所提供 的 TTL 机制。 通过设置 Job 的 ​`.spec.ttlSecondsAfterFinished`​ 字段，可以让该控制器清理掉 已结束的资源。

TTL 控制器清理 Job 时，会级联式地删除 Job 对象。 换言之，它会删除所有依赖的对象，包括 Pod 及 Job 本身。 注意，当 Job 被删除时，系统会考虑其生命周期保障，例如其 Finalizers。

例如：

`apiVersion: batch/v1 kind: Job metadata:   name: pi-with-ttl spec:   ttlSecondsAfterFinished: 100   template:     spec:       containers:       - name: pi         image: perl         command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]       restartPolicy: Never`

Job ​`pi-with-ttl`​ 在结束 100 秒之后，可以成为被自动删除的对象。

如果该字段设置为 ​`0`​，Job 在结束之后立即成为可被自动删除的对象。 如果该字段没有设置，Job 不会在结束之后被 TTL 控制器自动清除。

Job 模式 
-------

Job 对象可以用来支持多个 Pod 的可靠的并发执行。 Job 对象不是设计用来支持相互通信的并行进程的，后者一般在科学计算中应用较多。 Job 的确能够支持对一组相互独立而又有所关联的 工作条目 的并行处理。 这类工作条目可能是要发送的电子邮件、要渲染的视频帧、要编解码的文件、NoSQL 数据库中要扫描的主键范围等等。

在一个复杂系统中，可能存在多个不同的工作条目集合。这里我们仅考虑用户希望一起管理的 工作条目集合之一 — 批处理作业。

并行计算的模式有好多种，每种都有自己的强项和弱点。这里要权衡的因素有：

*   每个工作条目对应一个 Job 或者所有工作条目对应同一 Job 对象。 后者更适合处理大量工作条目的场景； 前者会给用户带来一些额外的负担，而且需要系统管理大量的 Job 对象。
*   创建与工作条目相等的 Pod 或者令每个 Pod 可以处理多个工作条目。 前者通常不需要对现有代码和容器做较大改动； 后者则更适合工作条目数量较大的场合，原因同上。
*   有几种技术都会用到工作队列。这意味着需要运行一个队列服务，并修改现有程序或容器 使之能够利用该工作队列。 与之比较，其他方案在修改现有容器化应用以适应需求方面可能更容易一些。

下面是对这些权衡的汇总，列 2 到 4 对应上面的权衡比较。

模式

单个 Job 对象

Pods 数少于工作条目数？

直接使用应用无需修改?

每工作条目一 Pod 的队列

✓

有时

Pod 数量可变的队列

✓

✓

静态任务分派的带索引的 Job

✓

✓

Job 模版扩展

✓

当你使用 ​`.spec.completions`​ 来设置完成数时，Job 控制器所创建的每个 Pod 使用完全相同的 ​`[spec](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md target=)`​。 这意味着任务的所有 Pod 都有相同的命令行，都使用相同的镜像和数据卷，甚至连 环境变量都（几乎）相同。 这些模式是让每个 Pod 执行不同工作的几种不同形式。

下表显示的是每种模式下 ​`.spec.parallelism`​ 和 ​`.spec.completions`​ 所需要的设置。 其中，​`W`​ 表示的是工作条目的个数。

模式

.spec.completions

.spec.parallelism

每工作条目一 Pod 的队列

W

任意值

Pod 个数可变的队列

1

任意值

静态任务分派的带索引的 Job

W

Job 模版扩展

1

应该为 1

高级用法 
-----

### 挂起 Job 

FEATURE STATE: Kubernetes v1.21 \[alpha\]

> 该特性在 Kubernetes 1.21 版本中是 Alpha 阶段，启用该特性需要额外的步骤； 请确保你正在阅读与集群版本一致的文档。

Job 被创建时，Job 控制器会马上开始执行 Pod 创建操作以满足 Job 的需求， 并持续执行此操作直到 Job 完成为止。 不过你可能想要暂时挂起 Job 执行，或启动处于挂起状态的job， 并拥有一个自定义控制器以后再决定什么时候开始。

要挂起一个 Job，你可以更新 ​`.spec.suspend`​ 字段为 true， 之后，当你希望恢复其执行时，将其更新为 false。 创建一个 ​`.spec.suspend`​ 被设置为 true 的 Job 本质上会将其创建为被挂起状态。

当 Job 被从挂起状态恢复执行时，其 ​`.status.startTime`​ 字段会被重置为 当前的时间。这意味着 ​`.spec.activeDeadlineSeconds`​ 计时器会在 Job 挂起时 被停止，并在 Job 恢复执行时复位。

要记住的是，挂起 Job 会删除其所有活跃的 Pod。当 Job 被挂起时，你的 Pod 会 收到 SIGTERM 信号而被终止。 Pod 的体面终止期限会被考虑，不过 Pod 自身也必须在此期限之内处理完信号。 处理逻辑可能包括保存进度以便将来恢复，或者取消已经做出的变更等等。 Pod 以这种形式终止时，不会被记入 Job 的 ​`completions` ​计数。

处于被挂起状态的 Job 的定义示例可能是这样子：

`kubectl get job myjob -o yaml`

`apiVersion: batch/v1 kind: Job metadata:   name: myjob spec:   suspend: true   parallelism: 1   completions: 5   template:     spec:       ...`

Job 的 ​`status` ​可以用来确定 Job 是否被挂起，或者曾经被挂起。

`kubectl get jobs/myjob -o yaml`

`apiVersion: batch/v1 kind: Job # .metadata and .spec omitted status:   conditions:   - lastProbeTime: "2021-02-05T13:14:33Z"     lastTransitionTime: "2021-02-05T13:14:33Z"     status: "True"     type: Suspended   startTime: "2021-02-05T13:13:48Z"`

Job 的 "Suspended" 类型的状况在状态值为 "True" 时意味着 Job 正被 挂起；​`lastTransitionTime` ​字段可被用来确定 Job 被挂起的时长。 如果此状况字段的取值为 "False"，则 Job 之前被挂起且现在在运行。 如果 "Suspended" 状况在 ​`status` ​字段中不存在，则意味着 Job 从未 被停止执行。

当 Job 被挂起和恢复执行时，也会生成事件：

`kubectl describe jobs/myjob`

`Name:           myjob ... Events:   Type    Reason            Age   From            Message   ----    ------            ----  ----            -------   Normal  SuccessfulCreate  12m   job-controller  Created pod: myjob-hlrpl   Normal  SuccessfulDelete  11m   job-controller  Deleted pod: myjob-hlrpl   Normal  Suspended         11m   job-controller  Job suspended   Normal  SuccessfulCreate  3s    job-controller  Created pod: myjob-jvb44   Normal  Resumed           3s    job-controller  Job resumed`

最后四个事件，特别是 "Suspended" 和 "Resumed" 事件，都是因为 ​`.spec.suspend`​ 字段值被改来改去造成的。在这两个事件之间，我们看到没有 Pod 被创建，不过当 Job 被恢复执行时，Pod 创建操作立即被重启执行。

### 可变调度指令

FEATURE STATE: Kubernetes v1.23 \[beta\]

> 为了使用此功能，你必须在 API 服务器上启用 ​`JobMutableNodeSchedulingDirectives`​ 特性门控。 默认情况下启用。

在大多数情况下，并行作业会希望 Pod 在一定约束条件下运行， 比如所有的 Pod 都在同一个区域，或者所有的 Pod 都在 GPU 型号 x 或 y 上，而不是两者的混合。

suspend 字段是实现这些语义的第一步。 suspend 允许自定义队列控制器，以决定工作何时开始；然而，一旦工作被取消暂停， 自定义队列控制器对 Job 中 Pods 的实际放置位置没有影响。

此特性允许在 Job 开始之前更新调度指令，从而为定制队列提供影响 Pod 放置的能力，同时将 Pod 与节点间的分配关系留给 kube-scheduler 决定。 这一特性仅适用于之前从未被暂停过的、已暂停的 Job。 控制器能够影响 Pod 放置，同时参考实际 pod-to-node 分配给 kube-scheduler。这仅适用于从未暂停的 Jobs。

Job 的 Pod 模板中可以更新的字段是节点亲和性、节点选择器、容忍、标签和注解。

### 指定你自己的 Pod 选择算符

通常，当你创建一个 Job 对象时，你不会设置 ​`.spec.selector`​。 系统的默认值填充逻辑会在创建 Job 时添加此字段。 它会选择一个不会与任何其他 Job 重叠的选择算符设置。

不过，有些场合下，你可能需要重载这个自动设置的选择算符。 为了实现这点，你可以手动设置 Job 的 ​`spec.selector`​ 字段。

做这个操作时请务必小心。 如果你所设定的标签选择算符并不唯一针对 Job 对应的 Pod 集合，甚或该算符还能匹配 其他无关的 Pod，这些无关的 Job 的 Pod 可能会被删除。 或者当前 Job 会将另外一些 Pod 当作是完成自身工作的 Pods， 又或者两个 Job 之一或者二者同时都拒绝创建 Pod，无法运行至完成状态。 如果所设置的算符不具有唯一性，其他控制器（如 RC 副本控制器）及其所管理的 Pod 集合可能会变得行为不可预测。 Kubernetes 不会在你设置 ​`.spec.selector`​ 时尝试阻止你犯这类错误。

下面是一个示例场景，在这种场景下你可能会使用刚刚讲述的特性。

假定名为 ​`old` ​的 Job 已经处于运行状态。 你希望已有的 Pod 继续运行，但你希望 Job 接下来要创建的其他 Pod 使用一个不同的 Pod 模版，甚至希望 Job 的名字也发生变化。 你无法更新现有的 Job，因为这些字段都是不可更新的。 因此，你会删除 ​`old` ​Job，但 允许该 Job 的 Pod 集合继续运行。 这是通过 ​`kubectl delete jobs/old --cascade=orphan`​ 实现的。 在删除之前，我们先记下该 Job 所使用的选择算符。

`kubectl get job old -o yaml`

输出类似于：

`kind: Job metadata:   name: old   ... spec:   selector:     matchLabels:       controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002   ...`

接下来你会创建名为 ​`new` ​的新 Job，并显式地为其设置相同的选择算符。 由于现有 Pod 都具有标签 ​`controller-uid=a8f3d00d-c6d2-11e5-9f87-42010af00002`​， 它们也会被名为 ​`new` ​的 Job 所控制。

你需要在新 Job 中设置 ​`manualSelector: true`​，因为你并未使用系统通常自动为你 生成的选择算符。

`kind: Job metadata:   name: new   ... spec:   manualSelector: true   selector:     matchLabels:       controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002   ...`

新的 Job 自身会有一个不同于 ​`a8f3d00d-c6d2-11e5-9f87-42010af00002`​ 的唯一 ID。 设置 ​`manualSelector: true`​ 是在告诉系统你知道自己在干什么并要求系统允许这种不匹配 的存在。

### 使用 Finalizer 追踪 Job 

FEATURE STATE: Kubernetes v1.23 \[beta\]

> 要使用该行为，你必须为 API 服务器 和控制器管理器 启用 ​`JobTrackingWithFinalizers` ​特性门控。 默认是启用的。  
> 启用后，控制面基于下述行为追踪新的 Job。在启用该特性之前创建的 Job 不受影响。 作为用户，你会看到的唯一区别是控制面对 Job 完成情况的跟踪更加准确。

该功能未启用时，Job 控制器（Controller） 依靠计算集群中存在的 Pod 来跟踪作业状态。 也就是说，维持一个统计 ​`succeeded` ​和 ​`failed` ​的 Pod 的计数器。 然而，Pod 可以因为一些原因被移除，包括：

*   当一个节点宕机时，垃圾收集器会删除孤立（Orphan）Pod。
*   垃圾收集器在某个阈值后删除已完成的 Pod（处于 ​`Succeeded` ​或 ​`Failed` ​阶段）。
*   人工干预删除 Job 的 Pod。
*   一个外部控制器（不包含于 Kubernetes）来删除或取代 Pod。

如果你为你的集群启用了 ​​`JobTrackingWithFinalizers` ​​特性，控制面会跟踪属于任何 Job 的 Pod。 并注意是否有任何这样的 Pod 被从 API 服务器上删除。 为了实现这一点，Job 控制器创建的 Pod 带有 ​Finalizer ​`batch.kubernetes.io/job-tracking`​​。 控制器只有在 Pod 被记入 Job 状态后才会移除 Finalizer，允许 Pod 可以被其他控制器或用户删除。

Job 控制器只对新的 Job 使用新的算法。在启用该特性之前创建的 Job 不受影响。 你可以根据检查 Job 是否含有 ​`batch.kubernetes.io/job-tracking`​ 注解，来确定 Job 控制器是否正在使用 Pod Finalizer 追踪 Job。 你不应该给 Job 手动添加或删除该注解。

替代方案 
-----

### 裸 Pod 

当 Pod 运行所在的节点重启或者失败，Pod 会被终止并且不会被重启。 Job 会重新创建新的 Pod 来替代已终止的 Pod。 因为这个原因，我们建议你使用 Job 而不是独立的裸 Pod， 即使你的应用仅需要一个 Pod。

### 副本控制器 

Job 与副本控制器是彼此互补的。 副本控制器管理的是那些不希望被终止的 Pod （例如，Web 服务器）， Job 管理的是那些希望被终止的 Pod（例如，批处理作业）。

正如在 Pod 生命期 中讨论的， ​`Job` ​仅适合于 ​`restartPolicy` ​设置为 ​`OnFailure` ​或 ​`Never` ​的 Pod。 注意：如果 ​`restartPolicy` ​未设置，其默认值是 ​`Always`​。

### 单个 Job 启动控制器 Pod

另一种模式是用唯一的 Job 来创建 Pod，而该 Pod 负责启动其他 Pod，因此扮演了一种 后启动 Pod 的控制器的角色。 这种模式的灵活性更高，但是有时候可能会把事情搞得很复杂，很难入门， 并且与 Kubernetes 的集成度很低。

这种模式的实例之一是用 Job 来启动一个运行脚本的 Pod，脚本负责启动 Spark 主控制器， 运行 Spark 驱动，之后完成清理工作。

这种方法的优点之一是整个过程得到了 Job 对象的完成保障， 同时维持了对创建哪些 Pod、如何向其分派工作的完全控制能力。

##  6.  Kubernetes 已完成 Job 的自动清理
已完成 Job 的自动清理
-------------

FEATURE STATE: Kubernetes v1.23 \[stable\]

TTL-after-finished 控制器 提供了一种 TTL 机制来限制已完成执行的资源对象的生命周期。 TTL 控制器目前只处理 Job。

TTL-after-finished 控制器
----------------------

TTL-after-finished 控制器只支持 Job。集群操作员可以通过指定 Job 的 ​`.spec.ttlSecondsAfterFinished`​ 字段来自动清理已结束的作业（​`Complete` ​或 ​`Failed`​）。

TTL-after-finished 控制器假设作业能在执行完成后的 TTL 秒内被清理，也就是当 TTL 过期后。 当 TTL 控制器清理作业时，它将做级联删除操作，即删除资源对象的同时也删除其依赖对象。 注意，当资源被删除时，由该资源的生命周期保证其终结器（Finalizers）等被执行。

可以随时设置 TTL 秒。以下是设置 Job 的 ​`.spec.ttlSecondsAfterFinished`​ 字段的一些示例：

*   在作业清单（manifest）中指定此字段，以便 Job 在完成后的某个时间被自动清除。
*   将此字段设置为现有的、已完成的作业，以采用此新功能。
*   在创建作业时使用 mutating admission webhook 动态设置该字段。集群管理员可以使用它对完成的作业强制执行 TTL 策略。
*   使用 mutating admission webhook 在作业完成后动态设置该字段，并根据作业状态、标签等选择不同的 TTL 值。

警告
--

### 更新 TTL 秒数

请注意，在创建 Job 或已经执行结束后，仍可以修改其 TTL 周期，例如 Job 的 ​`.spec.ttlSecondsAfterFinished`​ 字段。 但是一旦 Job 变为可被删除状态（当其 TTL 已过期时），即使您通过 API 增加其 TTL 时长得到了成功的响应，系统也不保证 Job 将被保留。

### 时间偏差 

由于 TTL-after-finished 控制器使用存储在 Kubernetes 资源中的时间戳来确定 TTL 是否已过期， 因此该功能对集群中的时间偏差很敏感，这可能导致 TTL-after-finished 控制器在错误的时间清理资源对象。

时钟并不总是如此正确，但差异应该很小。 设置非零 TTL 时请注意避免这种风险。

##  7.  Kubernetes CronJob
CronJob
-------

CronJob 用于执行周期性的动作，例如备份、报告生成等。 这些任务中的每一个都应该配置为周期性重复的（例如：每天/每周/每月一次）； 你可以定义任务开始执行的时间间隔。

FEATURE STATE: Kubernetes v1.21 \[stable\]

CronJob 创建基于时隔重复调度的 Jobs。

一个 CronJob 对象就像 crontab (cron table) 文件中的一行。 它用 Cron 格式进行编写， 并周期性地在给定的调度时间执行 Job。

> 所有 CronJob 的 ​`schedule`​: 时间都是基于 kube-controller-manager. 的时区。  
> 如果你的控制平面在 Pod 或是裸容器中运行了 kube-controller-manager， 那么为该容器所设置的时区将会决定 Cron Job 的控制器所使用的时区。

> Kubernetes 项目官方并不支持设置如 ​`CRON_TZ` ​或者 ​`TZ` ​等变量。 ​`CRON_TZ` ​或者 ​`TZ` ​是用于解析和计算下一个 Job 创建时间所使用的内部库中一个实现细节。 不建议在生产集群中使用它。

为 CronJob 资源创建清单时，请确保所提供的名称是一个合法的 DNS 子域名. 名称不能超过 52 个字符。 这是因为 CronJob 控制器将自动在提供的 Job 名称后附加 11 个字符，并且存在一个限制， 即 Job 名称的最大长度不能超过 63 个字符。

### 示例

下面的 CronJob 示例清单会在每分钟打印出当前时间和问候消息：

`apiVersion: batch/v1 kind: CronJob metadata:   name: hello spec:   schedule: "* * * * *"   jobTemplate:     spec:       template:         spec:           containers:           - name: hello             image: busybox:1.28             imagePullPolicy: IfNotPresent             command:             - /bin/sh             - -c             - date; echo Hello from the Kubernetes cluster           restartPolicy: OnFailure`

### Cron 时间表语法

`# ┌───────────── 分钟 (0 - 59) # │ ┌───────────── 小时 (0 - 23) # │ │ ┌───────────── 月的某天 (1 - 31) # │ │ │ ┌───────────── 月份 (1 - 12) # │ │ │ │ ┌───────────── 周的某天 (0 - 6)（周日到周一；在某些系统上，7 也是星期日） # │ │ │ │ │                          或者是 sun，mon，tue，web，thu，fri，sat # │ │ │ │ │ # │ │ │ │ │ # * * * * *`

输入

描述

相当于

@yearly (or @annually)

每年 1 月 1 日的午夜运行一次

0 0 1 1 \*

@monthly

每月第一天的午夜运行一次

0 0 1 \* \*

@weekly

每周的周日午夜运行一次

0 0 \* \* 0

@daily (or @midnight)

每天午夜运行一次

0 0 \* \* \*

@hourly

每小时的开始一次

0 \* \* \* \*

例如，下面这行指出必须在每个星期五的午夜以及每个月 13 号的午夜开始任务：

​`0 0 13 * 5` ​

要生成 CronJob 时间表表达式，你还可以使用 [crontab.guru](https://crontab.guru/) 之类的 Web 工具。

CronJob 限制 
-----------

CronJob 根据其计划编排，在每次该执行任务的时候大约会创建一个 Job。 我们之所以说 "大约"，是因为在某些情况下，可能会创建两个 Job，或者不会创建任何 Job。 我们试图使这些情况尽量少发生，但不能完全杜绝。因此，Job 应该是 幂等的。

如果 ​`startingDeadlineSeconds` ​设置为很大的数值或未设置（默认），并且 ​`concurrencyPolicy` ​设置为 ​`Allow`​，则作业将始终至少运行一次。

> 如果 ​`startingDeadlineSeconds` ​的设置值低于 10 秒钟，CronJob 可能无法被调度。 这是因为 CronJob 控制器每 10 秒钟执行一次检查。

对于每个 CronJob，CronJob 控制器（Controller） 检查从上一次调度的时间点到现在所错过了调度次数。如果错过的调度次数超过 100 次， 那么它就不会启动这个任务，并记录这个错误:

`Cannot determine if job needs to be started. Too many missed start time (> 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew.`

需要注意的是，如果 ​`startingDeadlineSeconds` ​字段非空，则控制器会统计从 ​`startingDeadlineSeconds` ​设置的值到现在而不是从上一个计划时间到现在错过了多少次 Job。 例如，如果 ​`startingDeadlineSeconds` ​是 ​`200`​，则控制器会统计在过去 200 秒中错过了多少次 Job。

如果未能在调度时间内创建 CronJob，则计为错过。 例如，如果 ​`concurrencyPolicy` ​被设置为 ​`Forbid`​，并且当前有一个调度仍在运行的情况下， 试图调度的 CronJob 将被计算为错过。

例如，假设一个 CronJob 被设置为从 ​`08:30:00`​ 开始每隔一分钟创建一个新的 Job， 并且它的 ​`startingDeadlineSeconds` ​字段未被设置。如果 CronJob 控制器从 ​`08:29:00`​ 到 ​`10:21:00`​ 终止运行，则该 Job 将不会启动，因为其错过的调度 次数超过了 100。

为了进一步阐述这个概念，假设将 CronJob 设置为从 ​`08:30:00`​ 开始每隔一分钟创建一个新的 Job， 并将其 ​`startingDeadlineSeconds` ​字段设置为 200 秒。 如果 CronJob 控制器恰好在与上一个示例相同的时间段（​`08:29:00`​ 到 ​`10:21:00`​）终止运行， 则 Job 仍将从 ​`10:22:00`​ 开始。 造成这种情况的原因是控制器现在检查在最近 200 秒（即 3 个错过的调度）中发生了多少次错过的 Job 调度，而不是从现在为止的最后一个调度时间开始。

CronJob 仅负责创建与其调度时间相匹配的 Job，而 Job 又负责管理其代表的 Pod。

控制器版本 
------

从 Kubernetes v1.21 版本开始，CronJob 控制器的第二个版本被用作默认实现。 要禁用此默认 CronJob 控制器而使用原来的 CronJob 控制器，请在 kube-controller-manager 中设置特性门控 ​`CronJobControllerV2`​，将此标志设置为 ​`false`​。例如：

`--feature-gates="CronJobControllerV2=false"`

##  8.  Kubernetes ReplicationController
ReplicationController
---------------------

> 现在推荐使用配置 ReplicaSet 的 Deployment 来建立副本管理机制。

ReplicationController 确保在任何时候都有特定数量的 Pod 副本处于运行状态。 换句话说，ReplicationController 确保一个 Pod 或一组同类的 Pod 总是可用的。

ReplicationController 如何工作 
---------------------------

当 Pod 数量过多时，ReplicationController 会终止多余的 Pod。当 Pod 数量太少时，ReplicationController 将会启动新的 Pod。 与手动创建的 Pod 不同，由 ReplicationController 创建的 Pod 在失败、被删除或被终止时会被自动替换。 例如，在中断性维护（如内核升级）之后，你的 Pod 会在节点上重新创建。 因此，即使你的应用程序只需要一个 Pod，你也应该使用 ReplicationController 创建 Pod。 ReplicationController 类似于进程管理器，但是 ReplicationController 不是监控单个节点上的单个进程，而是监控跨多个节点的多个 Pod。

在讨论中，ReplicationController 通常缩写为 "rc"，并作为 kubectl 命令的快捷方式。

一个简单的示例是创建一个 ReplicationController 对象来可靠地无限期地运行 Pod 的一个实例。 更复杂的用例是运行一个多副本服务（如 web 服务器）的若干相同副本。

运行一个示例 ReplicationController 
-----------------------------

这个示例 ReplicationController 配置运行 nginx Web 服务器的三个副本。

`apiVersion: v1 kind: ReplicationController metadata:   name: nginx spec:   replicas: 3   selector:     app: nginx   template:     metadata:       name: nginx       labels:         app: nginx     spec:       containers:       - name: nginx         image: nginx         ports:         - containerPort: 80`

通过下载示例文件并运行以下命令来运行示例任务:

`kubectl apply -f https://k8s.io/examples/controllers/replication.yaml`

输出类似于：

`replicationcontroller/nginx created`

使用以下命令检查 ReplicationController 的状态:

`kubectl describe replicationcontrollers/nginx`

输出类似于：

`Name:        nginx Namespace:   default Selector:    app=nginx Labels:      app=nginx Annotations:    <none> Replicas:    3 current / 3 desired Pods Status: 0 Running / 3 Waiting / 0 Succeeded / 0 Failed Pod Template:   Labels:       app=nginx   Containers:    nginx:     Image:              nginx     Port:               80/TCP     Environment:        <none>     Mounts:             <none>   Volumes:              <none> Events:   FirstSeen       LastSeen     Count    From                        SubobjectPath    Type      Reason              Message   ---------       --------     -----    ----                        -------------    ----      ------              -------   20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-qrm3m   20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-3ntk0   20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-4ok8v`

在这里，创建了三个 Pod，但没有一个 Pod 正在运行，这可能是因为正在拉取镜像。 稍后，相同的命令可能会显示：

`Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed`

要以机器可读的形式列出属于 ReplicationController 的所有 Pod，可以使用如下命令：

`pods=$(kubectl get pods --selector=app=nginx --output=jsonpath={.items..metadata.name}) echo $pods`

输出类似于：

`nginx-3ntk0 nginx-4ok8v nginx-qrm3m`

这里，选择算符与 ReplicationController 的选择算符相同，并以不同的形式出现在 ​`replication.yaml`​ 中。 ​`--output=jsonpath`​ 选项指定了一个表达式，仅从返回列表中的每个 Pod 中获取名称。

编写一个 ReplicationController 规约
-----------------------------

与所有其它 Kubernetes 配置一样，ReplicationController 需要 ​`apiVersion`​、 ​`kind` ​和 ​`metadata` ​字段。

ReplicationController 也需要一个 ​`[.spec](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md target=)`​ 部分。

### Pod 模板 

​`.spec.template`​ 是 ​`.spec`​ 的唯一必需字段。

​`.spec.template`​ 是一个 Pod 模板。 它的模式与 Pod 完全相同，只是它是嵌套的，没有 ​`apiVersion` ​或 ​`kind` ​属性。

除了 Pod 所需的字段外，ReplicationController 中的 Pod 模板必须指定适当的标签和适当的重新启动策略。 对于标签，请确保不与其他控制器重叠。

只允许 ​`.spec.template.spec.restartPolicy`​ 等于 ​`Always`​，如果没有指定，这是默认值。

对于本地容器重启，ReplicationController 委托给节点上的代理， 例如 Kubelet 或 Docker。

### ReplicationController 上的标签 

ReplicationController 本身可以有标签 （​`.metadata.labels`​）。 通常，你可以将这些设置为 ​`.spec.template.metadata.labels`​； 如果没有指定 ​`.metadata.labels`​ 那么它默认为 ​`.spec.template.metadata.labels`​。

但是，Kubernetes 允许它们是不同的，​`.metadata.labels`​ 不会影响 ReplicationController 的行为。

### Pod 选择算符

​`.spec.selector`​ 字段是一个标签选择算符。 ReplicationController 管理标签与选择算符匹配的所有 Pod。 它不区分它创建或删除的 Pod 和其他人或进程创建或删除的 Pod。 这允许在不影响正在运行的 Pod 的情况下替换 ReplicationController。

如果指定了 ​`.spec.template.metadata.labels`​，它必须和 ​`.spec.selector`​ 相同，否则它将被 API 拒绝。 如果没有指定 ​`.spec.selector`​，它将默认为 ​`.spec.template.metadata.labels`​。

另外，通常不应直接使用另一个 ReplicationController 或另一个控制器（例如 Job） 来创建其标签与该选择算符匹配的任何 Pod。如果这样做，ReplicationController 会认为它创建了这些 Pod。 Kubernetes 并没有阻止你这样做。

如果你的确创建了多个控制器并且其选择算符之间存在重叠，那么你将不得不自己管理删除操作。

### 多个副本

你可以通过设置 ​`.spec.replicas`​ 来指定应该同时运行多少个 Pod。 在任何时候，处于运行状态的 Pod 个数都可能高于或者低于设定值。例如，副本个数刚刚被增加或减少时，或者一个 Pod 处于优雅终止过程中而其替代副本已经提前开始创建时。

如果你没有指定 ​`.spec.replicas`​ ，那么它默认是 1。

使用 ReplicationController
------------------------

### 删除一个 ReplicationController 以及它的 Pod

要删除一个 ReplicationController 以及它的 Pod，使用 ​`[kubectl delete](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands target=)`​。 kubectl 将 ReplicationController 缩放为 0 并等待以便在删除 ReplicationController 本身之前删除每个 Pod。 如果这个 kubectl 命令被中断，可以重新启动它。

当使用 REST API 或客户端库时，你需要明确地执行这些步骤（缩放副本为 0、 等待 Pod 删除，之后删除 ReplicationController 资源）。

### 只删除 ReplicationController

你可以删除一个 ReplicationController 而不影响它的任何 Pod。

使用 kubectl，为 ​`[kubectl delete](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands target=)`​ 指定 ​`--cascade=orphan`​ 选项。

当使用 REST API 或客户端库(/zh/docs/reference/using-api/client-libraries)时，只需删除 ReplicationController 对象。

一旦原始对象被删除，你可以创建一个新的 ReplicationController 来替换它。 只要新的和旧的 ​`.spec.selector`​ 相同，那么新的控制器将领养旧的 Pod。 但是，它不会做出任何努力使现有的 Pod 匹配新的、不同的 Pod 模板。 如果希望以受控方式更新 Pod 以使用新的 spec，请执行滚动更新操作。

### 从 ReplicationController 中隔离 Pod

通过更改 Pod 的标签，可以从 ReplicationController 的目标中删除 Pod。 此技术可用于从服务中删除 Pod 以进行调试、数据恢复等。以这种方式删除的 Pod 将被自动替换（假设复制副本的数量也没有更改）。

常见的使用模式
-------

### 重新调度

如上所述，无论你想要继续运行 1 个 Pod 还是 1000 个 Pod，一个 ReplicationController 都将确保存在指定数量的 Pod，即使在节点故障或 Pod 终止(例如，由于另一个控制代理的操作)的情况下也是如此。

### 扩缩容 

通过设置 ​`replicas` ​字段，ReplicationController 可以允许扩容或缩容副本的数量。 你可以手动或通过自动缩放控制代理来控制 ReplicationController 执行此操作。

### 滚动更新

ReplicationController 的设计目的是通过逐个替换 Pod 以方便滚动更新服务。

如 [#1353](https://github.com/kubernetes/kubernetes/issues/1353) PR 中所述，建议的方法是使用 1 个副本创建一个新的 ReplicationController， 逐个扩容新的（+1）和缩容旧的（-1）控制器，然后在旧的控制器达到 0 个副本后将其删除。 这一方法能够实现可控的 Pod 集合更新，即使存在意外失效的状况。

理想情况下，滚动更新控制器将考虑应用程序的就绪情况，并确保在任何给定时间都有足够数量的 Pod 有效地提供服务。

这两个 ReplicationController 将需要创建至少具有一个不同标签的 Pod，比如 Pod 主要容器的镜像标签，因为通常是镜像更新触发滚动更新。

滚动更新是在客户端工具 ​`[kubectl rolling-update](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands target=)`​ 中实现的。访问 kubectl rolling-update 任务以获得更多的具体示例。

### 多个版本跟踪

除了在滚动更新过程中运行应用程序的多个版本之外，通常还会使用多个版本跟踪来长时间， 甚至持续运行多个版本。这些跟踪将根据标签加以区分。

例如，一个服务可能把具有 ​`tier in (frontend), environment in (prod)`​ 的所有 Pod 作为目标。 现在假设你有 10 个副本的 Pod 组成了这个层。但是你希望能够 ​`canary` ​（金丝雀）发布这个组件的新版本。 你可以为大部分副本设置一个 ReplicationController，其中 ​`replicas` ​设置为 9， 标签为 ​`tier=frontend, environment=prod, track=stable`​ 而为 ​`canary` ​设置另一个 ReplicationController，其中 ​`replicas` ​设置为 1， 标签为 ​`tier=frontend, environment=prod, track=canary`​。 现在这个服务覆盖了 ​`canary` ​和非 ​`canary` ​Pod。但你可以单独处理 ReplicationController，以测试、监控结果等。

### 和服务一起使用 ReplicationController

多个 ReplicationController 可以位于一个服务的后面，例如，一部分流量流向旧版本， 一部分流量流向新版本。

一个 ReplicationController 永远不会自行终止，但它不会像服务那样长时间存活。 服务可以由多个 ReplicationController 控制的 Pod 组成，并且在服务的生命周期内 （例如，为了执行 Pod 更新而运行服务），可以创建和销毁许多 ReplicationController。 服务本身和它们的客户端都应该忽略负责维护服务 Pod 的 ReplicationController 的存在。

编写多副本的应用
--------

由 ReplicationController 创建的 Pod 是可替换的，语义上是相同的， 尽管随着时间的推移，它们的配置可能会变得异构。 这显然适合于多副本的无状态服务器，但是 ReplicationController 也可以用于维护主选、 分片和工作池应用程序的可用性。 这样的应用程序应该使用动态的工作分配机制，例如 RabbitMQ 工作队列， 而不是静态的或者一次性定制每个 Pod 的配置，这被认为是一种反模式。 执行的任何 Pod 定制，例如资源的垂直自动调整大小（例如，CPU 或内存）， 都应该由另一个在线控制器进程执行，这与 ReplicationController 本身没什么不同。

ReplicationController 的职责
-------------------------

ReplicationController 仅确保所需的 Pod 数量与其标签选择算符匹配，并且是可操作的。 目前，它的计数中只排除终止的 Pod。 未来，可能会考虑系统提供的[就绪状态](https://github.com/kubernetes/kubernetes/issues/620)和其他信息， 我们可能会对替换策略添加更多控制， 我们计划发出事件，这些事件可以被外部客户端用来实现任意复杂的替换和/或缩减策略。

ReplicationController 永远被限制在这个狭隘的职责范围内。 它本身既不执行就绪态探测，也不执行活跃性探测。 它不负责执行自动缩放，而是由外部自动缩放器控制（如 [#492](https://github.com/kubernetes/kubernetes/issues/492) 中所述），后者负责更改其 replicas 字段值。 我们不会向 ReplicationController 添加调度策略(例如， [spreading](https://github.com/kubernetes/kubernetes/issues/367 target=))。 它也不应该验证所控制的 Pod 是否与当前指定的模板匹配，因为这会阻碍自动调整大小和其他自动化过程。 类似地，完成期限、整理依赖关系、配置扩展和其他特性也属于其他地方。 我们甚至计划考虑批量创建 Pod 的机制（查阅 [#170](https://github.com/kubernetes/kubernetes/issues/170)）。

ReplicationController 旨在成为可组合的构建基元。 我们希望在它和其他补充原语的基础上构建更高级别的 API 或者工具，以便于将来的用户使用。 kubectl 目前支持的 "macro" 操作（运行、缩放、滚动更新）就是这方面的概念示例。 例如，我们可以想象类似于 [Asgard](https://netflixtechblog.com/asgard-web-based-cloud-management-and-deployment-2c9fc4e4d3a1) 的东西管理 ReplicationController、自动定标器、服务、调度策略、金丝雀发布等。

API 对象
------

在 Kubernetes REST API 中 Replication controller 是顶级资源。 更多关于 API 对象的详细信息可以在 [ReplicationController API 对象](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/ target=)找到

ReplicationController 的替代方案
---------------------------

### ReplicaSet

​`ReplicaSet` ​是下一代 ReplicationController， 支持新的基于集合的标签选择算符。 它主要被 ​`Deployment` ​用来作为一种编排 Pod 创建、删除及更新的机制。 请注意，我们推荐使用 Deployment 而不是直接使用 ReplicaSet，除非 你需要自定义更新编排或根本不需要更新。

### Deployment （推荐） 

​`Deployment` ​是一种更高级别的 API 对象，用于更新其底层 ReplicaSet 及其 Pod。 如果你想要这种滚动更新功能，那么推荐使用 Deployment，因为它们是声明式的、服务端的，并且具有其它特性。

### 裸 Pod

与用户直接创建 Pod 的情况不同，ReplicationController 能够替换因某些原因 被删除或被终止的 Pod ，例如在节点故障或中断节点维护的情况下，例如内核升级。 因此，我们建议你使用 ReplicationController，即使你的应用程序只需要一个 Pod。 可以将其看作类似于进程管理器，它只管理跨多个节点的多个 Pod ，而不是单个节点上的单个进程。 ReplicationController 将本地容器重启委托给节点上的某个代理(例如，Kubelet 或 Docker)。

### Job

对于预期会自行终止的 Pod (即批处理任务)，使用 ​`Job` ​而不是 ReplicationController。

### DaemonSet 

对于提供机器级功能（例如机器监控或机器日志记录）的 Pod， 使用 ​`DaemonSet` ​而不是 ReplicationController。 这些 Pod 的生命期与机器的生命期绑定：它们需要在其他 Pod 启动之前在机器上运行， 并且在机器准备重新启动或者关闭时安全地终止。

#  9.  Kubernetes 服务、负载均衡和联网

##  1.  Kubernetes 使用拓扑键实现拓扑感知的流量路由
使用拓扑键实现拓扑感知的流量路由
----------------

FEATURE STATE: Kubernetes v1.21 \[deprecated\]

> 此功能特性，尤其是 Alpha 阶段的 ​`topologyKeys` ​API，在 Kubernetes v1.21 版本中已被废弃。Kubernetes v1.21 版本中引入的 拓扑感知的提示, 提供类似的功能。

服务拓扑（Service Topology）可以让一个服务基于集群的 Node 拓扑进行流量路由。 例如，一个服务可以指定流量是被优先路由到一个和客户端在同一个 Node 或者在同一可用区域的端点。

拓扑感知的流量路由
---------

默认情况下，发往 ​`ClusterIP` ​或者 ​`NodePort` ​服务的流量可能会被路由到 服务的任一后端的地址。Kubernetes 1.7 允许将“外部”流量路由到接收到流量的 节点上的 Pod。对于 ​`ClusterIP` ​服务，无法完成同节点优先的路由，你也无法 配置集群优选路由到同一可用区中的端点。 通过在 Service 上配置 ​`topologyKeys`​，你可以基于来源节点和目标节点的 标签来定义流量路由策略。

通过对源和目的之间的标签匹配，作为集群操作者的你可以根据节点间彼此“较近”和“较远” 来定义节点集合。你可以基于符合自身需求的任何度量值来定义标签。 例如，在公有云上，你可能更偏向于把流量控制在同一区内，因为区间流量是有费用成本的， 而区内流量则没有。 其它常见需求还包括把流量路由到由 ​`DaemonSet` ​管理的本地 Pod 上，或者 把将流量转发到连接在同一机架交换机的节点上，以获得低延时。

使用服务拓扑
------

如果集群启用了 ​`ServiceTopology` ​特性门控， 你就可以在 Service 规约中设定 ​`topologyKeys` ​字段，从而控制其流量路由。 此字段是 ​`Node` ​标签的优先顺序字段，将用于在访问这个 ​`Service` ​时对端点进行排序。 流量会被定向到第一个标签值和源 ​`Node` ​标签值相匹配的 ​`Node`​。 如果这个 ​`Service` ​没有匹配的后端 ​`Node`​，那么第二个标签会被使用做匹配， 以此类推，直到没有标签。

如果没有匹配到，流量会被拒绝，就如同这个 ​`Service` ​根本没有后端。 换言之，系统根据可用后端的第一个拓扑键来选择端点。 如果这个字段被配置了而没有后端可以匹配客户端拓扑，那么这个 ​`Service` ​对那个客户端是没有后端的，链接应该是失败的。 这个字段配置为 ​`"*"`​ 意味着任意拓扑。 这个通配符值如果使用了，那么只有作为配置值列表中的最后一个才有用。

如果 ​`topologyKeys` ​没有指定或者为空，就没有启用这个拓扑约束。

一个集群中，其 ​`Node` ​的标签被打为其主机名，区域名和地区名。 那么就可以设置 ​`Service` ​的 ​`topologyKeys` ​的值，像下面的做法一样定向流量了。

*   只定向到同一个 ​`Node` ​上的端点，​`Node` ​上没有端点存在时就失败： 配置 \[​`"kubernetes.io/hostname"`​\]。
*   偏向定向到同一个 ​`Node` ​上的端点，回退同一区域的端点上，然后是同一地区， 其它情况下就失败：配置 \[​`​"kubernetes.io/hostname"​, "topology.kubernetes.io/zone", "topology.kubernetes.io/region"`​\]。 这或许很有用，例如，数据局部性很重要的情况下。
*   偏向于同一区域，但如果此区域中没有可用的终结点，则回退到任何可用的终结点： 配置 \[​`"topology.kubernetes.io/zone", "*"`​\]。

约束条件
----

*   服务拓扑和 ​`externalTrafficPolicy=Local`​ 是不兼容的，所以 ​`Service` ​不能同时使用这两种特性。 但是在同一个集群的不同 ​`Service` ​上是可以分别使用这两种特性的，只要不在同一个 ​`Service` ​上就可以。
*   有效的拓扑键目前只有：​`kubernetes.io/hostname`​、​`topology.kubernetes.io/zone`​ 和 ​`topology.kubernetes.io/region`​，但是未来会推广到其它的 ​`Node` ​标签。
*   拓扑键必须是有效的标签，并且最多指定16个。
*   通配符：​`"*"`​，如果要用，则必须是拓扑键值的最后一个值。

示例
--

以下是使用服务拓扑功能的常见示例。

### 仅节点本地端点 

仅路由到节点本地端点的一种服务。如果节点上不存在端点，流量则被丢弃：

`apiVersion: v1 kind: Service metadata:   name: my-service spec:   selector:     app: my-app   ports:     - protocol: TCP       port: 80       targetPort: 9376   topologyKeys:     - "kubernetes.io/hostname"`

### 首选节点本地端点

首选节点本地端点，如果节点本地端点不存在，则回退到集群范围端点的一种服务：

`apiVersion: v1 kind: Service metadata:   name: my-service spec:   selector:     app: my-app   ports:     - protocol: TCP       port: 80       targetPort: 9376   topologyKeys:     - "kubernetes.io/hostname"     - "*"`

### 仅地域或区域端点

首选地域端点而不是区域端点的一种服务。 如果以上两种范围内均不存在端点， 流量则被丢弃。

`apiVersion: v1 kind: Service metadata:   name: my-service spec:   selector:     app: my-app   ports:     - protocol: TCP       port: 80       targetPort: 9376   topologyKeys:     - "topology.kubernetes.io/zone"     - "topology.kubernetes.io/region"`

### 优先选择节点本地端点、地域端点，然后是区域端点

优先选择节点本地端点，地域端点，然后是区域端点，最后才是集群范围端点的 一种服务。

`apiVersion: v1 kind: Service metadata:   name: my-service spec:   selector:     app: my-app   ports:     - protocol: TCP       port: 80       targetPort: 9376   topologyKeys:     - "kubernetes.io/hostname"     - "topology.kubernetes.io/zone"     - "topology.kubernetes.io/region"     - "*"`

##  2.  Kubernetes 服务
服务
--

将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。

使用 Kubernetes，你无需修改应用程序即可使用不熟悉的服务发现机制。 Kubernetes 为 Pods 提供自己的 IP 地址，并为一组 Pod 提供相同的 DNS 名， 并且可以在它们之间进行负载均衡。

动机
--

创建和销毁 Kubernetes Pod 以匹配集群状态。 Pod 是非永久性资源。 如果你使用 Deployment 来运行你的应用程序，则它可以动态创建和销毁 Pod。

每个 Pod 都有自己的 IP 地址，但是在 Deployment 中，在同一时刻运行的 Pod 集合可能与稍后运行该应用程序的 Pod 集合不同。

这导致了一个问题： 如果一组 Pod（称为“后端”）为集群内的其他 Pod（称为“前端”）提供功能， 那么前端如何找出并跟踪要连接的 IP 地址，以便前端可以使用提供工作负载的后端部分？

进入 Services。

Service 资源
----------

Kubernetes Service 定义了这样一种抽象：逻辑上的一组 Pod，一种可以访问它们的策略 —— 通常称为微服务。 Service 所针对的 Pods 集合通常是通过选择算符来确定的。 

举个例子，考虑一个图片处理后端，它运行了 3 个副本。这些副本是可互换的 —— 前端不需要关心它们调用了哪个后端副本。 然而组成这一组后端程序的 Pod 实际上可能会发生变化， 前端客户端不应该也没必要知道，而且也不需要跟踪这一组后端的状态。

Service 定义的抽象能够解耦这种关联。

### 云原生服务发现

如果你想要在应用程序中使用 Kubernetes API 进行服务发现，则可以查询 API 服务器 的 Endpoints 资源，只要服务中的 Pod 集合发生更改，Endpoints 就会被更新。

对于非本机应用程序，Kubernetes 提供了在应用程序和后端 Pod 之间放置网络端口或负载均衡器的方法。

定义 Service
----------

Service 在 Kubernetes 中是一个 REST 对象，和 Pod 类似。 像所有的 REST 对象一样，Service 定义可以基于 POST 方式，请求 API server 创建新的实例。 Service 对象的名称必须是合法的 RFC 1035 标签名称.。

例如，假定有一组 Pod，它们对外暴露了 9376 端口，同时还被打上 ​`app=MyApp`​ 标签：

`apiVersion: v1 kind: Service metadata:   name: my-service spec:   selector:     app: MyApp   ports:     - protocol: TCP       port: 80       targetPort: 9376`

上述配置创建一个名称为 "my-service" 的 Service 对象，它会将请求代理到使用 TCP 端口 9376，并且具有标签 ​`"app=MyApp"`​ 的 Pod 上。

Kubernetes 为该服务分配一个 IP 地址（有时称为 "集群IP"），该 IP 地址由服务代理使用。

服务选择算符的控制器不断扫描与其选择器匹配的 Pod，然后将所有更新发布到也称为 “my-service” 的 Endpoint 对象。

> 需要注意的是，Service 能够将一个接收 ​`port` ​映射到任意的 ​`targetPort`​。 默认情况下，​`targetPort` ​将被设置为与 ​`port` ​字段相同的值。

Pod 中的端口定义是有名字的，你可以在服务的 ​`targetPort` ​属性中引用这些名称。 即使服务中使用单个配置的名称混合使用 Pod，并且通过不同的端口号提供相同的网络协议，此功能也可以使用。 这为部署和发展服务提供了很大的灵活性。 例如，你可以更改 Pods 在新版本的后端软件中公开的端口号，而不会破坏客户端。

服务的默认协议是 TCP；你还可以使用任何其他受支持的协议。

由于许多服务需要公开多个端口，因此 Kubernetes 在服务对象上支持多个端口定义。 每个端口定义可以具有相同的 ​`protocol`​，也可以具有不同的协议。

### 没有选择算符的 Service 

服务最常见的是抽象化对 Kubernetes Pod 的访问，但是它们也可以抽象化其他种类的后端。 实例:

*   希望在生产环境中使用外部的数据库集群，但测试环境使用自己的数据库。
*   希望服务指向另一个 名字空间（Namespace） 中或其它集群中的服务。
*   你正在将工作负载迁移到 Kubernetes。 在评估该方法时，你仅在 Kubernetes 中运行一部分后端。

在任何这些场景中，都能够定义没有选择算符的 Service。 实例:

`apiVersion: v1 kind: Service metadata:   name: my-service spec:   ports:     - protocol: TCP       port: 80       targetPort: 9376`

由于此服务没有选择算符，因此不会自动创建相应的 Endpoint 对象。 你可以通过手动添加 Endpoint 对象，将服务手动映射到运行该服务的网络地址和端口：

`apiVersion: v1 kind: Endpoints metadata:   name: my-service subsets:   - addresses:       - ip: 192.0.2.42     ports:       - port: 9376`

Endpoints 对象的名称必须是合法的 DNS 子域名。

> 端点 IPs 必须不可以 是：本地回路（IPv4 的 127.0.0.0/8, IPv6 的 ::1/128）或 本地链接（IPv4 的 169.254.0.0/16 和 224.0.0.0/24，IPv6 的 fe80::/64)。  
> 端点 IP 地址不能是其他 Kubernetes 服务的集群 IP，因为 kube-proxy 不支持将虚拟 IP 作为目标。

访问没有选择算符的 Service，与有选择算符的 Service 的原理相同。 请求将被路由到用户定义的 Endpoint，YAML 中为：​`192.0.2.42:9376`​（TCP）。

ExternalName Service 是 Service 的特例，它没有选择算符，但是使用 DNS 名称。

### 超出容量的 Endpoints 

如果某个 Endpoints 资源中包含的端点个数超过 1000，则 Kubernetes v1.22 版本 （及更新版本）的集群会将为该 Endpoints 添加注解 ​`endpoints.kubernetes.io/over-capacity: truncated`​。 这一注解表明所影响到的 Endpoints 对象已经超出容量，此外 Endpoints 控制器还会将 Endpoints 对象数量截断到 1000。

### EndpointSlices

FEATURE STATE: Kubernetes v1.21 \[stable\]

EndpointSlices 是一种 API 资源，可以为 Endpoints 提供更可扩展的替代方案。 尽管从概念上讲与 Endpoints 非常相似，但 EndpointSlices 允许跨多个资源分布网络端点。 默认情况下，一旦到达 100 个 Endpoint，该 EndpointSlice 将被视为“已满”， 届时将创建其他 EndpointSlices 来存储任何其他 Endpoints。

EndpointSlices 提供了附加的属性和功能，这些属性和功能在 EndpointSlices 中有详细描述。

### 应用协议 

FEATURE STATE: Kubernetes v1.20 \[stable\]

​`appProtocol` ​字段提供了一种为每个 Service 端口指定应用协议的方式。 此字段的取值会被映射到对应的 Endpoints 和 EndpointSlices 对象。

该字段遵循标准的 Kubernetes 标签语法。 其值可以是 [IANA 标准服务名称](https://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.xhtml) 或以域名为前缀的名称，如 ​`mycompany.com/my-custom-protocol`​。

虚拟 IP 和 Service 代理
------------------

在 Kubernetes 集群中，每个 Node 运行一个 ​`kube-proxy`​ 进程。 ​`kube-proxy`​ 负责为 Service 实现了一种 VIP（虚拟 IP）的形式，而不是 ​`ExternalName` ​的形式。

### 为什么不使用 DNS 轮询？

时不时会有人问到为什么 Kubernetes 依赖代理将入站流量转发到后端。那其他方法呢？ 例如，是否可以配置具有多个 A 值（或 IPv6 为 AAAA）的 DNS 记录，并依靠轮询名称解析？

使用服务代理有以下几个原因：

*   DNS 实现的历史由来已久，它不遵守记录 TTL，并且在名称查找结果到期后对其进行缓存。
*   有些应用程序仅执行一次 DNS 查找，并无限期地缓存结果。
*   即使应用和库进行了适当的重新解析，DNS 记录上的 TTL 值低或为零也可能会给 DNS 带来高负载，从而使管理变得困难。

### userspace 代理模式

这种模式，kube-proxy 会监视 Kubernetes 控制平面对 Service 对象和 Endpoints 对象的添加和移除操作。 对每个 Service，它会在本地 Node 上打开一个端口（随机选择）。 任何连接到“代理端口”的请求，都会被代理到 Service 的后端 ​`Pods` ​中的某个上面（如 ​`Endpoints` ​所报告的一样）。 使用哪个后端 Pod，是 kube-proxy 基于 ​`SessionAffinity` ​来确定的。

最后，它配置 iptables 规则，捕获到达该 Service 的 ​`clusterIP`​（是虚拟 IP） 和 ​`Port` ​的请求，并重定向到代理端口，代理端口再代理请求到后端Pod。

默认情况下，用户空间模式下的 kube-proxy 通过轮转算法选择后端。

![](https://atts.w3cschool.cn/attachments/image/20220506/1651803046551067.svg)  

### iptables 代理模式

这种模式，​`kube-proxy`​ 会监视 Kubernetes 控制节点对 Service 对象和 Endpoints 对象的添加和移除。 对每个 Service，它会配置 iptables 规则，从而捕获到达该 Service 的 ​`clusterIP` ​和端口的请求，进而将请求重定向到 Service 的一组后端中的某个 Pod 上面。 对于每个 Endpoints 对象，它也会配置 iptables 规则，这个规则会选择一个后端组合。

默认的策略是，kube-proxy 在 iptables 模式下随机选择一个后端。

使用 iptables 处理流量具有较低的系统开销，因为流量由 Linux netfilter 处理， 而无需在用户空间和内核空间之间切换。 这种方法也可能更可靠。

如果 kube-proxy 在 iptables 模式下运行，并且所选的第一个 Pod 没有响应， 则连接失败。 这与用户空间模式不同：在这种情况下，kube-proxy 将检测到与第一个 Pod 的连接已失败， 并会自动使用其他后端 Pod 重试。

你可以使用 Pod 就绪探测器 验证后端 Pod 可以正常工作，以便 iptables 模式下的 kube-proxy 仅看到测试正常的后端。 这样做意味着你避免将流量通过 kube-proxy 发送到已知已失败的 Pod。

![](https://atts.w3cschool.cn/attachments/image/20220506/1651803096285815.svg)  

### IPVS 代理模式

FEATURE STATE: Kubernetes v1.11 \[stable\]

在 ​`ipvs` ​模式下，kube-proxy 监视 Kubernetes 服务和端点，调用 ​`netlink` ​接口相应地创建 IPVS 规则， 并定期将 IPVS 规则与 Kubernetes 服务和端点同步。 该控制循环可确保IPVS 状态与所需状态匹配。访问服务时，IPVS 将流量定向到后端Pod之一。

IPVS代理模式基于类似于 iptables 模式的 netfilter 挂钩函数， 但是使用哈希表作为基础数据结构，并且在内核空间中工作。 这意味着，与 iptables 模式下的 kube-proxy 相比，IPVS 模式下的 kube-proxy 重定向通信的延迟要短，并且在同步代理规则时具有更好的性能。 与其他代理模式相比，IPVS 模式还支持更高的网络流量吞吐量。

IPVS 提供了更多选项来平衡后端 Pod 的流量。 这些是：

*   ​`rr`​：轮替（Round-Robin）
*   ​`lc`​：最少链接（Least Connection），即打开链接数量最少者优先
*   ​`dh`​：目标地址哈希（Destination Hashing）
*   ​`sh`​：源地址哈希（Source Hashing）
*   ​`sed`​：最短预期延迟（Shortest Expected Delay）
*   ​`nq`​：从不排队（Never Queue）

> 要在 IPVS 模式下运行 kube-proxy，必须在启动 kube-proxy 之前使 IPVS 在节点上可用。  
> 当 kube-proxy 以 IPVS 代理模式启动时，它将验证 IPVS 内核模块是否可用。 如果未检测到 IPVS 内核模块，则 kube-proxy 将退回到以 iptables 代理模式运行。

![](https://atts.w3cschool.cn/attachments/image/20220506/1651803184255528.svg)  

在这些代理模型中，绑定到服务 IP 的流量： 在客户端不了解 Kubernetes 或服务或 Pod 的任何信息的情况下，将 Port 代理到适当的后端。

如果要确保每次都将来自特定客户端的连接传递到同一 Pod， 则可以通过将 ​`service.spec.sessionAffinity`​ 设置为 "ClientIP" （默认值是 "None"），来基于客户端的 IP 地址选择会话关联。 你还可以通过适当设置 ​`service.spec.sessionAffinityConfig.clientIP.timeoutSeconds`​ 来设置最大会话停留时间。 （默认值为 10800 秒，即 3 小时）。

多端口 Service 
------------

对于某些服务，你需要公开多个端口。 Kubernetes 允许你在 Service 对象上配置多个端口定义。 为服务使用多个端口时，必须提供所有端口名称，以使它们无歧义。 例如：

`apiVersion: v1 kind: Service metadata:   name: my-service spec:   selector:     app: MyApp   ports:     - name: http       protocol: TCP       port: 80       targetPort: 9376     - name: https       protocol: TCP       port: 443       targetPort: 9377`

> 与一般的Kubernetes名称一样，端口名称只能包含小写字母数字字符 和 ​`-`​。 端口名称还必须以字母数字字符开头和结尾。  
> 例如，名称 ​`123-abc`​ 和 ​`web` ​有效，但是 ​`123_abc`​ 和 ​`-web`​ 无效。

选择自己的 IP 地址
-----------

在 ​`Service` ​创建的请求中，可以通过设置 ​`spec.clusterIP`​ 字段来指定自己的集群 IP 地址。 比如，希望替换一个已经已存在的 DNS 条目，或者遗留系统已经配置了一个固定的 IP 且很难重新配置。

用户选择的 IP 地址必须合法，并且这个 IP 地址在 ​`service-cluster-ip-range`​ CIDR 范围内， 这对 API 服务器来说是通过一个标识来指定的。 如果 IP 地址不合法，API 服务器会返回 HTTP 状态码 422，表示值不合法。

流量策略 
-----

### 外部流量策略 

你可以通过设置 ​`spec.externalTrafficPolicy`​ 字段来控制来自于外部的流量是如何路由的。 可选值有 ​`Cluster` ​和 ​`Local`​。字段设为 ​`Cluster` ​会将外部流量路由到所有就绪的端点， 设为 ​`Local` ​会只路由到当前节点上就绪的端点。 如果流量策略设置为 ​`Local`​，而且当前节点上没有就绪的端点，kube-proxy 不会转发请求相关服务的任何流量。

> FEATURE STATE: Kubernetes v1.22 \[alpha\]  
> 如果你启用了 kube-proxy 的 ​`ProxyTerminatingEndpoints` ​特性门控， kube-proxy 会检查节点是否有本地的端点，以及是否所有的本地端点都被标记为终止中。  
> 如果本地有端点，而且所有端点处于终止中的状态，那么 kube-proxy 会忽略任何设为 ​`Local` ​的外部流量策略。 在所有本地端点处于终止中的状态的同时，kube-proxy 将请求指定服务的流量转发到位于其它节点的 状态健康的端点，如同外部流量策略设为 ​`Cluster`​。  
> 针对处于正被终止状态的端点这一转发行为使得外部负载均衡器可以优雅地排出由 ​`NodePort` ​服务支持的连接，就算是健康检查节点端口开始失败也是如此。 否则，当节点还在负载均衡器的节点池内，在 Pod 终止过程中的流量会被丢掉，这些流量可能会丢失。

### 内部流量策略 

FEATURE STATE: Kubernetes v1.22 \[beta\]

你可以设置 ​`spec.internalTrafficPolicy`​ 字段来控制内部来源的流量是如何转发的。可设置的值有 ​`Cluster` ​和 ​`Local`​。 将字段设置为 ​`Cluster` ​会将内部流量路由到所有就绪端点，设置为 ​`Local` ​只会路由到当前节点上就绪的端点。 如果流量策略是 ​`Local`​，而且当前节点上没有就绪的端点，那么 kube-proxy 会丢弃流量。

服务发现 
-----

Kubernetes 支持两种基本的服务发现模式 —— 环境变量和 DNS。

### 环境变量 

当 Pod 运行在 ​`Node` ​上，kubelet 会为每个活跃的 Service 添加一组环境变量。 它同时支持 [Docker links](https://docs.docker.com/network/links/)兼容 变量、 简单的 ​`{SVCNAME}_SERVICE_HOST`​ 和 ​`{SVCNAME}_SERVICE_PORT`​ 变量。 这里 Service 的名称需大写，横线被转换成下划线。

举个例子，一个名称为 ​`redis-master`​ 的 Service 暴露了 TCP 端口 6379， 同时给它分配了 Cluster IP 地址 10.0.0.11，这个 Service 生成了如下环境变量：

`REDIS_MASTER_SERVICE_HOST=10.0.0.11 REDIS_MASTER_SERVICE_PORT=6379 REDIS_MASTER_PORT=tcp://10.0.0.11:6379 REDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379 REDIS_MASTER_PORT_6379_TCP_PROTO=tcp REDIS_MASTER_PORT_6379_TCP_PORT=6379 REDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11`

> 当你具有需要访问服务的 Pod 时，并且你正在使用环境变量方法将端口和集群 IP 发布到客户端 Pod 时，必须在客户端 Pod 出现 之前 创建服务。 否则，这些客户端 Pod 将不会设定其环境变量。  
> 如果仅使用 DNS 查找服务的集群 IP，则无需担心此设定问题。

### DNS

你可以（几乎总是应该）使用附加组件 为 Kubernetes 集群设置 DNS 服务。

支持集群的 DNS 服务器（例如 CoreDNS）监视 Kubernetes API 中的新服务，并为每个服务创建一组 DNS 记录。 如果在整个集群中都启用了 DNS，则所有 Pod 都应该能够通过其 DNS 名称自动解析服务。

例如，如果你在 Kubernetes 命名空间 ​`my-ns`​ 中有一个名为 ​`my-service`​ 的服务， 则控制平面和 DNS 服务共同为 ​`my-service.my-ns`​ 创建 DNS 记录。 ​`my-ns`​ 命名空间中的 Pod 应该能够通过按名检索 ​`my-service`​ 来找到服务 （​`my-service.my-ns`​ 也可以工作）。

其他命名空间中的 Pod 必须将名称限定为 ​`my-service.my-ns`​。 这些名称将解析为为服务分配的集群 IP。

Kubernetes 还支持命名端口的 DNS SRV（服务）记录。 如果 ​`my-service.my-ns`​ 服务具有名为 ​`http`的端口，且协议设置为 TCP， 则可以对 ​`_http._tcp.my-service.my-ns`​ 执行 DNS SRV 查询查询以发现该端口号, ​`"http"`​ 以及 IP 地址。

Kubernetes DNS 服务器是唯一的一种能够访问 ​`ExternalName` ​类型的 Service 的方式。 

无头服务（Headless Services） 
------------------------

有时不需要或不想要负载均衡，以及单独的 Service IP。 遇到这种情况，可以通过指定 Cluster IP（​`spec.clusterIP`​）的值为 ​`"None"`​ 来创建 ​`Headless` ​Service。

你可以使用无头 Service 与其他服务发现机制进行接口，而不必与 Kubernetes 的实现捆绑在一起。

对这无头 Service 并不会分配 Cluster IP，kube-proxy 不会处理它们， 而且平台也不会为它们进行负载均衡和路由。 DNS 如何实现自动配置，依赖于 Service 是否定义了选择算符。

### 带选择算符的服务

对定义了选择算符的无头服务，Endpoint 控制器在 API 中创建了 Endpoints 记录， 并且修改 DNS 配置返回 A 记录（IP 地址），通过这个地址直接到达 ​`Service` ​的后端 Pod 上。

### 无选择算符的服务 

对没有定义选择算符的无头服务，Endpoint 控制器不会创建 ​`Endpoints` ​记录。 然而 DNS 系统会查找和配置，无论是：

*   对于 ​`ExternalName` ​类型的服务，查找其 CNAME 记录
*   对所有其他类型的服务，查找与 Service 名称相同的任何 ​`Endpoints` ​的记录

发布服务（服务类型) 
-----------

对一些应用的某些部分（如前端），可能希望将其暴露给 Kubernetes 集群外部 的 IP 地址。

Kubernetes ​`ServiceTypes` ​允许指定你所需要的 Service 类型，默认是 ​`ClusterIP`​。

​`Type` ​的取值以及行为如下：

*   ​`ClusterIP`​：通过集群的内部 IP 暴露服务，选择该值时服务只能够在集群内部访问。 这也是默认的 ​`ServiceType`​。
*   ​`NodePort`​：通过每个节点上的 IP 和静态端口（​`NodePort`​）暴露服务。 ​`NodePort` ​服务会路由到自动创建的 ​`ClusterIP` ​服务。 通过请求 ​`<节点 IP>:<节点端口>`​，你可以从集群的外部访问一个 ​`NodePort` ​服务。
*   ​`LoadBalancer`​：使用云提供商的负载均衡器向外部暴露服务。 外部负载均衡器可以将流量路由到自动创建的 ​`NodePort` ​服务和 ​`ClusterIP` ​服务上。
*   ​`ExternalName`​：通过返回 ​`CNAME` ​和对应值，可以将服务映射到 ​`externalName` ​字段的内容（例如，​`foo.bar.example.com`​）。 无需创建任何类型代理。

> 你需要使用 kube-dns 1.7 及以上版本或者 CoreDNS 0.0.8 及以上版本才能使用 ​`ExternalName` ​类型。

你也可以使用 Ingress 来暴露自己的服务。 Ingress 不是一种服务类型，但它充当集群的入口点。 它可以将路由规则整合到一个资源中，因为它可以在同一IP地址下公开多个服务。

### NodePort 类型 

如果你将 ​`type` ​字段设置为 ​`NodePort`​，则 Kubernetes 控制平面将在 ​`--service-node-port-range`​ 标志指定的范围内分配端口（默认值：30000-32767）。 每个节点将那个端口（每个节点上的相同端口号）代理到你的服务中。 你的服务在其 ​`.spec.ports[*].nodePort`​ 字段中要求分配的端口。

如果你想指定特定的 IP 代理端口，则可以设置 kube-proxy 中的 ​`--nodeport-addresses`​ 参数 或者将kube-proxy 配置文件 中的等效 ​`nodePortAddresses` ​字段设置为特定的 IP 块。 该标志采用逗号分隔的 IP 块列表（例如，​`10.0.0.0/8`​、​`192.0.2.0/25`​）来指定 kube-proxy 应该认为是此节点本地的 IP 地址范围。

例如，如果你使用 ​`--nodeport-addresses=127.0.0.0/8`​ 标志启动 kube-proxy， 则 kube-proxy 仅选择 NodePort Services 的本地回路接口。 ​`--nodeport-addresses`​ 的默认值是一个空列表。 这意味着 kube-proxy 应该考虑 NodePort 的所有可用网络接口。 （这也与早期的 Kubernetes 版本兼容）。

如果需要特定的端口号，你可以在 ​`nodePort` ​字段中指定一个值。 控制平面将为你分配该端口或报告 API 事务失败。 这意味着你需要自己注意可能发生的端口冲突。 你还必须使用有效的端口号，该端口号在配置用于 NodePort 的范围内。

使用 NodePort 可以让你自由设置自己的负载均衡解决方案， 配置 Kubernetes 不完全支持的环境， 甚至直接暴露一个或多个节点的 IP。

需要注意的是，Service 能够通过 ​`<NodeIP>:spec.ports[*].nodePort`​ 和 ​`spec.clusterIp:spec.ports[*].port`​ 而对外可见。 如果设置了 kube-proxy 的 ​`--nodeport-addresses`​ 参数或 kube-proxy 配置文件中的等效字段， ​ `<NodeIP>`​ 将被过滤 NodeIP。

例如：

``apiVersion: v1 kind: Service metadata:   name: my-service spec:   type: NodePort   selector:     app: MyApp   ports:       # 默认情况下，为了方便起见，`targetPort` 被设置为与 `port` 字段相同的值。     - port: 80       targetPort: 80       # 可选字段       # 默认情况下，为了方便起见，Kubernetes 控制平面会从某个范围内分配一个端口号（默认：30000-32767）       nodePort: 30007``

### LoadBalancer 类型 

在使用支持外部负载均衡器的云提供商的服务时，设置 ​`type` ​的值为 ​`"LoadBalancer"`​， 将为 Service 提供负载均衡器。 负载均衡器是异步创建的，关于被提供的负载均衡器的信息将会通过 Service 的 ​`status.loadBalancer`​ 字段发布出去。

实例：

`apiVersion: v1 kind: Service metadata:   name: my-service spec:   selector:     app: MyApp   ports:     - protocol: TCP       port: 80       targetPort: 9376   clusterIP: 10.0.171.239   type: LoadBalancer status:   loadBalancer:     ingress:       - ip: 192.0.2.127`

来自外部负载均衡器的流量将直接重定向到后端 Pod 上，不过实际它们是如何工作的，这要依赖于云提供商。

某些云提供商允许设置 ​`loadBalancerIP`​。 在这些情况下，将根据用户设置的 ​`loadBalancerIP` ​来创建负载均衡器。 如果没有设置 ​`loadBalancerIP` ​字段，将会给负载均衡器指派一个临时 IP。 如果设置了 ​`loadBalancerIP`​，但云提供商并不支持这种特性，那么设置的 ​`loadBalancerIP` ​值将会被忽略掉。

> 在 Azure 上，如果要使用用户指定的公共类型 ​`loadBalancerIP`​，则 首先需要创建静态类型的公共 IP 地址资源。 此公共 IP 地址资源应与集群中其他自动创建的资源位于同一资源组中。 例如，​`MC_myResourceGroup_myAKSCluster_eastus`​。  
> 将分配的 IP 地址设置为 loadBalancerIP。确保你已更新云提供程序配置文件中的 securityGroupName。 有关对 ​`CreatingLoadBalancerFailed` ​权限问题进行故障排除的信息， 请参阅 [与 Azure Kubernetes 服务（AKS）负载平衡器一起使用静态 IP 地址](https://docs.microsoft.com/en-us/azure/aks/static-ip) 或 [在 AKS 集群上使用高级联网时出现 CreatingLoadBalancerFailed](https://github.com/Azure/AKS/issues/357)。

#### 混合协议类型的负载均衡器 

FEATURE STATE: Kubernetes v1.20 \[alpha\]

默认情况下，对于 LoadBalancer 类型的服务，当定义了多个端口时，所有 端口必须具有相同的协议，并且该协议必须是受云提供商支持的协议。

如果为 kube-apiserver 启用了 ​`MixedProtocolLBService` ​特性门控， 则当定义了多个端口时，允许使用不同的协议。

> 可用于 LoadBalancer 类型服务的协议集仍然由云提供商决定。

### 禁用负载均衡器节点端口分配

FEATURE STATE: Kubernetes v1.20 \[alpha\]

从 v1.20 版本开始， 你可以通过设置 ​`spec.allocateLoadBalancerNodePorts`​ 为 ​`false` ​对类型为 LoadBalancer 的服务禁用节点端口分配。 这仅适用于直接将流量路由到 Pod 而不是使用节点端口的负载均衡器实现。 默认情况下，​`spec.allocateLoadBalancerNodePorts`​ 为 ​`true`​， LoadBalancer 类型的服务继续分配节点端口。 如果现有服务已被分配节点端口，将参数 ​`spec.allocateLoadBalancerNodePorts`​ 设置为 ​`false` ​时，这些服务上已分配置的节点端口不会被自动释放。 你必须显式地在每个服务端口中删除 ​`nodePorts` ​项以释放对应端口。 你必须启用 ​`ServiceLBNodePortControl` ​特性门控才能使用该字段。

#### 设置负载均衡器实现的类别

FEATURE STATE: Kubernetes v1.22 \[beta\]

​`spec.loadBalancerClass`​ 允许你不使用云提供商的默认负载均衡器实现，转而使用指定的负载均衡器实现。 这个特性从 v1.21 版本开始可以使用，你在 v1.21 版本中使用这个字段必须启用 ​`ServiceLoadBalancerClass` ​特性门控，这个特性门控从 v1.22 版本及以后默认打开。 默认情况下，​`.spec.loadBalancerClass`​ 的取值是 ​`nil`​，如果集群使用 ​`--cloud-provider`​ 配置了云提供商， ​`LoadBalancer` ​类型服务会使用云提供商的默认负载均衡器实现。 如果设置了 ​`.spec.loadBalancerClass`​，则假定存在某个与所指定的类相匹配的 负载均衡器实现在监视服务变化。 所有默认的负载均衡器实现（例如，由云提供商所提供的）都会忽略设置了此字段 的服务。​`.spec.loadBalancerClass`​ 只能设置到类型为 ​`LoadBalancer` ​的 Service 之上，而且一旦设置之后不可变更。

​`.spec.loadBalancerClass`​ 的值必须是一个标签风格的标识符， 可以有选择地带有类似 "​`internal-vip`​" 或 "​`example.com/internal-vip`​" 这类 前缀。没有前缀的名字是保留给最终用户的。

#### 内部负载均衡器

在混合环境中，有时有必要在同一(虚拟)网络地址块内路由来自服务的流量。

在水平分割 DNS 环境中，你需要两个服务才能将内部和外部流量都路由到你的端点（Endpoints）。

如要设置内部负载均衡器，请根据你所使用的云运营商，为服务添加以下注解之一。

*   GCP

`[...] metadata:     name: my-service     annotations:         cloud.google.com/load-balancer-type: "Internal" [...]`

*   AWS
    
    `[...] metadata:     name: my-service     annotations:         service.beta.kubernetes.io/aws-load-balancer-internal: "true" [...]`
    
*   Azure
    
    `[...] metadata:     name: my-service     annotations:         service.beta.kubernetes.io/azure-load-balancer-internal: "true" [...]`
    
*   IBM Cloud
    
    `[...] metadata:     name: my-service     annotations:         service.kubernetes.io/ibm-load-balancer-cloud-provider-ip-type: "private" [...]`
    
*   OpenStack
    
    `[...] metadata:     name: my-service     annotations:         service.beta.kubernetes.io/openstack-internal-load-balancer: "true" [...]`
    
*   Baidu Cloud
    
    `[...] metadata:     name: my-service     annotations:         service.beta.kubernetes.io/cce-load-balancer-internal-vpc: "true" [...]`
    
*   Tencent Cloud
    
    `[...] metadata:   annotations:     service.kubernetes.io/qcloud-loadbalancer-internal-subnetid: subnet-xxxxx [...]`
    
*   Alibaba Cloud
    
    `[...] metadata:   annotations:     service.beta.kubernetes.io/alibaba-cloud-loadbalancer-address-type: "intranet" [...]`
    

### AWS TLS 支持

为了对在 AWS 上运行的集群提供 TLS/SSL 部分支持，你可以向 ​`LoadBalancer` ​服务添加三个注解：

`metadata:   name: my-service   annotations:     service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:us-east-1:123456789012:certificate/12345678-1234-1234-1234-123456789012`

第一个指定要使用的证书的 ARN。 它可以是已上载到 IAM 的第三方颁发者的证书， 也可以是在 AWS Certificate Manager 中创建的证书。

`metadata:   name: my-service   annotations:     service.beta.kubernetes.io/aws-load-balancer-backend-protocol: (https|http|ssl|tcp)`

第二个注解指定 Pod 使用哪种协议。 对于 HTTPS 和 SSL，ELB 希望 Pod 使用证书 通过加密连接对自己进行身份验证。

HTTP 和 HTTPS 选择第7层代理：ELB 终止与用户的连接，解析标头，并在转发请求时向 ​`X-Forwarded-For`​ 标头注入用户的 IP 地址（Pod 仅在连接的另一端看到 ELB 的 IP 地址）。

TCP 和 SSL 选择第4层代理：ELB 转发流量而不修改报头。

在某些端口处于安全状态而其他端口未加密的混合使用环境中，可以使用以下注解：

`metadata:   name: my-service   annotations:     service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http     service.beta.kubernetes.io/aws-load-balancer-ssl-ports: "443,8443"`

在上例中，如果服务包含 ​`80`​、​`443` ​和 ​`8443` ​三个端口， 那么 ​`443` ​和 ​`8443` ​将使用 SSL 证书， 而 ​`80` ​端口将转发 HTTP 数据包。

从 Kubernetes v1.9 起可以使用 [预定义的 AWS SSL 策略](https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-security-policy-table.html) 为你的服务使用 HTTPS 或 SSL 侦听器。 要查看可以使用哪些策略，可以使用 ​`aws` ​命令行工具：

`aws elb describe-load-balancer-policies --query 'PolicyDescriptions[].PolicyName'`
    

然后，你可以使用 "​`service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy`​" 注解; 例如：

`metadata:   name: my-service   annotations:     service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: "ELBSecurityPolicy-TLS-1-2-2017-01"`

#### AWS 上的 PROXY 协议支持

为了支持在 AWS 上运行的集群，启用 [PROXY 协议](https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt)。 你可以使用以下服务注解：

`metadata:   name: my-service   annotations:     service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: "*"`

从 1.3.0 版开始，此注解的使用适用于 ELB 代理的所有端口，并且不能进行其他配置。

#### AWS 上的 ELB 访问日志

有几个注解可用于管理 AWS 上 ELB 服务的访问日志。

注解 ​`service.beta.kubernetes.io/aws-load-balancer-access-log-enabled`​ 控制是否启用访问日志。

注解 ​`service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval`​ 控制发布访问日志的时间间隔（以分钟为单位）。你可以指定 5 分钟或 60 分钟的间隔。

注解 ​`service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name`​ 控制存储负载均衡器访问日志的 Amazon S3 存储桶的名称。

注解 ​`service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix`​ 指定为 Amazon S3 存储桶创建的逻辑层次结构。

`` metadata:   name: my-service   annotations:     service.beta.kubernetes.io/aws-load-balancer-access-log-enabled: "true"     # 指定是否为负载均衡器启用访问日志     service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval: "60"     # 发布访问日志的时间间隔。你可以将其设置为 5 分钟或 60 分钟。     service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name: "my-bucket"     # 用来存放访问日志的 Amazon S3 Bucket 名称     service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix: "my-bucket-prefix/prod"     # 你为 Amazon S3 Bucket 所创建的逻辑层次结构，例如 `my-bucket-prefix/prod` ``

#### AWS 上的连接排空 

可以将注解 ​`service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled`​ 设置为 ​`"true"`​ 来管理 ELB 的连接排空。 注解 ​`service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout`​ 也可以用于设置最大时间（以秒为单位），以保持现有连接在注销实例之前保持打开状态。

`metadata:   name: my-service   annotations:     service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled: "true"     service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout: "60"`
        

#### 其他 ELB 注解

还有其他一些注解，用于管理经典弹性负载均衡器，如下所述。

`metadata:   name: my-service   annotations:     # 按秒计的时间，表示负载均衡器关闭连接之前连接可以保持空闲     # （连接上无数据传输）的时间长度     service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "60"      # 指定该负载均衡器上是否启用跨区的负载均衡能力     service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"      # 逗号分隔列表值，每一项都是一个键-值耦对，会作为额外的标签记录于 ELB 中     service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: "environment=prod,owner=devops"      # 将某后端视为健康、可接收请求之前需要达到的连续成功健康检查次数。     # 默认为 2，必须介于 2 和 10 之间     service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold: ""      # 将某后端视为不健康、不可接收请求之前需要达到的连续不成功健康检查次数。     # 默认为 6，必须介于 2 和 10 之间     service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: "3"      # 对每个实例进行健康检查时，连续两次检查之间的大致间隔秒数     # 默认为 10，必须介于 5 和 300 之间     service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: "20"      # 时长秒数，在此期间没有响应意味着健康检查失败     # 此值必须小于 service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval     # 默认值为 5，必须介于 2 和 60 之间     service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: "5"      # 由已有的安全组所构成的列表，可以配置到所创建的 ELB 之上。     # 与注解 service.beta.kubernetes.io/aws-load-balancer-extra-security-groups 不同，     # 这一设置会替代掉之前指定给该 ELB 的所有其他安全组，也会覆盖掉为此     # ELB 所唯一创建的安全组。      # 此列表中的第一个安全组 ID 被用来作为决策源，以允许入站流量流入目标工作节点     # (包括服务流量和健康检查）。     # 如果多个 ELB 配置了相同的安全组 ID，为工作节点安全组添加的允许规则行只有一个，     # 这意味着如果你删除了这些 ELB 中的任何一个，都会导致该规则记录被删除，     # 以至于所有共享该安全组 ID 的其他 ELB 都无法访问该节点。     # 此注解如果使用不当，会导致跨服务的不可用状况。     service.beta.kubernetes.io/aws-load-balancer-security-groups: "sg-53fae93f"      # 额外的安全组列表，将被添加到所创建的 ELB 之上。     # 添加时，会保留为 ELB 所专门创建的安全组。     # 这样会确保每个 ELB 都有一个唯一的安全组 ID 和与之对应的允许规则记录，     # 允许请求（服务流量和健康检查）发送到目标工作节点。     # 这里顶一个安全组可以被多个服务共享。     service.beta.kubernetes.io/aws-load-balancer-extra-security-groups: "sg-53fae93f,sg-42efd82e"      # 用逗号分隔的一个键-值偶对列表，用来为负载均衡器选择目标节点     service.beta.kubernetes.io/aws-load-balancer-target-node-labels: "ingress-gw,gw-name=public-api"`

#### AWS 上网络负载均衡器支持

FEATURE STATE: Kubernetes v1.15 \[beta\]

要在 AWS 上使用网络负载均衡器，可以使用注解 ​`service.beta.kubernetes.io/aws-load-balancer-type`​，将其取值设为 ​`nlb`​。

`metadata:   name: my-service   annotations:     service.beta.kubernetes.io/aws-load-balancer-type: "nlb"`

> NLB 仅适用于某些实例类。有关受支持的实例类型的列表， 请参见 [AWS文档](https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-register-targets.html target=) 中关于所支持的实例类型的 Elastic Load Balancing 说明。

与经典弹性负载平衡器不同，网络负载平衡器（NLB）将客户端的 IP 地址转发到该节点。 如果服务的 ​`.spec.externalTrafficPolicy`​ 设置为 ​`Cluster` ​，则客户端的IP地址不会传达到最终的 Pod。

通过将 ​`.spec.externalTrafficPolicy`​ 设置为 ​`Local`​，客户端IP地址将传播到最终的 Pod， 但这可能导致流量分配不均。 没有针对特定 LoadBalancer 服务的任何 Pod 的节点将无法通过自动分配的 ​`.spec.healthCheckNodePort`​ 进行 NLB 目标组的运行状况检查，并且不会收到任何流量。

为了获得均衡流量，请使用 DaemonSet 或指定 Pod 反亲和性 使其不在同一节点上。

你还可以将 NLB 服务与内部负载平衡器 注解一起使用。

为了使客户端流量能够到达 NLB 后面的实例，使用以下 IP 规则修改了节点安全组：

Rule

Protocol

Port(s)

IpRange(s)

IpRange Description

Health Check

TCP

NodePort(s) (`.spec.healthCheckNodePort` for `.spec.externalTrafficPolicy = Local`)

Subnet CIDR

kubernetes.io/rule/nlb/health=<loadBalancerName>

Client Traffic

TCP

NodePort(s)

`.spec.loadBalancerSourceRanges` (defaults to `0.0.0.0/0`)

kubernetes.io/rule/nlb/client=<loadBalancerName>

MTU Discovery

ICMP

3,4

`.spec.loadBalancerSourceRanges` (defaults to `0.0.0.0/0`)

kubernetes.io/rule/nlb/mtu=<loadBalancerName>

为了限制哪些客户端IP可以访问网络负载平衡器，请指定 ​`loadBalancerSourceRanges`​。

`spec:   loadBalancerSourceRanges:     - "143.231.0.0/16"`

> 如果未设置 ​`.spec.loadBalancerSourceRanges`​ ，则 Kubernetes 允许从 ​`0.0.0.0/0`​ 到节点安全组的流量。 如果节点具有公共 IP 地址，请注意，非 NLB 流量也可以到达那些修改后的安全组中的所有实例。

有关弹性 IP 注解和更多其他常见用例， 请参阅[AWS负载均衡控制器文档](https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/guide/service/annotations/)。

#### 腾讯 Kubernetes 引擎（TKE）上的 CLB 注解

以下是在 TKE 上管理云负载均衡器的注解。

`metadata:   name: my-service   annotations:     # 绑定负载均衡器到指定的节点。     service.kubernetes.io/qcloud-loadbalancer-backends-label: key in (value1, value2)      # 为已有负载均衡器添加 ID。     service.kubernetes.io/tke-existed-lbid：lb-6swtxxxx      # 负载均衡器（LB）的自定义参数尚不支持修改 LB 类型。     service.kubernetes.io/service.extensiveParameters: ""      # 自定义负载均衡监听器。     service.kubernetes.io/service.listenerParameters: ""      # 指定负载均衡类型。     # 可用参数: classic (Classic Cloud Load Balancer) 或 application (Application Cloud Load Balancer)     service.kubernetes.io/loadbalance-type: xxxxx      # 指定公用网络带宽计费方法。     # 可用参数: TRAFFIC_POSTPAID_BY_HOUR(bill-by-traffic) 和 BANDWIDTH_POSTPAID_BY_HOUR (bill-by-bandwidth).     service.kubernetes.io/qcloud-loadbalancer-internet-charge-type: xxxxxx      # 指定带宽参数 (取值范围： [1,2000] Mbps).     service.kubernetes.io/qcloud-loadbalancer-internet-max-bandwidth-out: "10"      # 当设置该注解时，负载平衡器将只注册正在运行 Pod 的节点，     # 否则所有节点将会被注册。     service.kubernetes.io/local-svc-only-bind-node-with-pod: true`

### ExternalName 类型 

类型为 ExternalName 的服务将服务映射到 DNS 名称，而不是典型的选择器，例如 ​`my-service`​ 或者 ​`cassandra`​。 你可以使用 ​`spec.externalName`​ 参数指定这些服务。

例如，以下 Service 定义将 ​`prod` ​名称空间中的 ​`my-service`​ 服务映射到 ​`my.database.example.com`​：

`apiVersion: v1 kind: Service metadata:   name: my-service   namespace: prod spec:   type: ExternalName   externalName: my.database.example.com`

> ExternalName 服务接受 IPv4 地址字符串，但作为包含数字的 DNS 名称，而不是 IP 地址。 类似于 IPv4 地址的外部名称不能由 CoreDNS 或 ingress-nginx 解析，因为外部名称旨在指定规范的 DNS 名称。 要对 IP 地址进行硬编码，请考虑使用 headless Services。

当查找主机 ​`my-service.prod.svc.cluster.local`​ 时，集群 DNS 服务返回 ​`CNAME` ​记录， 其值为 ​`my.database.example.com`​。 访问 ​`my-service`​ 的方式与其他服务的方式相同，但主要区别在于重定向发生在 DNS 级别，而不是通过代理或转发。 如果以后你决定将数据库移到集群中，则可以启动其 Pod，添加适当的选择器或端点以及更改服务的 ​`type`​。

> 对于一些常见的协议，包括 HTTP 和 HTTPS， 你使用 ExternalName 可能会遇到问题。 如果你使用 ExternalName，那么集群内客户端使用的主机名 与 ExternalName 引用的名称不同。  
> 对于使用主机名的协议，此差异可能会导致错误或意外响应。 HTTP 请求将具有源服务器无法识别的 Host: 标头；TLS 服 务器将无法提供与客户端连接的主机名匹配的证书。

> 本部分感谢 [Alen Komljen](https://akomljen.com/)的 [Kubernetes Tips - Part1](https://akomljen.com/kubernetes-tips-part-1/) 博客文章。

### 外部 IP 

如果外部的 IP 路由到集群中一个或多个 Node 上，Kubernetes Service 会被暴露给这些 externalIPs。 通过外部 IP（作为目的 IP 地址）进入到集群，打到 Service 的端口上的流量， 将会被路由到 Service 的 Endpoint 上。 ​`externalIPs` ​不会被 Kubernetes 管理，它属于集群管理员的职责范畴。

根据 Service 的规定，​`externalIPs` ​可以同任意的 ​`ServiceType` ​来一起指定。 在上面的例子中，​`my-service`​ 可以在 "​`80.11.12.10:80`​"(​`externalIP:port`​) 上被客户端访问。

`apiVersion: v1 kind: Service metadata:   name: my-service spec:   selector:     app: MyApp   ports:     - name: http       protocol: TCP       port: 80       targetPort: 9376   externalIPs:     - 80.11.12.10`

不足之处
----

为 VIP 使用用户空间代理，将只适合小型到中型规模的集群，不能够扩展到上千 Service 的大型集群。 查看[最初设计方案](https://github.com/kubernetes/kubernetes/issues/1107) 获取更多细节。

使用用户空间代理，隐藏了访问 Service 的数据包的源 IP 地址。 这使得一些类型的防火墙无法起作用。 iptables 代理不会隐藏 Kubernetes 集群内部的 IP 地址，但却要求客户端请求 必须通过一个负载均衡器或 Node 端口。

​`Type` ​字段支持嵌套功能 —— 每一层需要添加到上一层里面。 不会严格要求所有云提供商（例如，GCE 就没必要为了使一个 ​`LoadBalancer` ​能工作而分配一个 ​`NodePort`​，但是 AWS 需要 ），但当前 API 是强制要求的。

虚拟IP实施
------

对很多想使用 Service 的人来说，前面的信息应该足够了。 然而，有很多内部原理性的内容，还是值去理解的。

### 避免冲突

Kubernetes 最主要的哲学之一，是用户不应该暴露那些能够导致他们操作失败、但又不是他们的过错的场景。 对于 Service 资源的设计，这意味着如果用户的选择有可能与他人冲突，那就不要让用户自行选择端口号。 这是一个隔离性的失败。

为了使用户能够为他们的 Service 选择一个端口号，我们必须确保不能有2个 Service 发生冲突。 Kubernetes 通过为每个 Service 分配它们自己的 IP 地址来实现。

为了保证每个 Service 被分配到一个唯一的 IP，需要一个内部的分配器能够原子地更新 etcd 中的一个全局分配映射表， 这个更新操作要先于创建每一个 Service。 为了使 Service 能够获取到 IP，这个映射表对象必须在注册中心存在， 否则创建 Service 将会失败，指示一个 IP 不能被分配。

在控制平面中，一个后台 Controller 的职责是创建映射表 （需要支持从使用了内存锁的 Kubernetes 的旧版本迁移过来）。 同时 Kubernetes 会通过控制器检查不合理的分配（如管理员干预导致的） 以及清理已被分配但不再被任何 Service 使用的 IP 地址。

### Service IP 地址

不像 Pod 的 IP 地址，它实际路由到一个固定的目的地，Service 的 IP 实际上 不能通过单个主机来进行应答。 相反，我们使用 ​`iptables`​（Linux 中的数据包处理逻辑）来定义一个 虚拟 IP 地址（VIP），它可以根据需要透明地进行重定向。 当客户端连接到 VIP 时，它们的流量会自动地传输到一个合适的 Endpoint。 环境变量和 DNS，实际上会根据 Service 的 VIP 和端口来进行填充。

kube-proxy支持三种代理模式: 用户空间，iptables和IPVS；它们各自的操作略有不同。

#### Userspace 

作为一个例子，考虑前面提到的图片处理应用程序。 当创建后端 Service 时，Kubernetes master 会给它指派一个虚拟 IP 地址，比如 10.0.0.1。 假设 Service 的端口是 1234，该 Service 会被集群中所有的 ​`kube-proxy`​ 实例观察到。 当代理看到一个新的 Service， 它会打开一个新的端口，建立一个从该 VIP 重定向到 新端口的 iptables，并开始接收请求连接。

当一个客户端连接到一个 VIP，iptables 规则开始起作用，它会重定向该数据包到 "服务代理" 的端口。 "服务代理" 选择一个后端，并将客户端的流量代理到后端上。

这意味着 Service 的所有者能够选择任何他们想使用的端口，而不存在冲突的风险。 客户端可以连接到一个 IP 和端口，而不需要知道实际访问了哪些 Pod。

#### iptables

再次考虑前面提到的图片处理应用程序。 当创建后端 Service 时，Kubernetes 控制面板会给它指派一个虚拟 IP 地址，比如 10.0.0.1。 假设 Service 的端口是 1234，该 Service 会被集群中所有的 ​`kube-proxy`​ 实例观察到。 当代理看到一个新的 Service， 它会配置一系列的 iptables 规则，从 VIP 重定向到每个 Service 规则。 该特定于服务的规则连接到特定于 Endpoint 的规则，而后者会重定向（目标地址转译）到后端。

当客户端连接到一个 VIP，iptables 规则开始起作用。一个后端会被选择（或者根据会话亲和性，或者随机）， 数据包被重定向到这个后端。 不像用户空间代理，数据包从来不拷贝到用户空间，kube-proxy 不是必须为该 VIP 工作而运行， 并且客户端 IP 是不可更改的。

当流量打到 Node 的端口上，或通过负载均衡器，会执行相同的基本流程， 但是在那些案例中客户端 IP 是可以更改的。

#### IPVS

在大规模集群（例如 10000 个服务）中，iptables 操作会显着降低速度。 IPVS 专为负载平衡而设计，并基于内核内哈希表。 因此，你可以通过基于 IPVS 的 kube-proxy 在大量服务中实现性能一致性。 同时，基于 IPVS 的 kube-proxy 具有更复杂的负载均衡算法（最小连接、局部性、 加权、持久性）。

API 对象 
-------

Service 是 Kubernetes REST API 中的顶级资源。你可以在以下位置找到有关 API 对象的更多详细信息： [Service 对象 API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/ target=)

受支持的协议
------

### TCP

你可以将 TCP 用于任何类型的服务，这是默认的网络协议。

### UDP 

你可以将 UDP 用于大多数服务。 对于 type=LoadBalancer 服务，对 UDP 的支持取决于提供此功能的云提供商。

### SCTP

FEATURE STATE: Kubernetes v1.20 \[stable\]

一旦你使用了支持 SCTP 流量的网络插件，你就可以使用 SCTP 于更多的服务。 对于 type = LoadBalancer 的服务，SCTP 的支持取决于提供此设施的云供应商（大多数不支持）。

#### 警告

支持多宿主 SCTP 关联

> 支持多宿主SCTP关联要求 CNI 插件能够支持为一个 Pod 分配多个接口和IP地址。  
> 用于多宿主 SCTP 关联的 NAT 在相应的内核模块中需要特殊的逻辑。

##### Windows

> 基于 Windows 的节点不支持 SCTP。

##### 用户空间 kube-proxy

> 当 kube-proxy 处于用户空间模式时，它不支持 SCTP 关联的管理。

### HTTP 

如果你的云提供商支持它，则可以在 LoadBalancer 模式下使用服务来设置外部 HTTP/HTTPS 反向代理，并将其转发到该服务的 Endpoints。

> 你还可以使用 Ingress 代替 Service 来公开 HTTP/HTTPS 服务。

### PROXY 协议

如果你的云提供商支持它， 则可以在 LoadBalancer 模式下使用 Service 在 Kubernetes 本身之外配置负载均衡器， 该负载均衡器将转发前缀为 [PROXY 协议](https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt) 的连接。

负载平衡器将发送一系列初始字节，描述传入的连接，类似于此示例

`PROXY TCP4 192.0.2.202 10.0.42.7 12345 7\r\n`

上述是来自客户端的数据。

##  3.  Kubernetes Pod 与 Service 的 DNS
Pod 与 Service 的 DNS
-------------------

Kubernetes 为服务和 Pods 创建 DNS 记录。 你可以使用一致的 DNS 名称而非 IP 地址来访问服务。

介绍
--

Kubernetes DNS 在集群上调度 DNS Pod 和服务，并配置 kubelet 以告知各个容器 使用 DNS 服务的 IP 来解析 DNS 名称。

集群中定义的每个 Service （包括 DNS 服务器自身）都被赋予一个 DNS 名称。 默认情况下，客户端 Pod 的 DNS 搜索列表会包含 Pod 自身的名字空间和集群 的默认域。

### Service 的名字空间

DNS 查询可能因为执行查询的 Pod 所在的名字空间而返回不同的结果。 不指定名字空间的 DNS 查询会被限制在 Pod 所在的名字空间内。 要访问其他名字空间中的服务，需要在 DNS 查询中给出名字空间。

例如，假定名字空间 ​`test` ​中存在一个 Pod，​`prod` ​名字空间中存在一个服务 ​`data`​。

Pod 查询 ​`data` ​时没有返回结果，因为使用的是 Pod 的名字空间 ​`test`​。

Pod 查询​ `data.prod`​ 时则会返回预期的结果，因为查询中指定了名字空间。

DNS 查询可以使用 Pod 中的 ​`/etc/resolv.conf`​ 展开。kubelet 会为每个 Pod 生成此文件。例如，对 ​`data` ​的查询可能被展开为 ​`data.test.svc.cluster.local`​。 ​`search` ​选项的取值会被用来展开查询。要进一步了解 DNS 查询，可参阅 ​[`resolv.conf` ​手册页面](https://www.man7.org/linux/man-pages/man5/resolv.conf.5.html)。

`nameserver 10.32.0.10 search <namespace>.svc.cluster.local svc.cluster.local cluster.local options ndots:5`

概括起来，名字空间 ​`test` ​中的 Pod 可以成功地解析 ​`data.prod`​ 或者 ​`data.prod.svc.cluster.local`​。

### DNS 记录 

哪些对象会获得 DNS 记录呢？

1.  Services
2.  Pods

以下各节详细介绍了被支持的 DNS 记录类型和被支持的布局。 其它布局、名称或者查询即使碰巧可以工作，也应视为实现细节， 将来很可能被更改而且不会因此发出警告。 有关最新规范请查看 [Kubernetes 基于 DNS 的服务发现](https://github.com/kubernetes/dns/blob/master/docs/specification.md)。

### 服务 

#### A/AAAA 记录

“普通” 服务（除了无头服务）会以 ​`my-svc.my-namespace.svc.cluster-domain.example`​ 这种名字的形式被分配一个 DNS A 或 AAAA 记录，取决于服务的 IP 协议族。 该名称会解析成对应服务的集群 IP。

“无头（Headless）” 服务（没有集群 IP）也会以 ​`my-svc.my-namespace.svc.cluster-domain.example`​ 这种名字的形式被指派一个 DNS A 或 AAAA 记录， 具体取决于服务的 IP 协议族。 与普通服务不同，这一记录会被解析成对应服务所选择的 Pod 集合的 IP。 客户端要能够使用这组 IP，或者使用标准的轮转策略从这组 IP 中进行选择。

#### SRV 记录 

Kubernetes 会为命名端口创建 SRV 记录，这些端口是普通服务或 无头服务的一部分。 对每个命名端口，SRV 记录具有 ​`_my-port-name._my-port-protocol.my-svc.my-namespace.svc.cluster-domain.example`​ 这种形式。 对普通服务，该记录会被解析成端口号和域名：​`my-svc.my-namespace.svc.cluster-domain.example`​。 对无头服务，该记录会被解析成多个结果，服务对应的每个后端 Pod 各一个； 其中包含 Pod 端口号和形为 ​`auto-generated-name.my-svc.my-namespace.svc.cluster-domain.example`​ 的域名。

Pods
----

### A/AAAA 记录

一般而言，Pod 会对应如下 DNS 名字解析：

​`pod-ip-address.my-namespace.pod.cluster-domain.example` ​

例如，对于一个位于 ​`default` ​名字空间，IP 地址为 172.17.0.3 的 Pod， 如果集群的域名为 ​`cluster.local`​，则 Pod 会对应 DNS 名称：

​`172-17-0-3.default.pod.cluster.local`​

通过 Service 暴露出来的所有 Pod 都会有如下 DNS 解析名称可用：

​`pod-ip-address.service-name.my-namespace.svc.cluster-domain.example`​

### Pod 的 hostname 和 subdomain 字段

当前，创建 Pod 时其主机名取自 Pod 的 ​`metadata.name`​ 值。

Pod 规约中包含一个可选的 ​`hostname` ​字段，可以用来指定 Pod 的主机名。 当这个字段被设置时，它将优先于 Pod 的名字成为该 Pod 的主机名。 举个例子，给定一个 ​`hostname` ​设置为 "​`my-host`​" 的 Pod， 该 Pod 的主机名将被设置为 "​`my-host`​"。

Pod 规约还有一个可选的 ​`subdomain`​ 字段，可以用来指定 Pod 的子域名。 举个例子，某 Pod 的 ​`hostname` ​设置为 “​`foo`​”，​`subdomain` ​设置为 “​`bar`​”， 在名字空间 “​`my-namespace`​” 中对应的完全限定域名（FQDN）为 “​`foo.bar.my-namespace.svc.cluster-domain.example`​”。

示例：

`apiVersion: v1 kind: Service metadata:   name: default-subdomain spec:   selector:     name: busybox   clusterIP: None   ports:   - name: foo # 实际上不需要指定端口号     port: 1234     targetPort: 1234 --- apiVersion: v1 kind: Pod metadata:   name: busybox1   labels:     name: busybox spec:   hostname: busybox-1   subdomain: default-subdomain   containers:   - image: busybox:1.28     command:       - sleep       - "3600"     name: busybox --- apiVersion: v1 kind: Pod metadata:   name: busybox2   labels:     name: busybox spec:   hostname: busybox-2   subdomain: default-subdomain   containers:   - image: busybox:1.28     command:       - sleep       - "3600"     name: busybox`

如果某无头服务与某 Pod 在同一个名字空间中，且它们具有相同的子域名， 集群的 DNS 服务器也会为该 Pod 的全限定主机名返回 A 记录或 AAAA 记录。 例如，在同一个名字空间中，给定一个主机名为 “busybox-1”、 子域名设置为 “default-subdomain” 的 Pod，和一个名称为 “​`default-subdomain`​” 的无头服务，Pod 将看到自己的 FQDN 为 "​`busybox-1.default-subdomain.my-namespace.svc.cluster-domain.example`​"。 DNS 会为此名字提供一个 A 记录或 AAAA 记录，指向该 Pod 的 IP。 “​`busybox1`​” 和 “​`busybox2`​” 这两个 Pod 分别具有它们自己的 A 或 AAAA 记录。

Endpoints 对象可以为任何端点地址及其 IP 指定 ​`hostname`​。

> 因为没有为 Pod 名称创建 A 记录或 AAAA 记录，所以要创建 Pod 的 A 记录 或 AAAA 记录需要 ​`hostname`​。  
> 没有设置 ​`hostname` ​但设置了 ​`subdomain` ​的 Pod 只会为 无头服务创建 A 或 AAAA 记录（​`default-subdomain.my-namespace.svc.cluster-domain.example`​） 指向 Pod 的 IP 地址。 另外，除非在服务上设置了 ​`publishNotReadyAddresses=True`​，否则只有 Pod 进入就绪状态 才会有与之对应的记录。

### Pod 的 setHostnameAsFQDN 字段 

FEATURE STATE: Kubernetes v1.22 \[stable\]

前置条件：​`SetHostnameAsFQDN` ​特性门控 必须在 API 服务器 上启用。

当你在 Pod 规约中设置了 ​`setHostnameAsFQDN: true`​ 时，kubelet 会将 Pod 的全限定域名（FQDN）作为该 Pod 的主机名记录到 Pod 所在名字空间。 在这种情况下，​`hostname` ​和 ​`hostname --fqdn`​ 都会返回 Pod 的全限定域名。

> 在 Linux 中，内核的主机名字段（​`struct utsname`​ 的 ​`nodename` ​字段）限定 最多 64 个字符。  
> 如果 Pod 启用这一特性，而其 FQDN 超出 64 字符，Pod 的启动会失败。 Pod 会一直出于 ​`Pending` ​状态（通过 ​`kubectl` ​所看到的 ​`ContainerCreating`​）， 并产生错误事件，例如 "Failed to construct FQDN from pod hostname and cluster domain, FQDN ​`long-FQDN`​ is too long (64 characters is the max, 70 characters requested)." （无法基于 Pod 主机名和集群域名构造 FQDN，FQDN ​`long-FQDN`​ 过长，至多 64 字符，请求字符数为 70）。 对于这种场景而言，改善用户体验的一种方式是创建一个 准入 Webhook 控制器， 在用户创建顶层对象（如 Deployment）的时候控制 FQDN 的长度。

### Pod 的 DNS 策略 

DNS 策略可以逐个 Pod 来设定。目前 Kubernetes 支持以下特定 Pod 的 DNS 策略。 这些策略可以在 Pod 规约中的 ​`dnsPolicy` ​字段设置：

*   "​`Default`​": Pod 从运行所在的节点继承名称解析配置。
*   "​`ClusterFirst`​": 与配置的集群域后缀不匹配的任何 DNS 查询（例如 "www.kubernetes.io"） 都将转发到从节点继承的上游名称服务器。集群管理员可能配置了额外的存根域和上游 DNS 服务器。 
*   "​`ClusterFirstWithHostNet`​"：对于以 hostNetwork 方式运行的 Pod，应显式设置其 DNS 策略 "​`ClusterFirstWithHostNet`​"。
*   "​`None`​": 此设置允许 Pod 忽略 Kubernetes 环境中的 DNS 设置。Pod 会使用其 ​`dnsConfig` ​字段 所提供的 DNS 设置。

>  "Default" 不是默认的 DNS 策略。如果未明确指定 ​`dnsPolicy`​，则使用 "ClusterFirst"。

下面的示例显示了一个 Pod，其 DNS 策略设置为 "​`ClusterFirstWithHostNet`​"， 因为它已将 ​`hostNetwork` ​设置为 ​`true`​。

`apiVersion: v1 kind: Pod metadata:   name: busybox   namespace: default spec:   containers:   - image: busybox:1.28     command:       - sleep       - "3600"     imagePullPolicy: IfNotPresent     name: busybox   restartPolicy: Always   hostNetwork: true   dnsPolicy: ClusterFirstWithHostNet`

### Pod 的 DNS 配置 

FEATURE STATE: Kubernetes v1.14 \[stable\]

Pod 的 DNS 配置可让用户对 Pod 的 DNS 设置进行更多控制。

​`dnsConfig` ​字段是可选的，它可以与任何 ​`dnsPolicy` ​设置一起使用。 但是，当 Pod 的 ​`dnsPolicy` ​设置为 "​`None`​" 时，必须指定 ​`dnsConfig` ​字段。

用户可以在 ​`dnsConfig` ​字段中指定以下属性：

*   ​`nameservers`​：将用作于 Pod 的 DNS 服务器的 IP 地址列表。 最多可以指定 3 个 IP 地址。当 Pod 的 ​`dnsPolicy` ​设置为 "​`None`​" 时， 列表必须至少包含一个 IP 地址，否则此属性是可选的。 所列出的服务器将合并到从指定的 DNS 策略生成的基本名称服务器，并删除重复的地址。
*   ​`searches`​：用于在 Pod 中查找主机名的 DNS 搜索域的列表。此属性是可选的。 指定此属性时，所提供的列表将合并到根据所选 DNS 策略生成的基本搜索域名中。 重复的域名将被删除。Kubernetes 最多允许 6 个搜索域。
*   ​`options`​：可选的对象列表，其中每个对象可能具有 ​`name` ​属性（必需）和 ​`value` ​属性（可选）。 此属性中的内容将合并到从指定的 DNS 策略生成的选项。 重复的条目将被删除。

以下是具有自定义 DNS 设置的 Pod 示例：

`apiVersion: v1 kind: Pod metadata:   namespace: default   name: dns-example spec:   containers:     - name: test       image: nginx   dnsPolicy: "None"   dnsConfig:     nameservers:       - 1.2.3.4     searches:       - ns1.svc.cluster-domain.example       - my.dns.search.suffix     options:       - name: ndots         value: "2"       - name: edns0`

创建上面的 Pod 后，容器 ​`test` ​会在其 ​`/etc/resolv.conf`​ 文件中获取以下内容：

`nameserver 1.2.3.4 search ns1.svc.cluster-domain.example my.dns.search.suffix options ndots:2 edns0`

对于 IPv6 设置，搜索路径和名称服务器应按以下方式设置：

`kubectl exec -it dns-example -- cat /etc/resolv.conf`

输出类似于

`nameserver fd00:79:30::a search default.svc.cluster-domain.example svc.cluster-domain.example cluster-domain.example options ndots:5`

#### 扩展 DNS 配置 

FEATURE STATE: Kubernetes 1.22 \[alpha\]

对于 Pod DNS 配置，Kubernetes 默认允许最多 6 个 搜索域（ Search Domain） 以及一个最多 256 个字符的搜索域列表。

如果启用 kube-apiserver 和 kubelet 的特性门控 ​`ExpandedDNSConfig`​，Kubernetes 将可以有最多 32 个 搜索域以及一个最多 2048 个字符的搜索域列表。

##  4.  Kubernetes 使用 Service 连接到应用
Kubernetes 连接容器的模型
------------------

既然有了一个持续运行、可复制的应用，我们就能够将它暴露到网络上。

Kubernetes 假设 Pod 可与其它 Pod 通信，不管它们在哪个主机上。 Kubernetes 给每一个 Pod 分配一个集群私有 IP 地址，所以没必要在 Pod 与 Pod 之间创建连接或将容器的端口映射到主机端口。 这意味着同一个 Pod 内的所有容器能通过 localhost 上的端口互相连通，集群中的所有 Pod 也不需要通过 NAT 转换就能够互相看到。 本文档的剩余部分详述如何在上述网络模型之上运行可靠的服务。

本指南使用一个简单的 Nginx 服务器来演示概念验证原型。

在集群中暴露 Pod
----------

我们在之前的示例中已经做过，然而让我们以网络连接的视角再重做一遍。 创建一个 Nginx Pod，注意其中包含一个容器端口的规约：

`apiVersion: apps/v1 kind: Deployment metadata:   name: my-nginx spec:   selector:     matchLabels:       run: my-nginx   replicas: 2   template:     metadata:       labels:         run: my-nginx     spec:       containers:       - name: my-nginx         image: nginx         ports:         - containerPort: 80`

这使得可以从集群中任何一个节点来访问它。检查节点，该 Pod 正在运行：

`kubectl apply -f ./run-my-nginx.yaml kubectl get pods -l run=my-nginx -o wide`

`NAME                        READY     STATUS    RESTARTS   AGE       IP            NODE my-nginx-3800858182-jr4a2   1/1       Running   0          13s       10.244.3.4    kubernetes-minion-905m my-nginx-3800858182-kna2y   1/1       Running   0          13s       10.244.2.5    kubernetes-minion-ljyd`

检查 Pod 的 IP 地址：

`kubectl get pods -l run=my-nginx -o yaml | grep podIP     podIP: 10.244.3.4     podIP: 10.244.2.5`

你应该能够通过 ssh 登录到集群中的任何一个节点上，并使用诸如 ​`curl` ​之类的工具向这两个 IP 地址发出查询请求。 需要注意的是，容器不会使用该节点上的 80 端口，也不会使用任何特定的 NAT 规则去路由流量到 Pod 上。 这意味着可以在同一个节点上运行多个 Nginx Pod，使用相同的 ​`containerPort`​，并且可以从集群中任何其他的 Pod 或节点上使用 IP 的方式访问到它们。 如果你想的话，你依然可以将宿主节点的某个端口的流量转发到 Pod 中，但是出于网络模型的原因，你不必这么做。

创建 Service
----------

我们有一组在一个扁平的、集群范围的地址空间中运行 Nginx 服务的 Pod。 理论上，你可以直接连接到这些 Pod，但如果某个节点死掉了会发生什么呢？ Pod 会终止，Deployment 将创建新的 Pod，且使用不同的 IP。这正是 Service 要解决的问题。

Kubernetes Service 是集群中提供相同功能的一组 Pod 的抽象表达。 当每个 Service 创建时，会被分配一个唯一的 IP 地址（也称为 clusterIP）。 这个 IP 地址与 Service 的生命周期绑定在一起，只要 Service 存在，它就不会改变。 可以配置 Pod 使它与 Service 进行通信，Pod 知道与 Service 通信将被自动地负载均衡到该 Service 中的某些 Pod 上。

可以使用 ​`kubectl expose`​ 命令为 2个 Nginx 副本创建一个 Service：

`kubectl expose deployment/my-nginx`

`service/my-nginx exposed`

这等价于使用 ​`kubectl create -f`​ 命令及如下的 yaml 文件创建：

`apiVersion: v1 kind: Service metadata:   name: my-nginx   labels:     run: my-nginx spec:   ports:   - port: 80     protocol: TCP   selector:     run: my-nginx`

上述规约将创建一个 Service，该 Service 会将所有具有标签 ​`run: my-nginx`​ 的 Pod 的 TCP 80 端口暴露到一个抽象的 Service 端口上（​`targetPort`​：容器接收流量的端口；​`port`​：可任意取值的抽象的 Service 端口，其他 Pod 通过该端口访问 Service）。 查看 Service API 对象以了解 Service 所能接受的字段列表。 查看你的 Service 资源:

`kubectl get svc my-nginx`

    

`NAME       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE my-nginx   ClusterIP   10.0.162.149   <none>        80/TCP    21s`

正如前面所提到的，一个 Service 由一组 Pod 提供支撑。这些 Pod 通过 ​`endpoints` ​暴露出来。 Service Selector 将持续评估，结果被 POST 到一个名称为 ​`my-nginx`​ 的 Endpoint 对象上。 当 Pod 终止后，它会自动从 Endpoint 中移除，新的能够匹配上 Service Selector 的 Pod 将自动地被添加到 Endpoint 中。 检查该 Endpoint，注意到 IP 地址与在第一步创建的 Pod 是相同的。

`kubectl describe svc my-nginx`

        

`Name:                my-nginx Namespace:           default Labels:              run=my-nginx Annotations:         <none> Selector:            run=my-nginx Type:                ClusterIP IP:                  10.0.162.149 Port:                <unset> 80/TCP Endpoints:           10.244.2.5:80,10.244.3.4:80 Session Affinity:    None Events:              <none>`

`kubectl get ep my-nginx`

`NAME       ENDPOINTS                     AGE my-nginx   10.244.2.5:80,10.244.3.4:80   1m`

现在，你应该能够从集群中任意节点上使用 curl 命令向 ​`<CLUSTER-IP>:<PORT>`​ 发送请求以访问 Nginx Service。

访问 Service
----------

Kubernetes支持两种查找服务的主要模式: 环境变量和 DNS。前者开箱即用，而后者则需要 [CoreDNS 集群插件](https://github.com/kubernetes/kubernetes/tree/v1.24.0/cluster/addons/dns/coredns)

> 如果不需要服务环境变量（因为可能与预期的程序冲突，可能要处理的变量太多，或者仅使用DNS等），则可以通过在 ​`[pod spec](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/ target=)`​ 上将 ​`enableServiceLinks` ​标志设置为 ​`false` ​来禁用此模式。

### 环境变量

当 Pod 在节点上运行时，kubelet 会针对每个活跃的 Service 为 Pod 添加一组环境变量。 这就引入了一个顺序的问题。为解释这个问题，让我们先检查正在运行的 Nginx Pod 的环境变量（你的环境中的 Pod 名称将会与下面示例命令中的不同）：

`kubectl exec my-nginx-3800858182-jr4a2 -- printenv | grep SERVICE`

`KUBERNETES_SERVICE_HOST=10.0.0.1 KUBERNETES_SERVICE_PORT=443 KUBERNETES_SERVICE_PORT_HTTPS=443`

            

能看到环境变量中并没有你创建的 Service 相关的值。这是因为副本的创建先于 Service。 这样做的另一个缺点是，调度器可能会将所有 Pod 部署到同一台机器上，如果该机器宕机则整个 Service 都会离线。 要改正的话，我们可以先终止这 2 个 Pod，然后等待 Deployment 去重新创建它们。 这次 Service 会先于副本存在。这将实现调度器级别的 Pod 按 Service 分布（假定所有的节点都具有同样的容量），并提供正确的环境变量：

`kubectl scale deployment my-nginx --replicas=0; kubectl scale deployment my-nginx --replicas=2;  kubectl get pods -l run=my-nginx -o wide`

                

`NAME                        READY     STATUS    RESTARTS   AGE     IP            NODE my-nginx-3800858182-e9ihh   1/1       Running   0          5s      10.244.2.7    kubernetes-minion-ljyd my-nginx-3800858182-j4rm4   1/1       Running   0          5s      10.244.3.8    kubernetes-minion-905m`

你可能注意到，Pod 具有不同的名称，这是因为它们是被重新创建的。

`kubectl exec my-nginx-3800858182-e9ihh -- printenv | grep SERVICE`

`KUBERNETES_SERVICE_PORT=443 MY_NGINX_SERVICE_HOST=10.0.162.149 KUBERNETES_SERVICE_HOST=10.0.0.1 MY_NGINX_SERVICE_PORT=80 KUBERNETES_SERVICE_PORT_HTTPS=443`

### DNS

Kubernetes 提供了一个自动为其它 Service 分配 DNS 名字的 DNS 插件 Service。 你可以通过如下命令检查它是否在工作：

`kubectl get services kube-dns --namespace=kube-system`

`NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE kube-dns   ClusterIP   10.0.0.10    <none>        53/UDP,53/TCP   8m`

                    

本段剩余的内容假设你已经有一个拥有持久 IP 地址的 Service（my-nginx），以及一个为其 IP 分配名称的 DNS 服务器。 这里我们使用 CoreDNS 集群插件（应用名为 ​`kube-dns`​）， 所以在集群中的任何 Pod 中，你都可以使用标准方法（例如：​`gethostbyname()`​）与该 Service 通信。 如果 CoreDNS 没有在运行，你可以参照  [CoreDNS README](https://github.com/coredns/deployment/tree/master/kubernetes) 或者安装 CoreDNS 来启用它。 让我们运行另一个 curl 应用来进行测试：

`kubectl run curl --image=radial/busyboxplus:curl -i --tty`

`Waiting for pod default/curl-131556218-9fnch to be running, status is Pending, pod ready: false Hit enter for command prompt`

然后，按回车并执行命令 ​`nslookup my-nginx`​：

`[ root@curl-131556218-9fnch:/ ]$ nslookup my-nginx Server:    10.0.0.10 Address 1: 10.0.0.10  Name:      my-nginx Address 1: 10.0.162.149`

保护 Service
----------

到现在为止，我们只在集群内部访问了 Nginx 服务器。在将 Service 暴露到因特网之前，我们希望确保通信信道是安全的。 为实现这一目的，需要：

*   用于 HTTPS 的自签名证书（除非已经有了一个身份证书）
*   使用证书配置的 Nginx 服务器
*   使 Pod 可以访问证书的 Secret

你可以从 [Nginx https 示例](https://github.com/kubernetes/examples/tree/master/staging/https-nginx/)获取所有上述内容。 你需要安装 go 和 make 工具。如果你不想安装这些软件，可以按照后文所述的手动执行步骤执行操作。简要过程如下：

`make keys KEY=/tmp/nginx.key CERT=/tmp/nginx.crt kubectl create secret tls nginxsecret --key /tmp/nginx.key --cert /tmp/nginx.crt`

`secret/nginxsecret created`

`kubectl get secrets`

`NAME                  TYPE                                  DATA      AGE default-token-il9rc   kubernetes.io/service-account-token   1         1d nginxsecret           kubernetes.io/tls                     2         1m`

                        

以下是 configmap：

`kubectl create configmap nginxconfigmap --from-file=default.conf`

`configmap/nginxconfigmap created`

`kubectl get configmaps`

                            

`NAME             DATA   AGE nginxconfigmap   1      114s`

以下是你在运行 make 时遇到问题时要遵循的手动步骤（例如，在 Windows 上）：

`# 创建公钥和相对应的私钥 openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /d/tmp/nginx.key -out /d/tmp/nginx.crt -subj "/CN=my-nginx/O=my-nginx" # 对密钥实施 base64 编码 cat /d/tmp/nginx.crt | base64 cat /d/tmp/nginx.key | base64`

使用前面命令的输出来创建 yaml 文件，如下所示。 base64 编码的值应全部放在一行上。(由于示例数值过长这里用……替代)

`apiVersion: "v1" kind: "Secret" metadata:   name: "nginxsecret"   namespace: "default" type: kubernetes.io/tls   data:   tls.crt: "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0t……"   tls.key: "LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS0t……"`

现在使用文件创建 Secret：

`kubectl apply -f nginxsecrets.yaml kubectl get secrets`

`NAME                  TYPE                                  DATA      AGE default-token-il9rc   kubernetes.io/service-account-token   1         1d nginxsecret           kubernetes.io/tls                     2         1m`

现在修改 nginx 副本以启动一个使用 Secret 中的证书的 HTTPS 服务器以及相应的用于暴露其端口（80 和 443）的 Service：

`apiVersion: v1 kind: Service metadata:   name: my-nginx   labels:     run: my-nginx spec:   type: NodePort   ports:   - port: 8080     targetPort: 80     protocol: TCP     name: http   - port: 443     protocol: TCP     name: https   selector:     run: my-nginx --- apiVersion: apps/v1 kind: Deployment metadata:   name: my-nginx spec:   selector:     matchLabels:       run: my-nginx   replicas: 1   template:     metadata:       labels:         run: my-nginx     spec:       volumes:       - name: secret-volume         secret:           secretName: nginxsecret       - name: configmap-volume         configMap:           name: nginxconfigmap       containers:       - name: nginxhttps         image: bprashanth/nginxhttps:1.0         ports:         - containerPort: 443         - containerPort: 80         volumeMounts:         - mountPath: /etc/nginx/ssl           name: secret-volume         - mountPath: /etc/nginx/conf.d           name: configmap-volume`

关于 nginx-secure-app 清单，值得注意的几点如下：

*   它将 Deployment 和 Service 的规约放在了同一个文件中。
*   Nginx 服务器通过 80 端口处理 HTTP 流量，通过 443 端口处理 HTTPS 流量，而 Nginx Service 则暴露了这两个端口。
*   每个容器能通过挂载在 ​`/etc/nginx/ssl`​ 的卷访问秘钥。卷和密钥需要在 Nginx 服务器启动之前配置好。

`kubectl delete deployments,svc my-nginx; kubectl create -f ./nginx-secure-app.yaml`

这时，你可以从任何节点访问到 Nginx 服务器。

`kubectl get pods -o yaml | grep -i podip     podIP: 10.244.3.5 node $ curl -k https://10.244.3.5 ... <h1>Welcome to nginx!</h1>`

注意最后一步我们是如何提供 ​`-k`​ 参数执行 curl 命令的，这是因为在证书生成时， 我们不知道任何关于运行 nginx 的 Pod 的信息，所以不得不在执行 curl 命令时忽略 CName 不匹配的情况。 通过创建 Service，我们连接了在证书中的 CName 与在 Service 查询时被 Pod 使用的实际 DNS 名字。 让我们从一个 Pod 来测试（为了方便，这里使用同一个 Secret，Pod 仅需要使用 nginx.crt 去访问 Service）：

`apiVersion: apps/v1 kind: Deployment metadata:   name: curl-deployment spec:   selector:     matchLabels:       app: curlpod   replicas: 1   template:     metadata:       labels:         app: curlpod     spec:       volumes:       - name: secret-volume         secret:           secretName: nginxsecret       containers:       - name: curlpod         command:         - sh         - -c         - while true; do sleep 1; done         image: radial/busyboxplus:curl         volumeMounts:         - mountPath: /etc/nginx/ssl           name: secret-volume`

`kubectl apply -f ./curlpod.yaml kubectl get pods -l app=curlpod`

`NAME                               READY     STATUS    RESTARTS   AGE curl-deployment-1515033274-1410r   1/1       Running   0          1m`

`kubectl exec curl-deployment-1515033274-1410r -- curl https://my-nginx --cacert /etc/nginx/ssl/tls.crt ... <title>Welcome to nginx!</title> ...`

暴露 Service
----------

对应用的某些部分，你可能希望将 Service 暴露在一个外部 IP 地址上。 Kubernetes 支持两种实现方式：NodePort 和 LoadBalancer。 在上一段创建的 Service 使用了 ​`NodePort`​，因此，如果你的节点有一个公网 IP，那么 Nginx HTTPS 副本已经能够处理因特网上的流量。

`kubectl get svc my-nginx -o yaml | grep nodePort -C 5`

  `uid: 07191fb3-f61a-11e5-8ae5-42010af00002 spec:   clusterIP: 10.0.162.149   ports:   - name: http     nodePort: 31704     port: 8080     protocol: TCP     targetPort: 80   - name: https     nodePort: 32453     port: 443     protocol: TCP     targetPort: 443   selector:     run: my-nginx`

`kubectl get nodes -o yaml | grep ExternalIP -C 1`

  

    `- address: 104.197.41.11       type: ExternalIP     allocatable: --     - address: 23.251.152.56       type: ExternalIP     allocatable: ...  $ curl https://<EXTERNAL-IP>:<NODE-PORT> -k ... <h1>Welcome to nginx!</h1>`

让我们重新创建一个 Service 以使用云负载均衡器。 将 ​`my-nginx`​ Service 的 ​`Type` ​由 ​`NodePort` ​改成 ​`LoadBalancer`​：

`kubectl edit svc my-nginx kubectl get svc my-nginx`

`NAME       TYPE           CLUSTER-IP     EXTERNAL-IP        PORT(S)               AGE my-nginx   LoadBalancer   10.0.162.149   xx.xxx.xxx.xxx     8080:30163/TCP        21s`

`curl https://<EXTERNAL-IP> -k ... <title>Welcome to nginx!</title>`

在 ​`EXTERNAL-IP`​ 列中的 IP 地址能在公网上被访问到。​`CLUSTER-IP`​ 只能从集群/私有云网络中访问。

注意，在 AWS 上，类型 ​`LoadBalancer` ​的服务会创建一个 ELB，且 ELB 使用主机名（比较长），而不是 IP。 ELB 的主机名太长以至于不能适配标准 ​`kubectl get svc`​ 的输出，所以需要通过执行 ​`kubectl describe service my-nginx`​ 命令来查看它。 可以看到类似如下内容：

`kubectl describe service my-nginx ... LoadBalancer Ingress:   a320587ffd19711e5a37606cf4a74574-1142138393.us-east-1.elb.amazonaws.com ...`

##  5.  Kubernetes Ingress
Ingress
-------

FEATURE STATE: Kubernetes v1.19 \[stable\]

Ingress 是对集群中服务的外部访问进行管理的 API 对象，典型的访问方式是 HTTP。

Ingress 可以提供负载均衡、SSL 终结和基于名称的虚拟托管。

术语
--

为了表达更加清晰，本指南定义了以下术语：

*   节点（Node）: Kubernetes 集群中的一台工作机器，是集群的一部分。
*   集群（Cluster）: 一组运行由 Kubernetes 管理的容器化应用程序的节点。 在此示例和在大多数常见的 Kubernetes 部署环境中，集群中的节点都不在公共网络中。
*   边缘路由器（Edge Router）: 在集群中强制执行防火墙策略的路由器。可以是由云提供商管理的网关，也可以是物理硬件。
*   集群网络（Cluster Network）: 一组逻辑的或物理的连接，根据 Kubernetes 网络模型在集群内实现通信。
*   服务（Service）：Kubernetes 服务（Service）， 使用标签选择器（selectors）辨认一组 Pod。 除非另有说明，否则假定服务只具有在集群网络中可路由的虚拟 IP。

Ingress 是什么？
------------

[Ingress](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/ target=) 公开了从集群外部到集群内服务的 HTTP 和 HTTPS 路由。 流量路由由 Ingress 资源上定义的规则控制。

下面是一个将所有流量都发送到同一 Service 的简单 Ingress 示例：

![](https://atts.w3cschool.cn/attachments/image/20220506/1651816050984724.png)  

Ingress 可为 Service 提供外部可访问的 URL、负载均衡流量、终止 SSL/TLS，以及基于名称的虚拟托管。 Ingress 控制器 通常负责通过负载均衡器来实现 Ingress，尽管它也可以配置边缘路由器或其他前端来帮助处理流量。

Ingress 不会公开任意端口或协议。 将 HTTP 和 HTTPS 以外的服务公开到 Internet 时，通常使用 Service.Type=NodePort 或 Service.Type=LoadBalancer 类型的 Service。

环境准备
----

你必须拥有一个 Ingress 控制器 才能满足 Ingress 的要求。 仅创建 Ingress 资源本身没有任何效果。

你可能需要部署 Ingress 控制器，例如 [ingress-nginx](https://kubernetes.github.io/ingress-nginx/deploy/)。 你可以从许多 Ingress 控制器 中进行选择。

理想情况下，所有 Ingress 控制器都应符合参考规范。但实际上，不同的 Ingress 控制器操作略有不同。

> 确保你查看了 Ingress 控制器的文档，以了解选择它的注意事项。

Ingress 资源 
-----------

一个最小的 Ingress 资源示例：

`apiVersion: networking.k8s.io/v1 kind: Ingress metadata:   name: minimal-ingress   annotations:     nginx.ingress.kubernetes.io/rewrite-target: / spec:   ingressClassName: nginx-example   rules:   - http:       paths:       - path: /testpath         pathType: Prefix         backend:           service:             name: test             port:               number: 80`

Ingress 需要指定 ​`apiVersion`​、​`kind`​、 ​`metadata`​和 ​`spec` ​字段。 Ingress 对象的命名必须是合法的 DNS 子域名名称。 关于如何使用配置文件，请参见部署应用、 配置容器、 管理资源。 Ingress 经常使用注解（annotations）来配置一些选项，具体取决于 Ingress 控制器，例如 [重写目标注解](https://github.com/kubernetes/ingress-nginx/blob/main/docs/examples/rewrite/README.md)。 不同的 Ingress 控制器支持不同的注解。 查看你所选的 Ingress 控制器的文档，以了解其支持哪些注解。

Ingress [规约](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md target=) 提供了配置负载均衡器或者代理服务器所需的所有信息。 最重要的是，其中包含与所有传入请求匹配的规则列表。 Ingress 资源仅支持用于转发 HTTP(S) 流量的规则。

如果 ​`ingressClassName` ​被省略，那么你应该定义一个默认 Ingress 类。

有一些 Ingress 控制器不需要定义默认的 ​`IngressClass`​。比如：Ingress-NGINX 控制器可以通过[参数](https://kubernetes.github.io/ingress-nginx/ target=) ​`--watch-ingress-without-class`​ 来配置。 不过仍然 [推荐](https://kubernetes.github.io/ingress-nginx/ target=) 按下文所示来设置默认的 ​`IngressClass`​。

### Ingress 规则 

每个 HTTP 规则都包含以下信息：

*   可选的 ​`host`​。在此示例中，未指定 ​`host`​，因此该规则适用于通过指定 IP 地址的所有入站 HTTP 通信。 如果提供了 ​`host`​（例如 foo.bar.com），则 ​`rules` ​适用于该 ​`host`​。
*   路径列表 paths（例如，​`/testpath`​）,每个路径都有一个由 ​`serviceName` ​和 ​`servicePort` ​定义的关联后端。 在负载均衡器将流量定向到引用的服务之前，主机和路径都必须匹配传入请求的内容。
*   ​`backend`​（后端）是 Service 文档中所述的服务和端口名称的组合。 与规则的 ​`host` ​和 ​`path` ​匹配的对 Ingress 的 HTTP（和 HTTPS ）请求将发送到列出的 ​`backend`​。

通常在 Ingress 控制器中会配置 ​`defaultBackend`​（默认后端），以服务于无法与规约中 ​`path` ​匹配的所有请求。

### 默认后端 

没有设置规则的 Ingress 将所有流量发送到同一个默认后端，而 ​`.spec.defaultBackend`​ 则是在这种情况下处理请求的那个默认后端。 ​`defaultBackend` ​通常是 Ingress 控制器的配置选项，而非在 Ingress 资源中指定。 如果未设置任何的 ​`.spec.rules`​，那么必须指定 ​`.spec.defaultBackend`​。 如果未设置 ​`defaultBackend`​，那么如何处理所有与规则不匹配的流量将交由 Ingress 控制器决定（请参考你的 Ingress 控制器的文档以了解它是如何处理那些流量的）。

如果没有 ​`hosts` ​或 ​`paths` ​与 Ingress 对象中的 HTTP 请求匹配，则流量将被路由到默认后端。

### 资源后端 

​`Resource` ​后端是一个引用，指向同一命名空间中的另一个 Kubernetes 资源，将其作为 Ingress 对象。 ​`Resource` ​后端与 Service 后端是互斥的，在二者均被设置时会无法通过合法性检查。 ​`Resource` ​后端的一种常见用法是将所有入站数据导向带有静态资产的对象存储后端。

`apiVersion: networking.k8s.io/v1 kind: Ingress metadata:   name: ingress-resource-backend spec:   defaultBackend:     resource:       apiGroup: k8s.example.com       kind: StorageBucket       name: static-assets   rules:     - http:         paths:           - path: /icons             pathType: ImplementationSpecific             backend:               resource:                 apiGroup: k8s.example.com                 kind: StorageBucket                 name: icon-assets`

    

创建了如上的 Ingress 之后，你可以使用下面的命令查看它：

`kubectl describe ingress ingress-resource-backend`

`Name:             ingress-resource-backend Namespace:        default Address: Default backend:  APIGroup: k8s.example.com, Kind: StorageBucket, Name: static-assets Rules:   Host        Path  Backends   ----        ----  --------   *               /icons   APIGroup: k8s.example.com, Kind: StorageBucket, Name: icon-assets Annotations:  <none> Events:       <none>`

### 路径类型 

Ingress 中的每个路径都需要有对应的路径类型（Path Type）。未明确设置 ​`pathType` ​的路径无法通过合法性检查。当前支持的路径类型有三种：

*   ​`ImplementationSpecific`​：对于这种路径类型，匹配方法取决于 IngressClass。 具体实现可以将其作为单独的 ​`pathType` ​处理或者与 ​`Prefix` ​或 ​`Exact` ​类型作相同处理。
*   ​`Exact`​：精确匹配 URL 路径，且区分大小写。
*   ​`Prefix`​：基于以 ​`/`​ 分隔的 URL 路径前缀匹配。匹配区分大小写，并且对路径中的元素逐个完成。 路径元素指的是由 ​`/`​ 分隔符分隔的路径中的标签列表。 如果每个 p 都是请求路径 p 的元素前缀，则请求与路径 p 匹配。

> 如果路径的最后一个元素是请求路径中最后一个元素的子字符串，则不会匹配 （例如：​`/foo/bar`​ 匹配 ​`/foo/bar/baz`​, 但不匹配 ​`/foo/barbaz`​）。

### 示例

类型

路径

请求路径

匹配与否？

Prefix

`/`

（所有路径）

是

Exact

`/foo`

`/foo`

是

Exact

`/foo`

`/bar`

否

Exact

`/foo`

`/foo/`

否

Exact

`/foo/`

`/foo`

否

Prefix

`/foo`

`/foo`, `/foo/`

是

Prefix

`/foo/`

`/foo`, `/foo/`

是

Prefix

`/aaa/bb`

`/aaa/bbb`

否

Prefix

`/aaa/bbb`

`/aaa/bbb`

是

Prefix

`/aaa/bbb/`

`/aaa/bbb`

是，忽略尾部斜线

Prefix

`/aaa/bbb`

`/aaa/bbb/`

是，匹配尾部斜线

Prefix

`/aaa/bbb`

`/aaa/bbb/ccc`

是，匹配子路径

Prefix

`/aaa/bbb`

`/aaa/bbbxyz`

否，字符串前缀不匹配

Prefix

`/`, `/aaa`

`/aaa/ccc`

是，匹配 `/aaa` 前缀

Prefix

`/`, `/aaa`, `/aaa/bbb`

`/aaa/bbb`

是，匹配 `/aaa/bbb` 前缀

Prefix

`/`, `/aaa`, `/aaa/bbb`

`/ccc`

是，匹配 `/` 前缀

Prefix

`/aaa`

`/ccc`

否，使用默认后端

混合

`/foo` (Prefix), `/foo` (Exact)

`/foo`

是，优选 Exact 类型

#### 多重匹配 

在某些情况下，Ingress 中的多条路径会匹配同一个请求。 这种情况下最长的匹配路径优先。 如果仍然有两条同等的匹配路径，则精确路径类型优先于前缀路径类型。

主机名通配符 
-------

主机名可以是精确匹配（例如“​`foo.bar.com`​”）或者使用通配符来匹配 （例如“​`*.foo.com`​”）。 精确匹配要求 HTTP ​`host` ​头部字段与 ​`host` ​字段值完全匹配。 通配符匹配则要求 HTTP ​`host` ​头部字段与通配符规则中的后缀部分相同。

主机

host 头部

匹配与否？

`*.foo.com`

`bar.foo.com`

基于相同的后缀匹配

`*.foo.com`

`baz.bar.foo.com`

不匹配，通配符仅覆盖了一个 DNS 标签

`*.foo.com`

`foo.com`

不匹配，通配符仅覆盖了一个 DNS 标签

`apiVersion: networking.k8s.io/v1 kind: Ingress metadata:   name: ingress-wildcard-host spec:   rules:   - host: "foo.bar.com"     http:       paths:       - pathType: Prefix         path: "/bar"         backend:           service:             name: service1             port:               number: 80   - host: "*.foo.com"     http:       paths:       - pathType: Prefix         path: "/foo"         backend:           service:             name: service2             port:               number: 80`

Ingress 类 
----------

Ingress 可以由不同的控制器实现，通常使用不同的配置。 每个 Ingress 应当指定一个类，也就是一个对 IngressClass 资源的引用。 IngressClass 资源包含额外的配置，其中包括应当实现该类的控制器名称。

`apiVersion: networking.k8s.io/v1 kind: IngressClass metadata:   name: external-lb spec:   controller: example.com/ingress-controller   parameters:     apiGroup: k8s.example.com     kind: IngressParameters     name: external-lb`

IngressClass 中的 ​`.spec.parameters`​ 字段可用于引用其他资源以提供额外的相关配置。

参数（​`parameters`​）的具体类型取决于你在 ​`.spec.controller`​ 字段中指定的 Ingress 控制器。

### IngressClass 的作用域

取决于你的 Ingress 控制器，你可能可以使用集群范围设置的参数或某个名字空间范围的参数。

*   集群作用域

IngressClass 的参数默认是集群范围的。

如果你设置了 .spec.parameters 字段且未设置 .spec.parameters.scope 字段，或是将 .spec.parameters.scope 字段设为了 Cluster，那么该 IngressClass 所指代的即是一个集群作用域的资源。 参数的 kind（和 apiGroup 一起）指向一个集群作用域的 API（可能是一个定制资源（Custom Resource）），而它的 name 则为此 API 确定了一个具体的集群作用域的资源。

示例：

`--- apiVersion: networking.k8s.io/v1 kind: IngressClass metadata:   name: external-lb-1 spec:   controller: example.com/ingress-controller   parameters:     # 此 IngressClass 的配置定义在一个名为 “external-config-1” 的     # ClusterIngressParameter（API 组为 k8s.example.net）资源中。     # 这项定义告诉 Kubernetes 去寻找一个集群作用域的参数资源。     scope: Cluster     apiGroup: k8s.example.net     kind: ClusterIngressParameter     name: external-config-1`

*   命名空间作用域

FEATURE STATE: Kubernetes v1.23 \[stable\]

如果你设置了 ​`.spec.parameters`​ 字段且将 ​`.spec.parameters.scope`​ 字段设为了 ​`Namespace`​，那么该 IngressClass 将会引用一个命名空间作用域的资源。 ​`.spec.parameters.namespace`​ 必须和此资源所处的命名空间相同。

参数的 ​`kind`​（和 ​`apiGroup` ​一起）指向一个命名空间作用域的 API（例如：ConfigMap），而它的 ​`name` ​则确定了一个位于你指定的命名空间中的具体的资源。

命名空间作用域的参数帮助集群操作者将控制细分到用于工作负载的各种配置中（比如：负载均衡设置、API 网关定义）。如果你使用集群作用域的参数，那么你必须从以下两项中选择一项执行：

*   每次修改配置，集群操作团队需要批准其他团队的修改。
*   集群操作团队定义具体的准入控制，比如 RBAC 角色与角色绑定，以使得应用程序团队可以修改集群作用域的配置参数资源。

IngressClass API 本身是集群作用域的。

这里是一个引用命名空间作用域的配置参数的 IngressClass 的示例：

`--- apiVersion: networking.k8s.io/v1 kind: IngressClass metadata:   name: external-lb-2 spec:   controller: example.com/ingress-controller   parameters:     # 此 IngressClass 的配置定义在一个名为 “external-config” 的     # IngressParameter（API 组为 k8s.example.com）资源中，     # 该资源位于 “external-configuration” 命名空间中。     scope: Namespace     apiGroup: k8s.example.com     kind: IngressParameter     namespace: external-configuration     name: external-config`

### 废弃的注解 

在 Kubernetes 1.18 版本引入 IngressClass 资源和 ​`ingressClassName` ​字段之前，Ingress 类是通过 Ingress 中的一个 ​`kubernetes.io/ingress.class`​ 注解来指定的。 这个注解从未被正式定义过，但是得到了 Ingress 控制器的广泛支持。

Ingress 中新的 ​`ingressClassName` ​字段是该注解的替代品，但并非完全等价。 该注解通常用于引用实现该 Ingress 的控制器的名称，而这个新的字段则是对一个包含额外 Ingress 配置的 IngressClass 资源的引用，包括 Ingress 控制器的名称。

### 默认 Ingress 类 

你可以将一个特定的 IngressClass 标记为集群默认 Ingress 类。 将一个 IngressClass 资源的 ​`ingressclass.kubernetes.io/is-default-class`​ 注解设置为 ​`true` ​将确保新的未指定 ​`ingressClassName` ​字段的 Ingress 能够分配为这个默认的 IngressClass.

> 如果集群中有多个 IngressClass 被标记为默认，准入控制器将阻止创建新的未指定 ​`ingressClassName` ​的 Ingress 对象。 解决这个问题只需确保集群中最多只能有一个 IngressClass 被标记为默认。

有一些 Ingress 控制器不需要定义默认的 ​`IngressClass`​。比如：Ingress-NGINX 控制器可以通过[参数](https://kubernetes.github.io/ingress-nginx/ target=) ​`--watch-ingress-without-class`​ 来配置。 不过仍然[推荐](https://kubernetes.github.io/ingress-nginx/ target=) 设置默认的 ​`IngressClass`​。

`apiVersion: networking.k8s.io/v1 kind: IngressClass metadata:   labels:     app.kubernetes.io/component: controller   name: nginx-example   annotations:     ingressclass.kubernetes.io/is-default-class: "true" spec:   controller: k8s.io/ingress-nginx`

Ingress 类型 
-----------

### 由单个 Service 来完成的 Ingress 

现有的 Kubernetes 概念允许你暴露单个 Service。 你也可以通过指定无规则的 默认后端 来对 Ingress 进行此操作。

`apiVersion: networking.k8s.io/v1 kind: Ingress metadata:   name: test-ingress spec:   defaultBackend:     service:       name: test       port:         number: 80`

如果使用 ​`kubectl apply -f`​ 创建此 Ingress，则应该能够查看刚刚添加的 Ingress 的状态：

`kubectl get ingress test-ingress`

`NAME           CLASS         HOSTS   ADDRESS         PORTS   AGE test-ingress   external-lb   *       203.0.113.123   80      59s`

其中 ​`203.0.113.123`​ 是由 Ingress 控制器分配以满足该 Ingress 的 IP。

> 入口控制器和负载平衡器可能需要一两分钟才能分配 IP 地址。 在此之前，你通常会看到地址字段的值被设定为 ​ `<pending>`​。

### 简单扇出 

一个扇出（fanout）配置根据请求的 HTTP URI 将来自同一 IP 地址的流量路由到多个 Service。 Ingress 允许你将负载均衡器的数量降至最低。例如，这样的设置：

![](https://atts.w3cschool.cn/attachments/image/20220506/1651817058967921.png)  

将需要一个如下所示的 Ingress：

`apiVersion: networking.k8s.io/v1 kind: Ingress metadata:   name: simple-fanout-example spec:   rules:   - host: foo.bar.com     http:       paths:       - path: /foo         pathType: Prefix         backend:           service:             name: service1             port:               number: 4200       - path: /bar         pathType: Prefix         backend:           service:             name: service2             port:               number: 8080`

当你使用 ​`kubectl apply -f`​ 创建 Ingress 时：

`kubectl describe ingress simple-fanout-example`

`Name:             simple-fanout-example Namespace:        default Address:          178.91.123.132 Default backend:  default-http-backend:80 (10.8.2.3:8080) Rules:   Host         Path  Backends   ----         ----  --------   foo.bar.com                /foo   service1:4200 (10.8.0.90:4200)                /bar   service2:8080 (10.8.0.91:8080) Annotations:   nginx.ingress.kubernetes.io/rewrite-target:  / Events:   Type     Reason  Age                From                     Message   ----     ------  ----               ----                     -------   Normal   ADD     22s                loadbalancer-controller  default/test`

Ingress 控制器将提供实现特定的负载均衡器来满足 Ingress， 只要 Service (​`service1`​，​`service2`​) 存在。 当它这样做时，你会在 Address 字段看到负载均衡器的地址。

> 取决于你所使用的 Ingress 控制器， 你可能需要创建默认 HTTP 后端服务。

### 基于名称的虚拟托管 

基于名称的虚拟主机支持将针对多个主机名的 HTTP 流量路由到同一 IP 地址上。

![](https://atts.w3cschool.cn/attachments/image/20220506/1651817163509756.png)  

以下 Ingress 让后台负载均衡器基于[host 头部字段](https://datatracker.ietf.org/doc/html/rfc7230 target=) 来路由请求。

`apiVersion: networking.k8s.io/v1 kind: Ingress metadata:   name: name-virtual-host-ingress spec:   rules:   - host: foo.bar.com     http:       paths:       - pathType: Prefix         path: "/"         backend:           service:             name: service1             port:               number: 80   - host: bar.foo.com     http:       paths:       - pathType: Prefix         path: "/"         backend:           service:             name: service2             port:               number: 80`

如果你创建的 Ingress 资源没有在 ​`rules` ​中定义的任何 ​`hosts`​，则可以匹配指向 Ingress 控制器 IP 地址的任何网络流量，而无需基于名称的虚拟主机。

例如，以下 Ingress 会将请求 ​`first.bar.com`​ 的流量路由到 ​`service1`​，将请求 ​`second.bar.com`​ 的流量路由到 ​`service2`​，而所有其他流量都会被路由到 ​`service3`​。

`apiVersion: networking.k8s.io/v1 kind: Ingress metadata:   name: name-virtual-host-ingress-no-third-host spec:   rules:   - host: first.bar.com     http:       paths:       - pathType: Prefix         path: "/"         backend:           service:             name: service1             port:               number: 80   - host: second.bar.com     http:       paths:       - pathType: Prefix         path: "/"         backend:           service:             name: service2             port:               number: 80   - http:       paths:       - pathType: Prefix         path: "/"         backend:           service:             name: service3             port:               number: 80`

### TLS

你可以通过设定包含 TLS 私钥和证书的Secret 来保护 Ingress。 Ingress 只支持单个 TLS 端口 443，并假定 TLS 连接终止于 Ingress 节点（与 Service 及其 Pod 之间的流量都以明文传输）。 如果 Ingress 中的 TLS 配置部分指定了不同的主机，那么它们将根据通过 SNI TLS 扩展指定的主机名（如果 Ingress 控制器支持 SNI）在同一端口上进行复用。 TLS Secret 的数据中必须包含用于 TLS 的以键名 ​`tls.crt`​ 保存的证书和以键名 ​`tls.key`​ 保存的私钥。 例如：

`apiVersion: v1 kind: Secret metadata:   name: testsecret-tls   namespace: default data:   tls.crt: base64 编码的证书   tls.key: base64 编码的私钥 type: kubernetes.io/tls`

在 Ingress 中引用此 Secret 将会告诉 Ingress 控制器使用 TLS 加密从客户端到负载均衡器的通道。 你需要确保创建的 TLS Secret 创建自包含 ​`https-example.foo.com`​ 的公用名称（CN）的证书。 这里的公共名称也被称为全限定域名（FQDN）。

> 注意，默认规则上无法使用 TLS，因为需要为所有可能的子域名发放证书。 因此，​`tls` ​字段中的 ​`hosts` ​的取值需要与 ​`rules` ​字段中的 ​`host` ​完全匹配。

`apiVersion: networking.k8s.io/v1 kind: Ingress metadata:   name: tls-example-ingress spec:   tls:   - hosts:       - https-example.foo.com     secretName: testsecret-tls   rules:   - host: https-example.foo.com     http:       paths:       - path: /         pathType: Prefix         backend:           service:             name: service1             port:               number: 80`

> 各种 Ingress 控制器所支持的 TLS 功能之间存在差异。请参阅有关 [nginx](https://kubernetes.github.io/ingress-nginx/user-guide/tls/)、 [GCE](https://github.com/kubernetes/ingress-gce/blob/master/README.md target=) 或者任何其他平台特定的 Ingress 控制器的文档，以了解 TLS 如何在你的环境中工作。

### 负载均衡 

Ingress 控制器启动引导时使用一些适用于所有 Ingress 的负载均衡策略设置，例如负载均衡算法、后端权重方案等。 更高级的负载均衡概念（例如持久会话、动态权重）尚未通过 Ingress 公开。 你可以通过用于服务的负载均衡器来获取这些功能。

值得注意的是，尽管健康检查不是通过 Ingress 直接暴露的，在 Kubernetes 中存在并行的概念，比如 就绪检查， 允许你实现相同的目的。 请检查特定控制器的说明文档（[nginx](https://github.com/kubernetes/ingress-nginx/blob/main/README.md)、 [GCE](https://github.com/kubernetes/ingress-gce/blob/master/README.md target=)）以了解它们是怎样处理健康检查的。

更新 Ingress 
-----------

要更新现有的 Ingress 以添加新的 Host，可以通过编辑资源来对其进行更新：

`kubectl describe ingress test`

`Name:             test Namespace:        default Address:          178.91.123.132 Default backend:  default-http-backend:80 (10.8.2.3:8080) Rules:   Host         Path  Backends   ----         ----  --------   foo.bar.com                /foo   service1:80 (10.8.0.90:80) Annotations:   nginx.ingress.kubernetes.io/rewrite-target:  / Events:   Type     Reason  Age                From                     Message   ----     ------  ----               ----                     -------   Normal   ADD     35s                loadbalancer-controller  default/test`

`kubectl edit ingress test`

这一命令将打开编辑器，允许你以 YAML 格式编辑现有配置。 修改它来增加新的主机：

`spec:   rules:   - host: foo.bar.com     http:       paths:       - backend:           serviceName: service1           servicePort: 80         path: /foo         pathType: Prefix   - host: bar.baz.com     http:       paths:       - backend:           serviceName: service2           servicePort: 80         path: /foo         pathType: Prefix ..`

保存更改后，kubectl 将更新 API 服务器中的资源，该资源将告诉 Ingress 控制器重新配置负载均衡器。

验证：

`kubectl describe ingress test`

`Name:             test Namespace:        default Address:          178.91.123.132 Default backend:  default-http-backend:80 (10.8.2.3:8080) Rules:   Host         Path  Backends   ----         ----  --------   foo.bar.com                /foo   service1:80 (10.8.0.90:80)   bar.baz.com                /foo   service2:80 (10.8.0.91:80) Annotations:   nginx.ingress.kubernetes.io/rewrite-target:  / Events:   Type     Reason  Age                From                     Message   ----     ------  ----               ----                     -------   Normal   ADD     45s                loadbalancer-controller  default/test`

你也可以通过 ​`kubectl replace -f`​ 命令调用修改后的 Ingress yaml 文件来获得同样的结果。

跨可用区失败 
-------

不同的云厂商使用不同的技术来实现跨故障域的流量分布。

替代方案 
-----

不直接使用 Ingress 资源，也有多种方法暴露 Service：

*   使用 Service.Type=LoadBalancer
*   使用 Service.Type=NodePort

##  6.  Kubernetes Ingress 控制器
Ingress 控制器
-----------

为了让 Ingress 资源工作，集群必须有一个正在运行的 Ingress 控制器。

与作为 ​`kube-controller-manager`​ 可执行文件的一部分运行的其他类型的控制器不同， Ingress 控制器不是随集群自动启动的。 基于此页面，你可选择最适合你的集群的 ingress 控制器实现。

Kubernetes 作为一个项目，目前支持和维护 [AWS](https://github.com/kubernetes-sigs/aws-load-balancer-controller target=)、 [GCE](https://github.com/kubernetes/ingress-gce/blob/master/README.md) 和 [Nginx](https://github.com/kubernetes/ingress-nginx/blob/main/README.md target=) Ingress 控制器。

其他控制器
-----

*   [AKS 应用程序网关 Ingress 控制器](https://docs.microsoft.com/zh-cn/azure/application-gateway/tutorial-ingress-controller-add-on-existing?toc=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fazure%2Faks%2Ftoc.json&bc=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fazure%2Fbread%2Ftoc.json) 是一个配置 [Azure 应用程序网关](https://docs.microsoft.com/zh-cn/azure/application-gateway/overview) 的 Ingress 控制器。
*   [Ambassador](https://www.getambassador.io/) API 网关是一个基于 [Envoy](https://www.envoyproxy.io/) 的 Ingress 控制器。
*   [Apache APISIX Ingress 控制器](https://github.com/apache/apisix-ingress-controller) 是一个基于 [Apache APISIX 网关](https://github.com/apache/apisix) 的 Ingress 控制器。
*   [Avi Kubernetes Operator](https://github.com/vmware/load-balancer-and-ingress-services-for-kubernetes) 使用 [VMware NSX Advanced Load Balancer](https://avinetworks.com/) 提供第 4 到第 7 层的负载均衡。
*   [BFE Ingress 控制器](https://github.com/bfenetworks/ingress-bfe)是一个基于 [BFE](https://www.bfe-networks.net/) 的 Ingress 控制器。
*   [Citrix Ingress 控制器](https://github.com/citrix/citrix-k8s-ingress-controller target=) 可以用来与 Citrix Application Delivery Controller 一起使用。
*   [Contour](https://projectcontour.io/) 是一个基于 [Envoy](https://www.envoyproxy.io/) 的 Ingress 控制器。
*   [EnRoute](https://getenroute.io/) 是一个基于 [Envoy](https://www.envoyproxy.io/) 的 API 网关，可以用作 Ingress 控制器。
*   [Easegress IngressController](https://github.com/megaease/easegress/blob/main/doc/reference/ingresscontroller.md) 是一个基于 [Easegress](https://megaease.com/easegress/) 的 API 网关，可以用作 Ingress 控制器。
*   F5 BIG-IP 的 [用于 Kubernetes 的容器 Ingress 服务](https://clouddocs.f5.com/products/connectors/k8s-bigip-ctlr/latest) 让你能够使用 Ingress 来配置 F5 BIG-IP 虚拟服务器。
*   [Gloo](https://gloo.solo.io/) 是一个开源的、基于 [Envoy](https://www.envoyproxy.io/) 的 Ingress 控制器，能够提供 API 网关功能。
*   [HAProxy Ingress](https://haproxy-ingress.github.io/) 是一个针对 [HAProxy](https://www.haproxy.org/ target=) 的 Ingress 控制器。
*   [用于 Kubernetes 的 HAProxy Ingress 控制器](https://github.com/haproxytech/kubernetes-ingress target=) 也是一个针对 [HAProxy](https://www.haproxy.org/ target=) 的 Ingress 控制器。
*   [Istio Ingress](https://istio.io/latest/docs/tasks/traffic-management/ingress/kubernetes-ingress/) 是一个基于 [Istio](https://istio.io/) 的 Ingress 控制器。
*   [用于 Kubernetes 的 Kong Ingress 控制器](https://github.com/Kong/kubernetes-ingress-controller target=) 是一个用来驱动 [Kong Gateway](https://konghq.com/kong/) 的 Ingress 控制器。
*   [用于 Kubernetes 的 NGINX Ingress 控制器](https://www.nginx.com/products/nginx-ingress-controller/) 能够与 [NGINX](https://www.nginx.com/resources/glossary/nginx/) 网页服务器（作为代理）一起使用。
*   [Pomerium Ingress 控制器](https://www.pomerium.com/docs/k8s/ingress.html) 基于 [Pomerium](https://www.pomerium.com/)，能提供上下文感知的准入策略。
*   [Skipper](https://opensource.zalando.com/skipper/kubernetes/ingress-controller/) HTTP 路由器和反向代理可用于服务组装，支持包括 Kubernetes Ingress 这类使用场景，是一个用以构造你自己的定制代理的库。
*   [Traefik Kubernetes Ingress 提供程序](https://doc.traefik.io/traefik/providers/kubernetes-ingress/) 是一个用于 [Traefik](https://traefik.io/traefik/) 代理的 Ingress 控制器。
*   [Tyk Operator](https://github.com/TykTechnologies/tyk-operator) 使用自定义资源扩展 Ingress，为之带来 API 管理能力。Tyk Operator 使用开源的 Tyk Gateway & Tyk Cloud 控制面。
*   [Voyager](https://appscode.com/products/voyager) 是一个针对 [HAProxy](https://www.haproxy.org/ target=) 的 Ingress 控制器。

使用多个 Ingress 控制器
----------------

你可以使用 Ingress 类在集群中部署任意数量的 Ingress 控制器。 请注意你的 Ingress 类资源的 ​`.metadata.name`​ 字段。 当你创建 Ingress 时，你需要用此字段的值来设置 Ingress 对象的 ​`ingressClassName` ​字段。 ​`ingressClassName` ​是之前的注解做法的替代。

如果你不为 Ingress 指定一个 IngressClass，并且你的集群中只有一个 IngressClass 被标记为了集群默认，那么 Kubernetes 会应用此默认 IngressClass。 你可以通过将 ​`ingressclass.kubernetes.io/is-default-class`​ 注解 的值设置为 ​`"true"`​ 来将一个 IngressClass 标记为集群默认。

理想情况下，所有 Ingress 控制器都应满足此规范，但各种 Ingress 控制器的操作略有不同。

> 确保你查看了 ingress 控制器的文档，以了解选择它的注意事项。

##  7.  Kubernetes 拓扑感知提示
拓扑感知提示
------

FEATURE STATE: Kubernetes v1.23 \[beta\]

拓扑感知提示 包含客户怎么使用服务端点的建议，从而实现了拓扑感知的路由功能。 这种方法添加了元数据，以启用 EndpointSlice 和/或 Endpoints 对象的调用者， 这样，访问这些网络端点的请求流量就可以在它的发起点附近就近路由。

例如，你可以在一个地域内路由流量，以降低通信成本，或提高网络性能。

> “拓扑感知提示”特性处于 Beta 阶段，并且默认情况下未启用。 要试用此特性，你必须启用 ​`TopologyAwareHints` ​特性门控。

动机
--

Kubernetes 集群越来越多的部署到多区域环境中。 拓扑感知提示 提供了一种把流量限制在它的发起区域之内的机制。 这个概念一般被称之为 “拓扑感知路由”。 在计算 服务（Service） 的端点时， EndpointSlice 控制器会评估每一个端点的拓扑（地域和区域），填充提示字段，并将其分配到某个区域。 集群组件，例如kube-proxy 就可以使用这些提示信息，并用他们来影响流量的路由（倾向于拓扑上相邻的端点）。

使用拓扑感知提示
--------

你可以通过把注解 ​`service.kubernetes.io/topology-aware-hints`​ 的值设置为 ​`auto`​， 来激活服务的拓扑感知提示功能。 这告诉 EndpointSlice 控制器在它认为安全的时候来设置拓扑提示。 重要的是，这并不能保证总会设置提示（hints）。

工作原理 
-----

此特性启用的功能分为两个组件：EndpointSlice 控制器和 kube-proxy。 本节概述每个组件如何实现此特性。

### EndpointSlice 控制器

此特性开启后，EndpointSlice 控制器负责在 EndpointSlice 上设置提示信息。 控制器按比例给每个区域分配一定比例数量的端点。 这个比例来源于此区域中运行节点的 可分配 CPU 核心数。 例如，如果一个区域拥有 2 CPU 核心，而另一个区域只有 1 CPU 核心， 那控制器将给那个有 2 CPU 的区域分配两倍数量的端点。

以下示例展示了提供提示信息后 EndpointSlice 的样子：

`apiVersion: discovery.k8s.io/v1 kind: EndpointSlice metadata:   name: example-hints   labels:     kubernetes.io/service-name: example-svc addressType: IPv4 ports:   - name: http     protocol: TCP     port: 80 endpoints:   - addresses:       - "10.1.2.3"     conditions:       ready: true     hostname: pod-1     zone: zone-a     hints:       forZones:         - name: "zone-a"`

### kube-proxy

kube-proxy 组件依据 EndpointSlice 控制器设置的提示，过滤由它负责路由的端点。 在大多数场合，这意味着 kube-proxy 可以把流量路由到同一个区域的端点。 有时，控制器从某个不同的区域分配端点，以确保在多个区域之间更平均的分配端点。 这会导致部分流量被路由到其他区域。

保护措施
----

Kubernetes 控制平面和每个节点上的 kube-proxy，在使用拓扑感知提示功能前，会应用一些保护措施规则。 如果没有检出，kube-proxy 将无视区域限制，从集群中的任意节点上选择端点。

1.  端点数量不足： 如果一个集群中，端点数量少于区域数量，控制器不创建任何提示。
2.  不可能实现均衡分配： 在一些场合中，不可能实现端点在区域中的平衡分配。 例如，假设 zone-a 比 zone-b 大两倍，但只有 2 个端点， 那分配到 zone-a 的端点可能收到比 zone-b多两倍的流量。 如果控制器不能确定此“期望的过载”值低于每一个区域可接受的阈值，控制器将不指派提示信息。 重要的是，这不是基于实时反馈。所以对于单独的端点仍有可能超载。
3.  一个或多个节点信息不足： 如果任一节点没有设置标签 ​`topology.kubernetes.io/zone`​， 或没有上报可分配的 CPU 数据，控制平面将不会设置任何拓扑感知提示， 继而 kube-proxy 也就不能通过区域过滤端点。
4.  一个或多个端点没有设置区域提示： 当这类事情发生时， kube-proxy 会假设这是正在执行一个从/到拓扑感知提示的转移。 在这种场合下过滤Service 的端点是有风险的，所以 kube-proxy 回撤为使用所有的端点。
5.  不在提示中的区域： 如果 kube-proxy 不能根据一个指示在它所在的区域中发现一个端点， 它回撤为使用所有节点的端点。当你的集群新增一个新的区域时，这种情况发生概率很高。

限制
--

*   当 Service 的 ​`externalTrafficPolicy` ​或 ​`internalTrafficPolicy` ​设置值为 ​`Local` ​时， 拓扑感知提示功能不可用。 你可以在一个集群的不同服务中使用这两个特性，但不能在同一个服务中这么做。
*   这种方法不适用于大部分流量来自于一部分区域的服务。 相反的，这里假设入站流量将根据每个区域中节点的服务能力按比例的分配。
*   EndpointSlice 控制器在计算每一个区域的容量比例时，会忽略未就绪的节点。 在大量节点未就绪的场景下，这样做会带来非预期的结果。
*   EndpointSlice 控制器在计算每一个区域的部署比例时，并不会考虑 容忍度。 如果服务后台的 Pod 被限制只能运行在集群节点的一个子集上，这些信息并不会被使用。
*   这种方法和自动扩展机制之间不能很好的协同工作。例如，如果大量流量来源于一个区域， 那只有分配到该区域的端点才可用来处理流量。这会导致 Pod 自动水平扩展 要么不能拾取此事件，要么新增 Pod 被启动到其他区域。

##  8.  Kubernetes 服务内部流量策略
服务内部流量策略
--------

FEATURE STATE: Kubernetes v1.23 \[beta\]

服务内部流量策略 开启了内部流量限制，只路由内部流量到和发起方处于相同节点的服务端点。 这里的”内部“流量指当前集群中的 Pod 所发起的流量。 这种机制有助于节省开销，提升效率。

使用服务内部流量策略
----------

​`ServiceInternalTrafficPolicy` ​特性门控 是 Beta 功能，默认启用。 启用该功能后，你就可以通过将 Services 的 ​`.spec.internalTrafficPolicy`​ 项设置为 ​`Local`​， 来为它指定一个内部专用的流量策略。 此设置就相当于告诉 kube-proxy 对于集群内部流量只能使用本地的服务端口。

> 如果某节点上的 Pod 均不提供指定 Service 的服务端点， 即使该 Service 在其他节点上有可用的服务端点， Service 的行为看起来也像是它只有 0 个服务端点（只针对此节点上的 Pod）。

以下示例展示了把 Service 的 ​`.spec.internalTrafficPolicy`​ 项设为 ​`Local` ​时， Service 的样子：

`apiVersion: v1 kind: Service metadata:   name: my-service spec:   selector:     app: MyApp   ports:     - protocol: TCP       port: 80       targetPort: 9376   internalTrafficPolicy: Local`

工作原理
----

kube-proxy 基于 ​`spec.internalTrafficPolicy`​ 的设置来过滤路由的目标服务端点。 当它的值设为 ​`Local` ​时，只选择节点本地的服务端点。 当它的值设为 ​`Cluster` ​或缺省时，则选择所有的服务端点。 启用特性门控 ​`ServiceInternalTrafficPolicy` ​后， ​`spec.internalTrafficPolicy`​ 的值默认设为 ​`Cluster`​。

限制
--

*   在一个Service上，当 ​`externalTrafficPolicy` ​已设置为 ​`Local`​时，服务内部流量策略无法使用。 换句话说，在一个集群的不同 Service 上可以同时使用这两个特性，但在一个 Service 上不行。

##  9.  Kubernetes 端点切片（Endpoint Slices）
端点切片（Endpoint Slices）
---------------------

FEATURE STATE: Kubernetes v1.21 \[stable\]

端点切片（EndpointSlices） 提供了一种简单的方法来跟踪 Kubernetes 集群中的网络端点 （network endpoints）。它们为 Endpoints 提供了一种可伸缩和可拓展的替代方案。

动机 
---

Endpoints API 提供了在 Kubernetes 跟踪网络端点的一种简单而直接的方法。 不幸的是，随着 Kubernetes 集群和 服务 逐渐开始为更多的后端 Pods 处理和发送请求，原来的 API 的局限性变得越来越明显。 最重要的是那些因为要处理大量网络端点而带来的挑战。

由于任一服务的所有网络端点都保存在同一个 Endpoints 资源中，这类资源可能变得 非常巨大，而这一变化会影响到 Kubernetes 组件（比如主控组件）的性能，并 在 Endpoints 变化时产生大量的网络流量和额外的处理。 EndpointSlice 能够帮助你缓解这一问题，还能为一些诸如拓扑路由这类的额外 功能提供一个可扩展的平台。

Endpoint Slice 资源 
------------------

在 Kubernetes 中，​`EndpointSlice` ​包含对一组网络端点的引用。 指定选择器后控制面会自动为设置了 选择算符 的 Kubernetes 服务创建 EndpointSlice。 这些 EndpointSlice 将包含对与服务选择算符匹配的所有 Pod 的引用。 EndpointSlice 通过唯一的协议、端口号和服务名称将网络端点组织在一起。 EndpointSlice 的名称必须是合法的 DNS 子域名。

例如，下面是 Kubernetes 服务 ​`example` ​的 EndpointSlice 资源示例。

`apiVersion: discovery.k8s.io/v1 kind: EndpointSlice metadata:   name: example-abc   labels:     kubernetes.io/service-name: example addressType: IPv4 ports:   - name: http     protocol: TCP     port: 80 endpoints:   - addresses:     - "10.1.2.3"     conditions:       ready: true     hostname: pod-1     nodeName: node-1     zone: us-west2-a`

默认情况下，控制面创建和管理的 EndpointSlice 将包含不超过 100 个端点。 你可以使用 kube-controller-manager 的 ​`--max-endpoints-per-slice`​ 标志设置此值，最大值为 1000。

当涉及如何路由内部流量时，EndpointSlice 可以充当 kube-proxy 的决策依据。 启用该功能后，在服务的端点数量庞大时会有可观的性能提升。

地址类型
----

EndpointSlice 支持三种地址类型：

*   IPv4
*   IPv6
*   FQDN (完全合格的域名)

### 状况

EndpointSlice API 存储了可能对使用者有用的、有关端点的状况。 这三个状况分别是 ​`ready`​、​`serving` ​和 ​`terminating`​。

#### Ready（就绪）

​`ready` ​状况是映射 Pod 的 ​`Ready` ​状况的。 处于运行中的 Pod，它的 ​`Ready` ​状况被设置为 ​`True`​，应该将此 EndpointSlice 状况也设置为 ​`true`​。 出于兼容性原因，当 Pod 处于终止过程中，​`ready` ​永远不会为 ​`true`​。 消费者应参考 ​`serving` ​状况来检查处于终止中的 Pod 的就绪情况。 该规则的唯一例外是将 ​`spec.publishNotReadyAddresses`​ 设置为 ​`true` ​的服务。 这些服务（Service）的端点将始终将 ​`ready` ​状况设置为 ​`true`​。

#### Serving（服务中）

FEATURE STATE: Kubernetes v1.20 \[alpha\]

​`serving` ​状况与 ​`ready` ​状况相同，不同之处在于它不考虑终止状态。 如果 EndpointSlice API 的使用者关心 Pod 终止时的就绪情况，就应检查此状况。

> 尽管 ​`serving` ​与 ​`ready` ​几乎相同，但是它是为防止破坏 ​`ready` ​的现有含义而增加的。 如果对于处于终止中的端点，​`ready` ​可能是 ​`true`​，那么对于现有的客户端来说可能是有些意外的， 因为从始至终，Endpoints 或 EndpointSlice API 从未包含处于终止中的端点。 出于这个原因，​`ready` ​对于处于终止中的端点 总是 ​`false`​， 并且在 v1.20 中添加了新的状况 ​`serving`​，以便客户端可以独立于 ​`ready` ​的现有语义来跟踪处于终止中的 Pod 的就绪情况。

#### Terminating（终止中）

FEATURE STATE: Kubernetes v1.20 \[alpha\]

​`Terminating` ​是表示端点是否处于终止中的状况。 对于 Pod 来说，这是设置了删除时间戳的 Pod。

### 拓扑信息 

EndpointSlice 中的每个端点都可以包含一定的拓扑信息。 拓扑信息包括端点的位置，对应节点、可用区的信息。 这些信息体现为 EndpointSlices 的如下端点字段：

*   ​`nodeName` ​- 端点所在的 Node 名称；
*   ​`zone` ​- 端点所处的可用区。

> 在 v1 API 中，逐个端点设置的 ​`topology` ​实际上被去除，以鼓励使用专用 的字段 ​`nodeName` ​和 ​`zone`​。  
> 对 ​`EndpointSlice` ​对象的 ​`endpoint` ​字段设置任意的拓扑结构信息这一操作已被 废弃，不再被 v1 API 所支持。取而代之的是 v1 API 所支持的 ​`nodeName` ​和 ​`zone` ​这些独立的字段。这些字段可以在不同的 API 版本之间自动完成转译。 例如，v1beta1 API 中 ​`topology` ​字段的 ​`topology.kubernetes.io/zone`​ 取值可以 在 v1 API 中通过 ​`zone` ​字段访问。

### 管理 

通常，控制面（尤其是端点切片的 控制器） 会创建和管理 EndpointSlice 对象。EndpointSlice 对象还有一些其他使用场景， 例如作为服务网格（Service Mesh）的实现。这些场景都会导致有其他实体 或者控制器负责管理额外的 EndpointSlice 集合。

为了确保多个实体可以管理 EndpointSlice 而且不会相互产生干扰，Kubernetes 定义了 标签 ​`endpointslice.kubernetes.io/managed-by`​，用来标明哪个实体在管理某个 EndpointSlice。端点切片控制器会在自己所管理的所有 EndpointSlice 上将该标签值设置 为 ​`endpointslice-controller.k8s.io`​。 管理 EndpointSlice 的其他实体也应该为此标签设置一个唯一值。

### 属主关系 

在大多数场合下，EndpointSlice 都由某个 Service 所有，（因为）该端点切片正是 为该服务跟踪记录其端点。这一属主关系是通过为每个 EndpointSlice 设置一个 属主（owner）引用，同时设置 ​`kubernetes.io/service-name`​ 标签来标明的， 目的是方便查找隶属于某服务的所有 EndpointSlice。

### EndpointSlice 镜像 

在某些场合，应用会创建定制的 Endpoints 资源。为了保证这些应用不需要并发 的更改 Endpoints 和 EndpointSlice 资源，集群的控制面将大多数 Endpoints 映射到对应的 EndpointSlice 之上。

控制面对 Endpoints 资源进行映射的例外情况有：

*   Endpoints 资源上标签 ​`endpointslice.kubernetes.io/skip-mirror`​ 值为 ​`true`​。
*   Endpoints 资源包含标签 ​`control-plane.alpha.kubernetes.io/leader`​。
*   对应的 Service 资源不存在。
*   对应的 Service 的选择算符不为空。

每个 Endpoints 资源可能会被翻译到多个 EndpointSlices 中去。 当 Endpoints 资源中包含多个子网或者包含多个 IP 地址族（IPv4 和 IPv6）的端点时， 就有可能发生这种状况。 每个子网最多有 1000 个地址会被镜像到 EndpointSlice 中。

### EndpointSlices 的分布问题 

每个 EndpointSlice 都有一组端口值，适用于资源内的所有端点。 当为服务使用命名端口时，Pod 可能会就同一命名端口获得不同的端口号，因而需要 不同的 EndpointSlice。这有点像 Endpoints 用来对子网进行分组的逻辑。

控制面尝试尽量将 EndpointSlice 填满，不过不会主动地在若干 EndpointSlice 之间 执行再平衡操作。这里的逻辑也是相对直接的：

1.  列举所有现有的 EndpointSlices，移除那些不再需要的端点并更新那些已经 变化的端点。
2.  列举所有在第一步中被更改过的 EndpointSlices，用新增加的端点将其填满。
3.  如果还有新的端点未被添加进去，尝试将这些端点添加到之前未更改的切片中， 或者创建新切片。

这里比较重要的是，与在 EndpointSlice 之间完成最佳的分布相比，第三步中更看重 限制 EndpointSlice 更新的操作次数。例如，如果有 10 个端点待添加，有两个 EndpointSlice 中各有 5 个空位，上述方法会创建一个新的 EndpointSlice 而不是 将现有的两个 EndpointSlice 都填满。换言之，与执行多个 EndpointSlice 更新操作 相比较，方法会优先考虑执行一个 EndpointSlice 创建操作。

由于 kube-proxy 在每个节点上运行并监视 EndpointSlice 状态，EndpointSlice 的 每次变更都变得相对代价较高，因为这些状态变化要传递到集群中每个节点上。 这一方法尝试限制要发送到所有节点上的变更消息个数，即使这样做可能会导致有 多个 EndpointSlice 没有被填满。

在实践中，上面这种并非最理想的分布是很少出现的。大多数被 EndpointSlice 控制器 处理的变更都是足够小的，可以添加到某已有 EndpointSlice 中去的。并且，假使无法 添加到已有的切片中，不管怎样都会快就会需要一个新的 EndpointSlice 对象。 Deployment 的滚动更新为重新为 EndpointSlice 打包提供了一个自然的机会，所有 Pod 及其对应的端点在这一期间都会被替换掉。

### 重复的端点 

由于 EndpointSlice 变化的自身特点，端点可能会同时出现在不止一个 EndpointSlice 中。鉴于不同的 EndpointSlice 对象在不同时刻到达 Kubernetes 的监视/缓存中， 这种情况的出现是很自然的。 使用 EndpointSlice 的实现必须能够处理端点出现在多个切片中的状况。 关于如何执行端点去重（deduplication）的参考实现，你可以在 ​`kube-proxy`​ 的 ​`EndpointSlice` ​实现中找到。

##  10.  Kubernetes 网络策略
网络策略
----

如果你希望在 IP 地址或端口层面（OSI 第 3 层或第 4 层）控制网络流量， 则你可以考虑为集群中特定应用使用 Kubernetes 网络策略（NetworkPolicy）。 NetworkPolicy 是一种以应用为中心的结构，允许你设置如何允许 Pod 与网络上的各类网络“实体” （我们这里使用实体以避免过度使用诸如“端点”和“服务”这类常用术语， 这些术语在 Kubernetes 中有特定含义）通信。

Pod 可以通信的 Pod 是通过如下三个标识符的组合来辩识的：

1.  其他被允许的 Pods（例外：Pod 无法阻塞对自身的访问）
2.  被允许的名字空间
3.  IP 组块（例外：与 Pod 运行所在的节点的通信总是被允许的， 无论 Pod 或节点的 IP 地址）

在定义基于 Pod 或名字空间的 NetworkPolicy 时，你会使用 选择算符 来设定哪些流量 可以进入或离开与该算符匹配的 Pod。

同时，当基于 IP 的 NetworkPolicy 被创建时，我们基于 IP 组块（CIDR 范围） 来定义策略。

前置条件 
-----

网络策略通过网络插件 来实现。要使用网络策略，你必须使用支持 NetworkPolicy 的网络解决方案。 创建一个 NetworkPolicy 资源对象而没有控制器来使它生效的话，是没有任何作用的。

Pod 隔离的两种类型
-----------

Pod 有两种隔离: 出口的隔离和入口的隔离。它们涉及到可以建立哪些连接。 这里的“隔离”不是绝对的，而是意味着“有一些限制”。 另外的，“非隔离方向”意味着在所述方向上没有限制。这两种隔离（或不隔离）是独立声明的， 并且都与从一个 Pod 到另一个 Pod 的连接有关。

默认情况下，一个 Pod 的出口是非隔离的，即所有外向连接都是被允许的。如果有任何的 NetworkPolicy 选择该 Pod 并在其 ​`policyTypes` ​中包含 “Egress”，则该 Pod 是出口隔离的， 我们称这样的策略适用于该 Pod 的出口。当一个 Pod 的出口被隔离时， 唯一允许的来自 Pod 的连接是适用于出口的 Pod 的某个 NetworkPolicy 的 ​`egress` ​列表所允许的连接。 这些 ​`egress` ​列表的效果是相加的。

默认情况下，一个 Pod 对入口是非隔离的，即所有入站连接都是被允许的。如果有任何的 NetworkPolicy 选择该 Pod 并在其 ​`policyTypes` ​中包含 “Ingress”，则该 Pod 被隔离入口， 我们称这种策略适用于该 Pod 的入口。 当一个 Pod 的入口被隔离时，唯一允许进入该 Pod 的连接是来自该 Pod 节点的连接和适用于入口的 Pod 的某个 NetworkPolicy 的 ​`ingress` ​列表所允许的连接。这些 ​`ingress` ​列表的效果是相加的。

网络策略是相加的，所以不会产生冲突。如果策略适用于 Pod 某一特定方向的流量， Pod 在对应方向所允许的连接是适用的网络策略所允许的集合。 因此，评估的顺序不影响策略的结果。

要允许从源 Pod 到目的 Pod 的连接，源 Pod 的出口策略和目的 Pod 的入口策略都需要允许连接。 如果任何一方不允许连接，建立连接将会失败。

NetworkPolicy 资源
----------------

下面是一个 NetworkPolicy 的示例:

`apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata:   name: test-network-policy   namespace: default spec:   podSelector:     matchLabels:       role: db   policyTypes:     - Ingress     - Egress   ingress:     - from:         - ipBlock:             cidr: 172.17.0.0/16             except:               - 172.17.1.0/24         - namespaceSelector:             matchLabels:               project: myproject         - podSelector:             matchLabels:               role: frontend       ports:         - protocol: TCP           port: 6379   egress:     - to:         - ipBlock:             cidr: 10.0.0.0/24       ports:         - protocol: TCP           port: 5978`

> 除非选择支持网络策略的网络解决方案，否则将上述示例发送到API服务器没有任何效果。

必需字段：与所有其他的 Kubernetes 配置一样，NetworkPolicy 需要 ​`apiVersion`​、 ​`kind` ​和 ​`metadata` ​字段。

spec：NetworkPolicy 规约 中包含了在一个名字空间中定义特定网络策略所需的所有信息。

podSelector：每个 NetworkPolicy 都包括一个 ​`podSelector`​，它对该策略所 适用的一组 Pod 进行选择。示例中的策略选择带有 "role=db" 标签的 Pod。 空的 ​`podSelector` ​选择名字空间下的所有 Pod。

policyTypes: 每个 NetworkPolicy 都包含一个 ​`policyTypes` ​列表，其中包含 ​`Ingress` ​或 ​`Egress` ​或两者兼具。​`policyTypes` ​字段表示给定的策略是应用于 进入所选 Pod 的入站流量还是来自所选 Pod 的出站流量，或两者兼有。 如果 NetworkPolicy 未指定 ​`policyTypes` ​则默认情况下始终设置 ​`Ingress`​； 如果 NetworkPolicy 有任何出口规则的话则设置 ​`Egress`​。

ingress: 每个 NetworkPolicy 可包含一个 ​`ingress` ​规则的白名单列表。 每个规则都允许同时匹配 ​`from` ​和 ​`ports` ​部分的流量。示例策略中包含一条 简单的规则： 它匹配某个特定端口，来自三个来源中的一个，第一个通过 ​`ipBlock` ​指定，第二个通过 ​`namespaceSelector` ​指定，第三个通过 ​`podSelector` ​指定。

egress: 每个 NetworkPolicy 可包含一个 ​`egress` ​规则的白名单列表。 每个规则都允许匹配 ​`to` ​和 ​`port` ​部分的流量。该示例策略包含一条规则， 该规则将指定端口上的流量匹配到 ​`10.0.0.0/24`​ 中的任何目的地。

所以，该网络策略示例:

1.  隔离 "default" 名字空间下 "role=db" 的 Pod （如果它们不是已经被隔离的话）。
2.  （Ingress 规则）允许以下 Pod 连接到 "default" 名字空间下的带有 "role=db" 标签的所有 Pod 的 6379 TCP 端口：

*   "default" 名字空间下带有 "role=frontend" 标签的所有 Pod
*   带有 "project=myproject" 标签的所有名字空间中的 Pod
*   IP 地址范围为 172.17.0.0–172.17.0.255 和 172.17.2.0–172.17.255.255 （即，除了 172.17.1.0/24 之外的所有 172.17.0.0/16）

4.  （Egress 规则）允许从带有 "role=db" 标签的名字空间下的任何 Pod 到 CIDR 10.0.0.0/24 下 5978 TCP 端口的连接。

选择器 to 和 from 的行为 
------------------

可以在 ​`ingress` ​的 ​`from` ​部分或 ​`egress` ​的 ​`to` ​部分中指定四种选择器：

podSelector: 此选择器将在与 NetworkPolicy 相同的名字空间中选择特定的 Pod，应将其允许作为入站流量来源或出站流量目的地。

namespaceSelector：此选择器将选择特定的名字空间，应将所有 Pod 用作其 入站流量来源或出站流量目的地。

namespaceSelector 和 podSelector： 一个指定 ​`namespaceSelector` ​和 ​`podSelector` ​的 ​`to`​/​`from` ​条目选择特定名字空间中的特定 Pod。 注意使用正确的 YAML 语法；下面的策略：

  `...   ingress:   - from:     - namespaceSelector:         matchLabels:           user: alice       podSelector:         matchLabels:           role: client   ...`

在 ​`from` ​数组中仅包含一个元素，只允许来自标有 ​`role=client`​ 的 Pod 且 该 Pod 所在的名字空间中标有 ​`user=alice`​ 的连接。但是 这项 策略：

  `...   ingress:   - from:     - namespaceSelector:         matchLabels:           user: alice     - podSelector:         matchLabels:           role: client   ...`

在 ​`from` ​数组中包含两个元素，允许来自本地名字空间中标有 ​`role=client`​ 的 Pod 的连接，或 来自任何名字空间中标有 ​`user=alice`​ 的任何 Pod 的连接。

如有疑问，请使用 ​`kubectl describe`​ 查看 Kubernetes 如何解释该策略。

ipBlock: 此选择器将选择特定的 IP CIDR 范围以用作入站流量来源或出站流量目的地。 这些应该是集群外部 IP，因为 Pod IP 存在时间短暂的且随机产生。

集群的入站和出站机制通常需要重写数据包的源 IP 或目标 IP。 在发生这种情况时，不确定在 NetworkPolicy 处理之前还是之后发生， 并且对于网络插件、云提供商、​`Service` ​实现等的不同组合，其行为可能会有所不同。

对入站流量而言，这意味着在某些情况下，你可以根据实际的原始源 ​`IP` ​过滤传入的数据包， 而在其他情况下，NetworkPolicy 所作用的 源IP 则可能是 ​`LoadBalancer` ​或 Pod 的节点等。

对于出站流量而言，这意味着从 Pod 到被重写为集群外部 IP 的 ​`Service` ​IP 的连接可能会或可能不会受到基于 ​`ipBlock` ​的策略的约束

默认策略 
-----

默认情况下，如果名字空间中不存在任何策略，则所有进出该名字空间中 Pod 的流量都被允许。 以下示例使你可以更改该名字空间中的默认行为。

### 默认拒绝所有入站流量

你可以通过创建选择所有容器但不允许任何进入这些容器的入站流量的 NetworkPolicy 来为名字空间创建 “default” 隔离策略。

`apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata:   name: default-deny-ingress spec:   podSelector: {}   policyTypes:   - Ingress`

这样可以确保即使容器没有选择其他任何 NetworkPolicy，也仍然可以被隔离。 此策略不会更改默认的出口隔离行为。

### 默认允许所有入站流量

如果要允许所有流量进入某个名字空间中的所有 Pod（即使添加了导致某些 Pod 被视为 “隔离”的策略），则可以创建一个策略来明确允许该名字空间中的所有流量。

`apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata:   name: allow-all-ingress spec:   podSelector: {}   ingress:   - {}   policyTypes:   - Ingress`

### 默认拒绝所有出站流量

你可以通过创建选择所有容器但不允许来自这些容器的任何出站流量的 NetworkPolicy 来为名字空间创建 “default” 隔离策略。

`apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata:   name: default-deny-egress spec:   podSelector: {}   policyTypes:   - Egress`

此策略可以确保即使没有被其他任何 NetworkPolicy 选择的 Pod 也不会被允许流出流量。 此策略不会更改默认的入站流量隔离行为。

### 默认允许所有出站流量 

如果要允许来自名字空间中所有 Pod 的所有流量（即使添加了导致某些 Pod 被视为“隔离”的策略）， 则可以创建一个策略，该策略明确允许该名字空间中的所有出站流量。

`apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata:   name: allow-all-egress spec:   podSelector: {}   egress:   - {}   policyTypes:   - Egress`

### 默认拒绝所有入口和所有出站流量

你可以为名字空间创建“默认”策略，以通过在该名字空间中创建以下 NetworkPolicy 来阻止所有入站和出站流量。

`apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata:   name: default-deny-all spec:   podSelector: {}   policyTypes:   - Ingress   - Egress`

此策略可以确保即使没有被其他任何 NetworkPolicy 选择的 Pod 也不会被 允许入站或出站流量。

SCTP 支持
-------

FEATURE STATE: Kubernetes v1.20 \[stable\]

作为一个稳定特性，SCTP 支持默认是被启用的。 要在集群层面禁用 SCTP，你（或你的集群管理员）需要为 API 服务器指定 ​`--feature-gates=SCTPSupport=false,...`​ 来禁用 ​`SCTPSupport` ​特性门控。 启用该特性门控后，用户可以将 NetworkPolicy 的 ​`protocol` ​字段设置为 ​`SCTP`​。

> 你必须使用支持 SCTP 协议网络策略的 CNI 插件。

针对某个端口范围 
---------

FEATURE STATE: Kubernetes v1.22 \[beta\]

在编写 NetworkPolicy 时，你可以针对一个端口范围而不是某个固定端口。

这一目的可以通过使用 ​`endPort` ​字段来实现，如下例所示：

`apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata:   name: multi-port-egress   namespace: default spec:   podSelector:     matchLabels:       role: db   policyTypes:   - Egress   egress:   - to:     - ipBlock:         cidr: 10.0.0.0/24     ports:     - protocol: TCP       port: 32000       endPort: 32768`

上面的规则允许名字空间 ​`default` ​中所有带有标签 ​`role=db` ​的 Pod 使用 TCP 协议 与 ​`10.0.0.0/24`​ 范围内的 IP 通信，只要目标端口介于 32000 和 32768 之间就可以。

使用此字段时存在以下限制：

*   作为一种 Beta 阶段的特性，端口范围设定默认是被启用的。要在整个集群 范围内禁止使用 ​`endPort` ​字段，你（或者你的集群管理员）需要为 API 服务器设置 ​`-feature-gates=NetworkPolicyEndPort=false,...` ​以禁用 ​`NetworkPolicyEndPort` ​特性门控。
*   ​`endPort` ​字段必须等于或者大于 ​`port` ​字段的值。
*   两个字段的设置值都只能是数字。

> 你的集群所使用的 CNI 插件 必须支持在 NetworkPolicy 规约中使用 ​`endPort` ​字段。 如果你的网络插件 不支持 ​`endPort` ​字段，而你指定了一个包含 ​`endPort` ​字段的 NetworkPolicy， 策略只对单个 ​`port` ​字段生效。

基于名字指向某名字空间 
------------

FEATURE STATE: Kubernetes 1.22 \[stable\]

只要 ​`NamespaceDefaultLabelName` ​特性门控 被启用，Kubernetes 控制面会在所有名字空间上设置一个不可变更的标签 ​`kubernetes.io/metadata.name`​。该标签的值是名字空间的名称。

如果 NetworkPolicy 无法在某些对象字段中指向某名字空间，你可以使用标准的 标签方式来指向特定名字空间。

通过网络策略（至少目前还）无法完成的工作
--------------------

到 Kubernetes 1.24 为止，NetworkPolicy API 还不支持以下功能，不过 你可能可以使用操作系统组件（如 SELinux、OpenVSwitch、IPTables 等等） 或者第七层技术（Ingress 控制器、服务网格实现）或准入控制器来实现一些 替代方案。 如果你对 Kubernetes 中的网络安全性还不太了解，了解使用 NetworkPolicy API 还无法实现下面的用户场景是很值得的。

*   强制集群内部流量经过某公用网关（这种场景最好通过服务网格或其他代理来实现）；
*   与 TLS 相关的场景（考虑使用服务网格或者 Ingress 控制器）；
*   特定于节点的策略（你可以使用 CIDR 来表达这一需求不过你无法使用节点在 Kubernetes 中的其他标识信息来辩识目标节点）；
*   基于名字来选择服务（不过，你可以使用 标签 来选择目标 Pod 或名字空间，这也通常是一种可靠的替代方案）；
*   创建或管理由第三方来实际完成的“策略请求”；
*   实现适用于所有名字空间或 Pods 的默认策略（某些第三方 Kubernetes 发行版本 或项目可以做到这点）；
*   高级的策略查询或者可达性相关工具；
*   生成网络安全事件日志的能力（例如，被阻塞或接收的连接请求）；
*   显式地拒绝策略的能力（目前，NetworkPolicy 的模型默认采用拒绝操作， 其唯一的能力是添加允许策略）；
*   禁止本地回路或指向宿主的网络流量（Pod 目前无法阻塞 localhost 访问， 它们也无法禁止来自所在节点的访问请求）。

##  11.  Kubernetes IPv4/IPv6 双协议栈
IPv4/IPv6 双协议栈
--------------

FEATURE STATE: Kubernetes v1.23 \[stable\]

IPv4/IPv6 双协议栈网络能够将 IPv4 和 IPv6 地址分配给 Pod 和 Service。

从 1.21 版本开始，Kubernetes 集群默认启用 IPv4/IPv6 双协议栈网络， 以支持同时分配 IPv4 和 IPv6 地址。

支持的功能 
------

Kubernetes 集群的 IPv4/IPv6 双协议栈可提供下面的功能：

*   双协议栈 pod 网络 (每个 pod 分配一个 IPv4 和 IPv6 地址)
*   IPv4 和 IPv6 启用的服务
*   Pod 的集群外出口通过 IPv4 和 IPv6 路由

先决条件 
-----

为了使用 IPv4/IPv6 双栈的 Kubernetes 集群，需要满足以下先决条件：

*   Kubernetes 1.20 版本或更高版本，有关更早 Kubernetes 版本的使用双栈服务的信息， 请参考对应版本的 Kubernetes 文档。
*   提供商支持双协议栈网络（云提供商或其他提供商必须能够为 Kubernetes 节点提供可路由的 IPv4/IPv6 网络接口）
*   支持双协议栈的网络插件（如 Kubenet 或 Calico）

配置 IPv4/IPv6 双协议栈
-----------------

*   kube-apiserver:

*   ​`--service-cluster-ip-range=<IPv4 CIDR>,<IPv6 CIDR>`​

*   kube-controller-manager:

*   ​`--cluster-cidr=<IPv4 CIDR>,<IPv6 CIDR>`​
*   ​`--service-cluster-ip-range=<IPv4 CIDR>,<IPv6 CIDR>`​
*   ​`--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6`​ 对于 IPv4 默认为 /24，对于 IPv6 默认为 /64

*   kube-proxy:

*   ​`--cluster-cidr=<IPv4 CIDR>,<IPv6 CIDR>`​

*   kubelet:

*   当没有 ​`--cloud-provider`​ 时，管理员可以通过 ​`--node-ip`​ 来传递逗号分隔的 IP 地址， 为该节点手动配置双栈 ​`.status.addresses`​。 如果 Pod 以 HostNetwork 模式在该节点上运行，则 Pod 会用 ​`.status.podIPs`​ 字段来报告它的 IP 地址。 一个节点中的所有 ​`podIP` ​都会匹配该节点的由 ​`.status.addresses`​ 字段定义的 IP 组。

> IPv4 CIDR 的一个例子：​`10.244.0.0/16`​（尽管你会提供你自己的地址范围）。  
> IPv6 CIDR 的一个例子：​`fdXY:IJKL:MNOP:15::/64`​ （这里演示的是格式而非有效地址 - 请看 [RFC 4193](https://datatracker.ietf.org/doc/html/rfc4193)）。

服务
--

你可以使用 IPv4 或 IPv6 地址来创建 Service。 服务的地址族默认为第一个服务集群 IP 范围的地址族（通过 kube-apiserver 的 ​`--service-cluster-ip-range`​ 参数配置）。 当你定义服务时，可以选择将其配置为双栈。若要指定所需的行为，你可以设置 ​`.spec.ipFamilyPolicy`​ 字段为以下值之一：

*   ​`SingleStack`​：单栈服务。控制面使用第一个配置的服务集群 IP 范围为服务分配集群 IP。
*   ​`PreferDualStack`​：

*   为服务分配 IPv4 和 IPv6 集群 IP 地址。

*   ​`RequireDualStack`​：从 IPv4 和 IPv6 的地址范围分配服务的 ​`.spec.ClusterIPs` ​

*   从基于在 ​`.spec.ipFamilies`​ 数组中第一个元素的地址族的 ​`.spec.ClusterIPs`​ 列表中选择 ​`.spec.ClusterIP` ​

如果你想要定义哪个 IP 族用于单栈或定义双栈 IP 族的顺序，可以通过设置 服务上的可选字段 .spec.ipFamilies 来选择地址族。

> ​`.spec.ipFamilies`​ 字段是不可变的，因为系统无法为已经存在的服务重新分配 ​`.spec.ClusterIP`​。如果你想改变 ​`.spec.ipFamilies`​，则需要删除并重新创建服务。

你可以设置 ​`.spec.ipFamily`​ 为以下任何数组值：

*   ​`["IPv4"]` ​
*   ​`["IPv6"]` ​
*   ​`["IPv4","IPv6"]` ​（双栈）
*   ​`["IPv6","IPv4"]` ​（双栈）

你所列出的第一个地址族用于原来的 ​`.spec.ClusterIP`​ 字段。

### 双栈服务配置场景

以下示例演示多种双栈服务配置场景下的行为。

#### 新服务的双栈选项

1.  此服务规约中没有显式设定 ​`.spec.ipFamilyPolicy`​。当你创建此服务时，Kubernetes 从所配置的第一个 ​`service-cluster-ip-range`​ 种为服务分配一个集群IP，并设置 ​`.spec.ipFamilyPolicy`​ 为 ​`SingleStack`​。 （无选择算符的服务 和无头服务的行为方式 与此相同。）

`apiVersion: v1 kind: Service metadata:   name: my-service   labels:     app: MyApp spec:   selector:     app: MyApp   ports:     - protocol: TCP       port: 80`

        

3.  此服务规约显式地将 ​`.spec.ipFamilyPolicy`​ 设置为 ​`PreferDualStack`​。 当你在双栈集群上创建此服务时，Kubernetes 会为该服务分配 IPv4 和 IPv6 地址。 控制平面更新服务的 ​`.spec`​ 以记录 IP 地址分配。 字段 ​`.spec.ClusterIPs`​ 是主要字段，包含两个分配的 IP 地址；​`.spec.ClusterIP`​ 是次要字段， 其取值从 ​`.spec.ClusterIPs`​ 计算而来。

*   对于 .spec.ClusterIP 字段，控制面记录来自第一个服务集群 IP 范围 对应的地址族的 IP 地址。
*   对于单协议栈的集群，.spec.ClusterIPs 和 .spec.ClusterIP 字段都 仅仅列出一个地址。
*   对于启用了双协议栈的集群，将 .spec.ipFamilyPolicy 设置为 RequireDualStack 时，其行为与 PreferDualStack 相同。

`apiVersion: v1 kind: Service metadata:   name: my-service   labels:     app: MyApp spec:   ipFamilyPolicy: PreferDualStack   selector:     app: MyApp   ports:     - protocol: TCP       port: 80`

6.  下面的服务规约显式地在 ​`.spec.ipFamilies`​ 中指定 ​`IPv6` ​和 ​`IPv4`​，并 将 ​`.spec.ipFamilyPolicy`​ 设定为 ​`PreferDualStack`​。 当 Kubernetes 为 ​`.spec.ClusterIPs`​ 分配一个 IPv6 和一个 IPv4 地址时， ​`.spec.ClusterIP`​ 被设置成 IPv6 地址，因为它是 ​`.spec.ClusterIPs`​ 数组中的第一个元素， 覆盖其默认值。

`apiVersion: v1 kind: Service metadata:   name: my-service   labels:     app: MyApp spec:   ipFamilyPolicy: PreferDualStack   ipFamilies:   - IPv6   - IPv4   selector:     app: MyApp   ports:     - protocol: TCP       port: 80`

                

#### 现有服务的双栈默认值

下面示例演示了在服务已经存在的集群上新启用双栈时的默认行为。 （将现有集群升级到 1.21 或者更高版本会启用双协议栈支持。）

1.  在集群上启用双栈时，控制面会将现有服务（无论是 ​`IPv4` ​还是 ​`IPv6`​）配置 ​`.spec.ipFamilyPolicy`​ 为 ​`SingleStack` ​并设置​ `.spec.ipFamilies`​ 为服务的当前地址族。

`apiVersion: v1 kind: Service metadata:   name: my-service   labels:     app: MyApp spec:   selector:     app: MyApp   ports:     - protocol: TCP       port: 80`

你可以通过使用 kubectl 检查现有服务来验证此行为。

`kubectl get svc my-service -o yaml`

`apiVersion: v1 kind: Service metadata:   labels:     app: MyApp   name: my-service spec:   clusterIP: 10.0.197.123   clusterIPs:   - 10.0.197.123   ipFamilies:   - IPv4   ipFamilyPolicy: SingleStack   ports:   - port: 80     protocol: TCP     targetPort: 80   selector:     app: MyApp   type: ClusterIP status:   loadBalancer: {}`

6.  在集群上启用双栈时，带有选择算符的现有 无头服务 由控制面设置 ​`.spec.ipFamilyPolicy`​ 为 ​`SingleStack` ​并设置 ​`.spec.ipFamilies`​ 为第一个服务集群 IP 范围的地址族（通过配置 kube-apiserver 的 ​`--service-cluster-ip-range`​ 参数），即使 ​`.spec.ClusterIP`​ 的设置值为 ​`None` ​也如此。

`apiVersion: v1 kind: Service metadata:   name: my-service   labels:     app: MyApp spec:   selector:     app: MyApp   ports:     - protocol: TCP       port: 80`

        

你可以通过使用 kubectl 检查带有选择算符的现有无头服务来验证此行为。

`kubectl get svc my-service -o yaml`

`apiVersion: v1 kind: Service metadata:   labels:     app: MyApp   name: my-service spec:   clusterIP: None   clusterIPs:   - None   ipFamilies:   - IPv4   ipFamilyPolicy: SingleStack   ports:   - port: 80     protocol: TCP     targetPort: 80   selector:     app: MyApp`  

#### 在单栈和双栈之间切换服务

服务可以从单栈更改为双栈，也可以从双栈更改为单栈。

1.  要将服务从单栈更改为双栈，根据需要将 ​`.spec.ipFamilyPolicy`​ 从 ​`SingleStack` ​改为 ​`PreferDualStack` ​或 ​`RequireDualStack`​。 当你将此服务从单栈更改为双栈时，Kubernetes 将分配缺失的地址族，以便现在 该服务具有 IPv4 和 IPv6 地址。 编辑服务规约将 ​`.spec.ipFamilyPolicy`​ 从 ​`SingleStack` ​改为 ​`PreferDualStack`​。

之前：

`spec:   ipFamilyPolicy: SingleStack`

之后：

`spec:   ipFamilyPolicy: PreferDualStack`

6.  要将服务从双栈更改为单栈，请将 ​`.spec.ipFamilyPolicy`​ 从 ​`PreferDualStack` ​或 ​`RequireDualStack` ​改为 ​`SingleStack`​。 当你将此服务从双栈更改为单栈时，Kubernetes 只保留 ​`.spec.ClusterIPs`​ 数组中的第一个元素，并设置 ​`.spec.ClusterIP`​ 为那个 IP 地址， 并设置 ​`.spec.ipFamilies`​ 为 ​`.spec.ClusterIPs`​ 地址族。

### 无选择算符的无头服务

对于不带选择算符的无头服务， 若没有显式设置 ​`.spec.ipFamilyPolicy`​，则 ​`.spec.ipFamilyPolicy`​ 字段默认设置为 ​`RequireDualStack`​。

### LoadBalancer 类型服务

要为你的服务提供双栈负载均衡器：

*   将 ​`.spec.type`​ 字段设置为 ​`LoadBalancer` ​
*   将 ​`.spec.ipFamilyPolicy`​ 字段设置为 ​`PreferDualStack` ​或者 ​`RequireDualStack`​

> 为了使用双栈的负载均衡器类型服务，你的云驱动必须支持 IPv4 和 IPv6 的负载均衡器。

出站流量
----

如果你要启用出站流量，以便使用非公开路由 IPv6 地址的 Pod 到达集群外地址 （例如公网），则需要通过透明代理或 IP 伪装等机制使 Pod 使用公共路由的 IPv6 地址。 [ip-masq-agent](https://github.com/kubernetes-sigs/ip-masq-agent)项目 支持在双栈集群上进行 IP 伪装。

> 确认你的 CNI 驱动支持 IPv6。

#  10.  Kubernetes 存储

##  1.  Kubernetes 卷
卷
-

Container 中的文件在磁盘上是临时存放的，这给 Container 中运行的较重要的应用程序带来一些问题。 问题之一是当容器崩溃时文件丢失。 kubelet 会重新启动容器，但容器会以干净的状态重启。 第二个问题会在同一 ​`Pod` ​中运行多个容器并共享文件时出现。 Kubernetes 卷（Volume） 这一抽象概念能够解决这两个问题。

背景 
---

Docker 也有 [卷（Volume）](https://docs.docker.com/storage/) 的概念，但对它只有少量且松散的管理。 Docker 卷是磁盘上或者另外一个容器内的一个目录。 Docker 提供卷驱动程序，但是其功能非常有限。

Kubernetes 支持很多类型的卷。 Pod 可以同时使用任意数目的卷类型。 临时卷类型的生命周期与 Pod 相同，但持久卷可以比 Pod 的存活期长。 当 Pod 不再存在时，Kubernetes 也会销毁临时卷；不过 Kubernetes 不会销毁持久卷。 对于给定 Pod 中任何类型的卷，在容器重启期间数据都不会丢失。

卷的核心是一个目录，其中可能存有数据，Pod 中的容器可以访问该目录中的数据。 所采用的特定的卷类型将决定该目录如何形成的、使用何种介质保存数据以及目录中存放的内容。

使用卷时, 在 ​`.spec.volumes`​ 字段中设置为 Pod 提供的卷，并在 ​`.spec.containers[*].volumeMounts`​ 字段中声明卷在容器中的挂载位置。 容器中的进程看到的文件系统视图是由它们的 容器镜像 的初始内容以及挂载在容器中的卷（如果定义了的话）所组成的。 其中根文件系统同容器镜像的内容相吻合。 任何在该文件系统下的写入操作，如果被允许的话，都会影响接下来容器中进程访问文件系统时所看到的内容。

卷挂载在镜像中的指定路径下。 Pod 配置中的每个容器必须独立指定各个卷的挂载位置。

卷不能挂载到其他卷之上（不过存在一种使用 subPath 的相关机制），也不能与其他卷有硬链接。

卷类型 
----

Kubernetes 支持下列类型的卷：

### awsElasticBlockStore

​`awsElasticBlockStore` ​卷将 Amazon Web服务（AWS）[EBS 卷](https://aws.amazon.com/cn/ebs/) 挂载到你的 Pod 中。与 ​`emptyDir` ​在 Pod 被删除时也被删除不同，EBS 卷的内容在删除 Pod 时会被保留，卷只是被卸载掉了。 这意味着 EBS 卷可以预先填充数据，并且该数据可以在 Pod 之间共享。

你在使用 EBS 卷之前必须使用 ​`aws ec2 create-volume`​ 命令或者 AWS API 创建该卷。

使用 ​`awsElasticBlockStore` ​卷时有一些限制：

*   Pod 运行所在的节点必须是 AWS EC2 实例。
*   这些实例需要与 EBS 卷在相同的地域（Region）和可用区（Availability-Zone）。
*   EBS 卷只支持被挂载到单个 EC2 实例上。

#### 创建 EBS 卷

在将 EBS 卷用到 Pod 上之前，你首先要创建它。

`aws ec2 create-volume --availability-zone=eu-west-1a --size=10 --volume-type=gp2`

确保该区域与你的群集所在的区域相匹配。还要检查卷的大小和 EBS 卷类型都适合你的用途。

#### AWS EBS 配置示例

`apiVersion: v1 kind: Pod metadata:   name: test-ebs spec:   containers:   - image: k8s.gcr.io/test-webserver     name: test-container     volumeMounts:     - mountPath: /test-ebs       name: test-volume   volumes:   - name: test-volume     # 此 AWS EBS 卷必须已经存在     awsElasticBlockStore:       volumeID: "<volume-id>"       fsType: ext4`

如果 EBS 卷是分区的，你可以提供可选的字段 ​`partition: "<partition number>"`​ 来指定要挂载到哪个分区上。

#### AWS EBS CSI 卷迁移

FEATURE STATE: Kubernetes v1.17 \[beta\]

如果启用了对 ​`awsElasticBlockStore` ​的 ​`CSIMigration` ​特性支持，所有插件操作都不再指向树内插件（In-Tree Plugin），转而指向 ​`ebs.csi.aws.com`​ 容器存储接口（Container Storage Interface，CSI）驱动。 为了使用此特性，必须在集群中安装  [AWS EBS CSI 驱动](https://github.com/kubernetes-sigs/aws-ebs-csi-driver)， 并确保 ​`CSIMigration` ​和 ​`CSIMigrationAWS` ​Beta 功能特性被启用。

#### AWS EBS CSI 迁移结束

FEATURE STATE: Kubernetes v1.17 \[alpha\]

要禁止控制器管理器和 kubelet 加载 ​`awsElasticBlockStore` ​存储插件， 请将 ​`InTreePluginAWSUnregister` ​标志设置为 ​`true`​。

### azureDisk

​`azureDisk` ​卷类型用来在 Pod 上挂载 Microsoft Azure 数据盘（Data Disk） 。 若需了解更多详情，请参考 [azureDisk 卷插件](https://github.com/kubernetes/examples/blob/master/staging/volumes/azure_disk/README.md)。

#### azureDisk 的 CSI 迁移 

FEATURE STATE: Kubernetes v1.19 \[beta\]

启用 ​​`azureDisk` ​​的 ​​`CSIMigration` ​​功能后，所有插件操作从现有的树内插件重定向到 ​`disk.csi.azure.com`​ 容器存储接口（CSI）驱动程序。 为了使用此功能，必须在集群中安装 [​Azure 磁盘 CSI 驱动​程序](https://github.com/kubernetes-sigs/azuredisk-csi-driver)， 并且 ​`CSIMigration` ​和 ​`CSIMigrationAzureDisk` ​功能必须被启用。

#### azureDisk CSI 迁移完成

FEATURE STATE: Kubernetes v1.21 \[alpha\]

要禁止控制器管理器和 kubelet 加载 ​`azureDisk` ​存储插件， 请将 ​`InTreePluginAzureDiskUnregister` ​标志设置为 ​`true`​。

### azureFile

​`azureFile` ​卷类型用来在 Pod 上挂载 Microsoft Azure 文件卷（File Volume）（SMB 2.1 和 3.0）。 更多详情请参考 [azureFile 卷插件](https://github.com/kubernetes/examples/blob/master/staging/volumes/azure_file/README.md)。

#### azureFile CSI 迁移 

FEATURE STATE: Kubernetes v1.21 \[beta\]

启用 ​`azureFile` ​的 ​`CSIMigration` ​功能后，所有插件操作将从现有的树内插件重定向到 ​`file.csi.azure.com`​ 容器存储接口（CSI）驱动程序。要使用此功能，必须在集群中安装 Azure 文件 CSI 驱动程序， 并且 ​`CSIMigration` ​和 ​`CSIMigrationAzureFile` ​特性门控 必须被启用。

Azure 文件 CSI 驱动尚不支持为同一卷设置不同的 fsgroup。 如果 AzureFile CSI 迁移被启用，用不同的 fsgroup 来使用同一卷也是不被支持的。

#### azureDisk CSI 迁移完成

FEATURE STATE: Kubernetes v1.21 \[alpha\]

要禁止控制器管理器和 kubelet 加载 ​`azureDisk` ​存储插件， 请将 ​`InTreePluginAzureDiskUnregister` ​标志设置为 ​`true`​。

### cephfs

​`cephfs` ​卷允许你将现存的 CephFS 卷挂载到 Pod 中。 不像 ​`emptyDir` ​那样会在 Pod 被删除的同时也会被删除，​`cephfs` ​卷的内容在 Pod 被删除时会被保留，只是卷被卸载了。 这意味着 ​`cephfs` ​卷可以被预先填充数据，且这些数据可以在 Pod 之间共享。同一 ​`cephfs` ​卷可同时被多个写者挂载。

> 在使用 Ceph 卷之前，你的 Ceph 服务器必须已经运行并将要使用的 share 导出（exported）。

更多信息请参考 [CephFS 示例](https://github.com/kubernetes/examples/tree/master/volumes/cephfs/)。

### cinder

> Kubernetes 必须配置了 OpenStack Cloud Provider。

​`cinder` ​卷类型用于将 OpenStack Cinder 卷挂载到 Pod 中。

#### Cinder 卷示例配置

`apiVersion: v1 kind: Pod metadata:   name: test-cinder spec:   containers:   - image: k8s.gcr.io/test-webserver     name: test-cinder-container     volumeMounts:     - mountPath: /test-cinder       name: test-volume   volumes:   - name: test-volume     # 此 OpenStack 卷必须已经存在     cinder:       volumeID: "<volume-id>"       fsType: ext4`

#### OpenStack CSI 迁移

FEATURE STATE: Kubernetes v1.21 \[beta\]

Cinder 的 ​`CSIMigration` ​功能在 Kubernetes 1.21 版本中是默认被启用的。 此特性会将插件的所有操作从现有的树内插件重定向到 ​`cinder.csi.openstack.org`​ 容器存储接口（CSI）驱动程序。 为了使用此功能，必须在集群中安装 [OpenStack Cinder CSI 驱动程序](https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/cinder-csi-plugin/using-cinder-csi-plugin.md)， 你可以通过设置 ​`CSIMigrationOpenStack` ​特性门控 为 ​`false` ​来禁止 Cinder CSI 迁移。 如果你禁用了 ​`CSIMigrationOpenStack` ​功能特性，则树内的 Cinder 卷插件 会负责 Cinder 卷存储管理的方方面面。

### configMap

​`configMap` ​卷提供了向 Pod 注入配置数据的方法。 ConfigMap 对象中存储的数据可以被 ​`configMap` ​类型的卷引用，然后被 Pod 中运行的容器化应用使用。

引用 configMap 对象时，你可以在 volume 中通过它的名称来引用。 你可以自定义 ConfigMap 中特定条目所要使用的路径。 下面的配置显示了如何将名为 ​`log-config`​ 的 ConfigMap 挂载到名为 ​`configmap-pod`​ 的 Pod 中：

`apiVersion: v1 kind: Pod metadata:   name: configmap-pod spec:   containers:     - name: test       image: busybox:1.28       volumeMounts:         - name: config-vol           mountPath: /etc/config   volumes:     - name: config-vol       configMap:         name: log-config         items:           - key: log_level             path: log_level`

    

​`log-config`​ ConfigMap 以卷的形式挂载，并且存储在 ​`log_level` ​条目中的所有内容都被挂载到 Pod 的 ​`/etc/config/log_level`​ 路径下。 请注意，这个路径来源于卷的 ​`mountPath` ​和 ​`log_level` ​键对应的 ​`path`​。

> *   在使用 ConfigMap 之前你首先要创建它。
> *   容器以 subPath 卷挂载方式使用 ConfigMap 时，将无法接收 ConfigMap 的更新。
> *   文本数据挂载成文件时采用 UTF-8 字符编码。如果使用其他字符编码形式，可使用 ​`binaryData` ​字段。

### downwardAPI

​`downwardAPI` ​卷用于使 downward API 数据对应用程序可用。 这种卷类型挂载一个目录并在纯文本文件中写入所请求的数据。

> 容器以 subPath 卷挂载方式使用 downwardAPI 时，将不能接收到它的更新。

### emptyDir

当 Pod 分派到某个 Node 上时，​`emptyDir` ​卷会被创建，并且在 Pod 在该节点上运行期间，卷一直存在。 就像其名称表示的那样，卷最初是空的。 尽管 Pod 中的容器挂载 ​`emptyDir` ​卷的路径可能相同也可能不同，这些容器都可以读写 ​`emptyDir` ​卷中相同的文件。 当 Pod 因为某些原因被从节点上删除时，​`emptyDir` ​卷中的数据也会被永久删除。

> 容器崩溃并不会导致 Pod 被从节点上移除，因此容器崩溃期间 ​`emptyDir` ​卷中的数据是安全的。

​`emptyDir` ​的一些用途：

*   缓存空间，例如基于磁盘的归并排序。
*   为耗时较长的计算任务提供检查点，以便任务能方便地从崩溃前状态恢复执行。
*   在 Web 服务器容器服务数据时，保存内容管理器容器获取的文件。

取决于你的环境，​`emptyDir` ​卷存储在该节点所使用的介质上；这里的介质可以是磁盘或 SSD 或网络存储。但是，你可以将 ​`emptyDir.medium`​ 字段设置为 ​`"Memory"`​，以告诉 Kubernetes 为你挂载 tmpfs（基于 RAM 的文件系统）。 虽然 tmpfs 速度非常快，但是要注意它与磁盘不同。 tmpfs 在节点重启时会被清除，并且你所写入的所有文件都会计入容器的内存消耗，受容器内存限制约束。

> 当启用 ​`SizeMemoryBackedVolumes` ​特性门控 时，你可以为基于内存提供的卷指定大小。 如果未指定大小，则基于内存的卷的大小为 Linux 主机上内存的 50％。

#### emptyDir 配置示例 

`apiVersion: v1 kind: Pod metadata:   name: test-pd spec:   containers:   - image: k8s.gcr.io/test-webserver     name: test-container     volumeMounts:     - mountPath: /cache       name: cache-volume   volumes:   - name: cache-volume     emptyDir: {}`

### fc (光纤通道)

​`fc` ​卷类型允许将现有的光纤通道块存储卷挂载到 Pod 中。 可以使用卷配置中的参数 ​`targetWWNs` ​来指定单个或多个目标 WWN（World Wide Names）。 如果指定了多个 WWN，targetWWNs 期望这些 WWN 来自多路径连接。

> 你必须配置 FC SAN Zoning，以便预先向目标 WWN 分配和屏蔽这些 LUN（卷），这样 Kubernetes 主机才可以访问它们。

更多详情请参考 [FC 示例](https://github.com/kubernetes/examples/tree/master/staging/volumes/fibre_channel)。

### flocker （已弃用）

[Flocker](https://github.com/ClusterHQ/flocker) 是一个开源的、集群化的容器数据卷管理器。 Flocker 提供了由各种存储后端所支持的数据卷的管理和编排。

使用 ​`flocker` ​卷可以将一个 Flocker 数据集挂载到 Pod 中。 如果数据集在 Flocker 中不存在，则需要首先使用 Flocker CLI 或 Flocker API 创建数据集。 如果数据集已经存在，那么 Flocker 将把它重新附加到 Pod 被调度的节点。 这意味着数据可以根据需要在 Pod 之间共享。

> 在使用 Flocker 之前你必须先安装运行自己的 Flocker。

更多详情请参考 [Flocker 示例](https://github.com/kubernetes/examples/tree/master/staging/volumes/flocker)。

### gcePersistentDisk

​`gcePersistentDisk` ​卷能将谷歌计算引擎 (GCE) [持久盘（PD）](https://cloud.google.com/compute/docs/disks) 挂载到你的 Pod 中。 不像 ​`emptyDir` ​那样会在 Pod 被删除的同时也会被删除，持久盘卷的内容在删除 Pod 时会被保留，卷只是被卸载了。 这意味着持久盘卷可以被预先填充数据，并且这些数据可以在 Pod 之间共享。

> 在使用 PD 前，你必须使用 ​`gcloud` ​或者 GCE API 或 UI 创建它。

使用 ​`gcePersistentDisk` ​时有一些限制：

*   运行 Pod 的节点必须是 GCE VM
*   这些 VM 必须和持久盘位于相同的 GCE 项目和区域（zone）

GCE PD 的一个特点是它们可以同时被多个消费者以只读方式挂载。 这意味着你可以用数据集预先填充 PD，然后根据需要并行地在尽可能多的 Pod 中提供该数据集。 不幸的是，PD 只能由单个使用者以读写模式挂载 —— 即不允许同时写入。

在由 ReplicationController 所管理的 Pod 上使用 GCE PD 将会失败，除非 PD 是只读模式或者副本的数量是 0 或 1。

#### 创建 GCE 持久盘（PD） 

在 Pod 中使用 GCE 持久盘之前，你首先要创建它。

`gcloud compute disks create --size=500GB --zone=us-central1-a my-data-disk`

#### GCE 持久盘配置示例

`apiVersion: v1 kind: Pod metadata:   name: test-pd spec:   containers:   - image: k8s.gcr.io/test-webserver     name: test-container     volumeMounts:     - mountPath: /test-pd       name: test-volume   volumes:   - name: test-volume     # 此 GCE PD 必须已经存在     gcePersistentDisk:       pdName: my-data-disk       fsType: ext4`

#### 区域持久盘 

[区域持久盘](https://cloud.google.com/compute/docs/disks/ target=) 功能允许你创建能在同一区域的两个可用区中使用的持久盘。 要使用这个功能，必须以持久卷（PersistentVolume）的方式提供卷；直接从 Pod 引用这种卷是不可以的。

#### 手动供应基于区域 PD 的 PersistentVolume

使用为 GCE PD 定义的存储类 可以实现动态供应。在创建 PersistentVolume 之前，你首先要创建 PD。

`gcloud beta compute disks create --size=500GB my-data-disk     --region us-central1     --replica-zones us-central1-a,us-central1-b`

PersistentVolume 示例：

`apiVersion: v1 kind: PersistentVolume metadata:   name: test-volume   labels:     failure-domain.beta.kubernetes.io/zone: us-central1-a__us-central1-b spec:   capacity:     storage: 400Gi   accessModes:   - ReadWriteOnce   gcePersistentDisk:     pdName: my-data-disk     fsType: ext4   nodeAffinity:     required:       nodeSelectorTerms:       - matchExpressions:         # failure-domain.beta.kubernetes.io/zone 应在 1.21 之前使用         - key: topology.kubernetes.io/zone           operator: In           values:           - us-central1-a           - us-central1-b`

#### GCE CSI 迁移 

FEATURE STATE: Kubernetes v1.17 \[beta\]

启用 GCE PD 的 ​`CSIMigration` ​功能后，所有插件操作将从现有的树内插件重定向到 ​`pd.csi.storage.gke.io`​ 容器存储接口（ CSI ）驱动程序。 为了使用此功能，必须在集群中上安装 GCE PD CSI驱动程序， 并且 ​`CSIMigration` ​和 ​`CSIMigrationGCE` ​Beta 功能必须被启用。

#### GCE CSI 迁移完成

FEATURE STATE: Kubernetes v1.21 \[alpha\]

要禁止控制器管理器和 kubelet 加载 ​`gcePersistentDisk` ​存储插件，请将 ​`InTreePluginGCEUnregister` ​标志设置为 ​`true`​。

### gitRepo (已弃用) 

> ​`gitRepo` ​卷类型已经被废弃。如果需要在容器中提供 git 仓库，请将一个 EmptyDir 卷挂载到 InitContainer 中，使用 git 命令完成仓库的克隆操作，然后将 EmptyDir 卷挂载到 Pod 的容器中。

​`gitRepo` ​卷是一个卷插件的例子。 该查卷挂载一个空目录，并将一个 Git 代码仓库克隆到这个目录中供 Pod 使用。

下面给出一个 ​`gitRepo` ​卷的示例：

`apiVersion: v1 kind: Pod metadata:   name: server spec:   containers:   - image: nginx     name: nginx     volumeMounts:     - mountPath: /mypath       name: git-volume   volumes:   - name: git-volume     gitRepo:       repository: "git@somewhere:me/my-git-repository.git"       revision: "22f1d8406d464b0c0874075539c1f2e96c253775"`

### glusterfs

​`glusterfs` ​卷能将 [Glusterfs](https://www.gluster.org/) (一个开源的网络文件系统) 挂载到你的 Pod 中。不像 ​`emptyDir` ​那样会在删除 Pod 的同时也会被删除，​`glusterfs` ​卷的内容在删除 Pod 时会被保存，卷只是被卸载。 这意味着 ​`glusterfs` ​卷可以被预先填充数据，并且这些数据可以在 Pod 之间共享。 GlusterFS 可以被多个写者同时挂载。

> 在使用前你必须先安装运行自己的 GlusterFS。

更多详情请参考 [GlusterFS 示例](https://github.com/kubernetes/examples/tree/master/volumes/glusterfs)。

### hostPath

> HostPath 卷存在许多安全风险，最佳做法是尽可能避免使用 HostPath。 当必须使用 HostPath 卷时，它的范围应仅限于所需的文件或目录，并以只读方式挂载。  
> 如果通过 AdmissionPolicy 限制 HostPath 对特定目录的访问，则必须要求 ​`volumeMounts` ​使用 ​`readOnly` ​挂载以使策略生效。

​`hostPath` ​卷能将主机节点文件系统上的文件或目录挂载到你的 Pod 中。 虽然这不是大多数 Pod 需要的，但是它为一些应用程序提供了强大的逃生舱。

例如，​`hostPath` ​的一些用法有：

*   运行一个需要访问 Docker 内部机制的容器；可使用 ​`hostPath` ​挂载 ​`/var/lib/docker`​ 路径。
*   在容器中运行 cAdvisor 时，以 ​`hostPath` ​方式挂载 ​`/sys`​。
*   允许 Pod 指定给定的 ​`hostPath` ​在运行 Pod 之前是否应该存在，是否应该创建以及应该以什么方式存在。

除了必需的 ​`path` ​属性之外，用户可以选择性地为 ​`hostPath` ​卷指定 ​`type`​。

支持的 ​`type` ​值如下：

取值

行为

空字符串（默认）用于向后兼容，这意味着在安装 hostPath 卷之前不会执行任何检查。

`DirectoryOrCreate`

如果在给定路径上什么都不存在，那么将根据需要创建空目录，权限设置为 0755，具有与 kubelet 相同的组和属主信息。

`Directory`

在给定路径上必须存在的目录。

`FileOrCreate`

如果在给定路径上什么都不存在，那么将在那里根据需要创建空文件，权限设置为 0644，具有与 kubelet 相同的组和所有权。

`File`

在给定路径上必须存在的文件。

`Socket`

在给定路径上必须存在的 UNIX 套接字。

`CharDevice`

在给定路径上必须存在的字符设备。

`BlockDevice`

在给定路径上必须存在的块设备。

当使用这种类型的卷时要小心，因为：

*   HostPath 卷可能会暴露特权系统凭据（例如 Kubelet）或特权 API（例如容器运行时套接字），可用于容器逃逸或攻击集群的其他部分。
*   具有相同配置（例如基于同一 PodTemplate 创建）的多个 Pod 会由于节点上文件的不同而在不同节点上有不同的行为。
*   下层主机上创建的文件或目录只能由 root 用户写入。你需要在 特权容器 中以 root 身份运行进程，或者修改主机上的文件权限以便容器能够写入 ​`hostPath` ​卷。

#### hostPath 配置示例： 

`apiVersion: v1 kind: Pod metadata:   name: test-pd spec:   containers:   - image: k8s.gcr.io/test-webserver     name: test-container     volumeMounts:     - mountPath: /test-pd       name: test-volume   volumes:   - name: test-volume     hostPath:       # 宿主上目录位置       path: /data       # 此字段为可选       type: Directory`

> ​`FileOrCreate` ​模式不会负责创建文件的父目录。 如果欲挂载的文件的父目录不存在，Pod 启动会失败。 为了确保这种模式能够工作，可以尝试把文件和它对应的目录分开挂载，如 FileOrCreate 配置 所示。

#### hostPath FileOrCreate 配置示例 

`apiVersion: v1 kind: Pod metadata:   name: test-webserver spec:   containers:   - name: test-webserver     image: k8s.gcr.io/test-webserver:latest     volumeMounts:     - mountPath: /var/local/aaa       name: mydir     - mountPath: /var/local/aaa/1.txt       name: myfile   volumes:   - name: mydir     hostPath:       # 确保文件所在目录成功创建。       path: /var/local/aaa       type: DirectoryOrCreate   - name: myfile     hostPath:       path: /var/local/aaa/1.txt       type: FileOrCreate`

### iscsi

​`iscsi` ​卷能将 iSCSI (基于 IP 的 SCSI) 卷挂载到你的 Pod 中。 不像 ​`emptyDir` ​那样会在删除 Pod 的同时也会被删除，​`iscsi` ​卷的内容在删除 Pod 时会被保留，卷只是被卸载。 这意味着 ​`iscsi` ​卷可以被预先填充数据，并且这些数据可以在 Pod 之间共享。

> 在使用 iSCSI 卷之前，你必须拥有自己的 iSCSI 服务器，并在上面创建卷。

iSCSI 的一个特点是它可以同时被多个用户以只读方式挂载。 这意味着你可以用数据集预先填充卷，然后根据需要在尽可能多的 Pod 上使用它。 不幸的是，iSCSI 卷只能由单个使用者以读写模式挂载。不允许同时写入。

更多详情请参考 [iSCSI 示例](https://github.com/kubernetes/examples/tree/master/volumes/iscsi)。

### local

​`local` ​卷所代表的是某个被挂载的本地存储设备，例如磁盘、分区或者目录。

​`local` ​卷只能用作静态创建的持久卷。尚不支持动态配置。

与 ​`hostPath` ​卷相比，​`local` ​卷能够以持久和可移植的方式使用，而无需手动将 Pod 调度到节点。系统通过查看 PersistentVolume 的节点亲和性配置，就能了解卷的节点约束。

然而，​`local` ​卷仍然取决于底层节点的可用性，并不适合所有应用程序。 如果节点变得不健康，那么 ​`local` ​卷也将变得不可被 Pod 访问。使用它的 Pod 将不能运行。 使用 ​`local` ​卷的应用程序必须能够容忍这种可用性的降低，以及因底层磁盘的耐用性特征而带来的潜在的数据丢失风险。

下面是一个使用 ​`local` ​卷和 ​`nodeAffinity` ​的持久卷示例：

`apiVersion: v1 kind: PersistentVolume metadata:   name: example-pv spec:   capacity:     storage: 100Gi   volumeMode: Filesystem   accessModes:   - ReadWriteOnce   persistentVolumeReclaimPolicy: Delete   storageClassName: local-storage   local:     path: /mnt/disks/ssd1   nodeAffinity:     required:       nodeSelectorTerms:       - matchExpressions:         - key: kubernetes.io/hostname           operator: In           values:           - example-node`

使用 ​`local` ​卷时，你需要设置 PersistentVolume 对象的 ​`nodeAffinity` ​字段。 Kubernetes 调度器使用 PersistentVolume 的 ​`nodeAffinity` ​信息来将使用 ​`local` ​卷的 Pod 调度到正确的节点。

PersistentVolume 对象的 ​`volumeMode` ​字段可被设置为 "Block" （而不是默认值 "Filesystem"），以将 ​`local` ​卷作为原始块设备暴露出来。

使用 ​`local` ​卷时，建议创建一个 StorageClass 并将其 ​`volumeBindingMode` ​设置为 ​`WaitForFirstConsumer`​。要了解更多详细信息，请参考 local StorageClass 示例。 延迟卷绑定的操作可以确保 Kubernetes 在为 PersistentVolumeClaim 作出绑定决策时，会评估 Pod 可能具有的其他节点约束，例如：如节点资源需求、节点选择器、Pod亲和性和 Pod 反亲和性。

你可以在 Kubernetes 之外单独运行静态驱动以改进对 local 卷的生命周期管理。 请注意，此驱动尚不支持动态配置。 有关如何运行外部 ​`local` ​卷驱动，请参考 [local 卷驱动用户指南](https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner)。

> 如果不使用外部静态驱动来管理卷的生命周期，用户需要手动清理和删除 local 类型的持久卷。

### nfs

​`nfs` ​卷能将 NFS (网络文件系统) 挂载到你的 Pod 中。 不像 ​`emptyDir` ​那样会在删除 Pod 的同时也会被删除，​`nfs` ​卷的内容在删除 Pod 时会被保存，卷只是被卸载。 这意味着 ​`nfs` ​卷可以被预先填充数据，并且这些数据可以在 Pod 之间共享。

> 在使用 NFS 卷之前，你必须运行自己的 NFS 服务器并将目标 share 导出备用。

要了解更多详情请参考 [NFS 示例](https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs)。

### persistentVolumeClaim

​`persistentVolumeClaim` ​卷用来将持久卷（PersistentVolume）挂载到 Pod 中。 持久卷申领（PersistentVolumeClaim）是用户在不知道特定云环境细节的情况下“申领”持久存储（例如 GCE PersistentDisk 或者 iSCSI 卷）的一种方法。

### portworxVolume

​`portworxVolume` ​是一个可伸缩的块存储层，能够以超融合（hyperconverged）的方式与 Kubernetes 一起运行。 [Portworx](https://portworx.com/use-case/kubernetes-storage/) 支持对服务器上存储的指纹处理、基于存储能力进行分层以及跨多个服务器整合存储容量。 Portworx 可以以 in-guest 方式在虚拟机中运行，也可以在裸金属 Linux 节点上运行。

​`portworxVolume` ​类型的卷可以通过 Kubernetes 动态创建，也可以预先配备并在 Kubernetes Pod 内引用。 下面是一个引用预先配备的 PortworxVolume 的示例 Pod：

`apiVersion: v1 kind: Pod metadata:   name: test-portworx-volume-pod spec:   containers:   - image: k8s.gcr.io/test-webserver     name: test-container     volumeMounts:     - mountPath: /mnt       name: pxvol   volumes:   - name: pxvol     # 此 Portworx 卷必须已经存在     portworxVolume:       volumeID: "pxvol"       fsType: "<fs-type>"`

> 在 Pod 中使用 portworxVolume 之前，你要确保有一个名为 ​`pxvol` ​的 PortworxVolume 存在。

更多详情可以参考 [Portworx 卷](https://github.com/kubernetes/examples/blob/master/staging/volumes/portworx/README.md)。

### projected （投射）

投射卷能将若干现有的卷来源映射到同一目录上。

### quobyte (已弃用)

​`quobyte` ​卷允许将现有的 [Quobyte](https://www.quobyte.com/) 卷挂载到你的 Pod 中。

> 在使用 Quobyte 卷之前，你首先要进行安装 Quobyte 并创建好卷。

Quobyte 支持容器存储接口（CSI）。 推荐使用 CSI 插件以在 Kubernetes 中使用 Quobyte 卷。 Quobyte 的 GitHub 项目包含以 CSI 形式部署 Quobyte 的 [说明](https://github.com/quobyte/quobyte-csi target=) 及使用示例。

### rbd

​`rbd` ​卷允许将 [Rados 块设备](https://docs.ceph.com/en/latest/rbd/)卷挂载到你的 Pod 中。 不像 ​`emptyDir` ​那样会在删除 Pod 的同时也会被删除，​`rbd` ​卷的内容在删除 Pod 时会被保存，卷只是被卸载。 这意味着 ​`rbd` ​卷可以被预先填充数据，并且这些数据可以在 Pod 之间共享。

> 在使用 RBD 之前，你必须安装运行 Ceph。

RBD 的一个特性是它可以同时被多个用户以只读方式挂载。 这意味着你可以用数据集预先填充卷，然后根据需要在尽可能多的 Pod 中并行地使用卷。 不幸的是，RBD 卷只能由单个使用者以读写模式安装。不允许同时写入。

更多详情请参考 [RBD 示例](https://github.com/kubernetes/examples/tree/master/volumes/rbd)。

#### RBD CSI 迁移

FEATURE STATE: Kubernetes v1.23 \[alpha\]

启用 RBD 的 ​`CSIMigration` ​功能后，所有插件操作从现有的树内插件重定向到 ​`rbd.csi.ceph.com`​ CSI 驱动程序。 要使用该功能，必须在集群内安装 [Ceph CSI 驱动](https://github.com/ceph/ceph-csi)，并启用 ​`CSIMigration` ​和 ​`csiMigrationRBD` ​特性门控。

> 作为一位管理存储的 Kubernetes 集群操作者，在尝试迁移到 RBD CSI 驱动前，你必须完成下列先决事项：  
> 
> *   你必须在集群中安装 v3.5.0 或更高版本的 Ceph CSI 驱动（​`rbd.csi.ceph.com`​）。
> *   因为 ​`clusterID` ​是 CSI 驱动程序必需的参数，而树内存储类又将 ​`monitors` ​作为一个必需的参数，所以 Kubernetes 存储管理者需要根据 ​`monitors` ​的哈希值（例：​`#echo -n '<monitors_string>' | md5sum`​）来创建 ​`clusterID`​，并保持该 ​`monitors` ​存在于该 ​`clusterID` ​的配置中。
> *   同时，如果树内存储类的 ​`adminId` ​的值不是 ​`admin`​，那么其 ​`adminSecretName` ​就需要被修改成 ​`adminId` ​参数的 base64 编码值。

### secret

​`secret` ​卷用来给 Pod 传递敏感信息，例如密码。你可以将 Secret 存储在 Kubernetes API 服务器上，然后以文件的形式挂在到 Pod 中，无需直接与 Kubernetes 耦合。 ​`secret` ​卷由 tmpfs（基于 RAM 的文件系统）提供存储，因此它们永远不会被写入非易失性（持久化的）存储器。

> 使用前你必须在 Kubernetes API 中创建 secret。

> 容器以 subPath 卷挂载方式挂载 Secret 时，将感知不到 Secret 的更新。

### storageOS (已弃用)

​`storageos` ​卷允许将现有的 [StorageOS](https://www.storageos.com/) 卷挂载到你的 Pod 中。

StorageOS 在 Kubernetes 环境中以容器的形式运行，这使得应用能够从 Kubernetes 集群中的任何节点访问本地的或挂接的存储。为应对节点失效状况，可以复制数据。 若需提高利用率和降低成本，可以考虑瘦配置（Thin Provisioning）和数据压缩。

作为其核心能力之一，StorageOS 为容器提供了可以通过文件系统访问的块存储。

StorageOS 容器需要 64 位的 Linux，并且没有其他的依赖关系。 StorageOS 提供免费的开发者授权许可。

> 你必须在每个希望访问 StorageOS 卷的或者将向存储资源池贡献存储容量的节点上运行 StorageOS 容器。有关安装说明，请参阅 [StorageOS 文档](https://docs.ondat.io/)。

``apiVersion: v1 kind: Pod metadata:   labels:     name: redis     role: master   name: test-storageos-redis spec:   containers:     - name: master       image: kubernetes/redis:v1       env:         - name: MASTER           value: "true"       ports:         - containerPort: 6379       volumeMounts:         - mountPath: /redis-master-data           name: redis-data   volumes:     - name: redis-data       storageos:         # `redis-vol01` 卷必须在 StorageOS 中存在，并位于 `default` 名字空间内         volumeName: redis-vol01         fsType: ext4``

关于 StorageOS 的进一步信息、动态供应和持久卷申领等等，请参考 [StorageOS 示例](https://github.com/kubernetes/examples/tree/master/volumes/storageos)。

### vsphereVolume

> 你必须配置 Kubernetes 的 vSphere 云驱动。云驱动的配置方法请参考 [vSphere 使用指南](https://github.com/vmware/)。

​`vsphereVolume` ​用来将 vSphere VMDK 卷挂载到你的 Pod 中。 在卸载卷时，卷的内容会被保留。 vSphereVolume 卷类型支持 VMFS 和 VSAN 数据仓库。

> 在挂载到 Pod 之前，你必须用下列方式之一创建 VMDK。

#### 创建 VMDK 卷 

选择下列方式之一创建 VMDK。

*   使用 vmkfstools 创建

首先 ssh 到 ESX，然后使用下面的命令来创建 VMDK：

`vmkfstools -c 2G /vmfs/volumes/DatastoreName/volumes/myDisk.vmdk`

*   使用 vmware-vdiskmanager 创建

使用下面的命令创建 VMDK：

`vmware-vdiskmanager -c -t 0 -s 40GB -a lsilogic myDisk.vmdk`

#### vSphere VMDK 配置示例 

`apiVersion: v1 kind: Pod metadata:   name: test-vmdk spec:   containers:   - image: k8s.gcr.io/test-webserver     name: test-container     volumeMounts:     - mountPath: /test-vmdk       name: test-volume   volumes:   - name: test-volume     # 此 VMDK 卷必须已经存在     vsphereVolume:       volumePath: "[DatastoreName] volumes/myDisk"       fsType: ext4`

进一步信息可参考 [vSphere 卷](https://github.com/kubernetes/examples/tree/master/staging/volumes/vsphere)。

#### vSphere CSI 迁移 

FEATURE STATE: Kubernetes v1.19 \[beta\]

当 ​`vsphereVolume` ​的 ​`CSIMigration` ​特性被启用时，所有插件操作都被从树内插件重定向到 ​`csi.vsphere.vmware.com`​ CSI 驱动。 为了使用此功能特性，必须在集群中安装 [vSphere CSI 驱动](https://github.com/kubernetes-sigs/vsphere-csi-driver)，并启用 ​`CSIMigration` ​和 ​`CSIMigrationvSphere` ​特性门控。

此特性还要求 vSphere vCenter/ESXi 的版本至少为 7.0u1，且 HW 版本至少为 VM version 15。

> vSphere CSI 驱动不支持内置 ​`vsphereVolume` ​的以下 StorageClass 参数：  
> 
> *   ​`diskformat`​
> *   ​`hostfailurestotolerate`​
> *   ​`forceprovisioning`​
> *   ​`cachereservation`​
> *   ​`diskstripes`​
> *   ​`objectspacereservation`​
> *   ​`iopslimit`​
> 
> 使用这些参数创建的现有卷将被迁移到 vSphere CSI 驱动，不过使用 vSphere CSI 驱动所创建的新卷都不会理会这些参数。

#### vSphere CSI 迁移完成 

FEATURE STATE: Kubernetes v1.19 \[beta\]

为了避免控制器管理器和 kubelet 加载 ​`vsphereVolume` ​插件，你需要将 ​`InTreePluginvSphereUnregister` ​特性设置为 ​`true`​。你还必须在所有工作节点上安装 ​`csi.vsphere.vmware.com`​ CSI 驱动。

#### Portworx CSI 迁移

FEATURE STATE: Kubernetes v1.23 \[alpha\]

Kubernetes 1.23 中加入了 Portworx 的 ​`CSIMigration` ​功能，但默认不会启用，因为该功能仍处于 alpha 阶段。 该功能会将所有的插件操作从现有的树内插件重定向到 ​`pxd.portworx.com`​ 容器存储接口（Container Storage Interface, CSI）驱动程序。 集群中必须安装  [Portworx CSI 驱动](https://docs.portworx.com/portworx-install-with-kubernetes/storage-operations/csi/)。 要启用此功能，请在 kube-controller-manager 和 kubelet 中设置 ​`CSIMigrationPortworx=true`​。

使用 subPath 
-----------

有时，在单个 Pod 中共享卷以供多方使用是很有用的。 ​`volumeMounts.subPath`​ 属性可用于指定所引用的卷内的子路径，而不是其根路径。

下面例子展示了如何配置某包含 LAMP 堆栈（Linux Apache MySQL PHP）的 Pod 使用同一共享卷。 此示例中的 ​`subPath` ​配置不建议在生产环境中使用。 PHP 应用的代码和相关数据映射到卷的 ​`html` ​文件夹，MySQL 数据库存储在卷的 ​`mysql` ​文件夹中：

`apiVersion: v1 kind: Pod metadata:   name: my-lamp-site spec:     containers:     - name: mysql       image: mysql       env:       - name: MYSQL_ROOT_PASSWORD         value: "rootpasswd"       volumeMounts:       - mountPath: /var/lib/mysql         name: site-data         subPath: mysql     - name: php       image: php:7.0-apache       volumeMounts:       - mountPath: /var/www/html         name: site-data         subPath: html     volumes:     - name: site-data       persistentVolumeClaim:         claimName: my-lamp-site-data`
        

### 使用带有扩展环境变量的 subPath 

FEATURE STATE: Kubernetes v1.17 \[stable\]

使用 ​`subPathExpr` ​字段可以基于 Downward API 环境变量来构造 ​`subPath` ​目录名。 ​`subPath` ​和 ​`subPathExpr` ​属性是互斥的。

在这个示例中，Pod 使用 ​`subPathExpr` ​来 hostPath 卷 ​`/var/log/pods`​ 中创建目录 ​`pod1`​。 ​`hostPath` ​卷采用来自 ​`downwardAPI` ​的 Pod 名称生成目录名。 宿主目录 ​`/var/log/pods/pod1`​ 被挂载到容器的 ​`/logs`​ 中。

`apiVersion: v1 kind: Pod metadata:   name: pod1 spec:   containers:   - name: container1     env:     - name: POD_NAME       valueFrom:         fieldRef:           apiVersion: v1           fieldPath: metadata.name     image: busybox:1.28     command: [ "sh", "-c", "while [ true ]; do echo 'Hello'; sleep 10; done | tee -a /logs/hello.txt" ]     volumeMounts:     - name: workdir1       mountPath: /logs       # 包裹变量名的是小括号，而不是大括号       subPathExpr: $(POD_NAME)   restartPolicy: Never   volumes:   - name: workdir1     hostPath:       path: /var/log/pods`
            

资源 
---

​`emptyDir` ​卷的存储介质（磁盘、SSD 等）是由保存 kubelet 数据的根目录（通常是 ​`/var/lib/kubelet`​）的文件系统的介质确定。 Kubernetes 对 ​`emptyDir` ​卷或者 ​`hostPath` ​卷可以消耗的空间没有限制，容器之间或 Pod 之间也没有隔离。

树外（Out-of-Tree）卷插件 
-------------------

Out-of-Tree 卷插件包括 容器存储接口（CSI） 和 FlexVolume（已弃用）。 它们使存储供应商能够创建自定义存储插件，而无需将插件源码添加到 Kubernetes 代码仓库。

以前，所有卷插件（如上面列出的卷类型）都是“树内（In-Tree）”的。 “树内”插件是与 Kubernetes 的核心组件一同构建、链接、编译和交付的。 这意味着向 Kubernetes 添加新的存储系统（卷插件）需要将代码合并到 Kubernetes 核心代码库中。

CSI 和 FlexVolume 都允许独立于 Kubernetes 代码库开发卷插件，并作为扩展部署（安装）在 Kubernetes 集群上。

对于希望创建树外（Out-Of-Tree）卷插件的存储供应商，请参考 [卷插件常见问题](https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md)。

### CSI

[容器存储接口](https://github.com/container-storage-interface/spec/blob/master/spec.md) (CSI) 为容器编排系统（如 Kubernetes）定义标准接口，以将任意存储系统暴露给它们的容器工作负载。

更多详情请阅读 [CSI 设计方案](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md)。

> Kubernetes v1.13 废弃了对 CSI 规范版本 0.2 和 0.3 的支持，并将在以后的版本中删除。

> CSI 驱动可能并非兼容所有的 Kubernetes 版本。 请查看特定 CSI 驱动的文档，以了解各个 Kubernetes 版本所支持的部署步骤以及兼容性列表。

一旦在 Kubernetes 集群上部署了 CSI 兼容卷驱动程序，用户就可以使用 ​`csi` ​卷类型来挂接、挂载 CSI 驱动所提供的卷。

​`csi` ​卷可以在 Pod 中以三种方式使用：

*   通过 PersistentVolumeClaim(#persistentvolumeclaim) 对象引用
*   使用一般性的临时卷 （Alpha 特性）
*   使用 CSI 临时卷， 前提是驱动支持这种用法（Beta 特性）

存储管理员可以使用以下字段来配置 CSI 持久卷：

*   ​`driver`​：指定要使用的卷驱动名称的字符串值。 这个值必须与 CSI 驱动程序在 ​`GetPluginInfoResponse` ​中返回的值相对应；该接口定义在 [CSI 规范](https://github.com/container-storage-interface/spec/blob/master/spec.md target=)中。 Kubernetes 使用所给的值来标识要调用的 CSI 驱动程序；CSI 驱动程序也使用该值来辨识哪些 PV 对象属于该 CSI 驱动程序。
*   ​`volumeHandle`​：唯一标识卷的字符串值。 该值必须与 CSI 驱动在 ​`CreateVolumeResponse` ​的 ​`volume_id` ​字段中返回的值相对应；接口定义在 [CSI 规范](https://github.com/container-storage-interface/spec/blob/master/spec.md target=) 中。 在所有对 CSI 卷驱动程序的调用中，引用该 CSI 卷时都使用此值作为 ​`volume_id` ​参数。
*   ​`readOnly`​：一个可选的布尔值，指示通过 ​`ControllerPublished` ​关联该卷时是否设置该卷为只读。默认值是 false。 该值通过 ​`ControllerPublishVolumeRequest` ​中的 ​`readonly` ​字段传递给 CSI 驱动。
*   ​`fsType`​：如果 PV 的 ​`VolumeMode` ​为 ​`Filesystem`​，那么此字段指定挂载卷时应该使用的文件系统。 如果卷尚未格式化，并且支持格式化，此值将用于格式化卷。 此值可以通过 ​`ControllerPublishVolumeRequest`​、​`NodeStageVolumeRequest` ​和 ​`NodePublishVolumeRequest` ​的 ​`VolumeCapability` ​字段传递给 CSI 驱动。
*   ​`volumeAttributes`​：一个字符串到字符串的映射表，用来设置卷的静态属性。 该映射必须与 CSI 驱动程序返回的 ​`CreateVolumeResponse` ​中的 ​`volume.attributes`​ 字段的映射相对应； [CSI 规范](https://github.com/container-storage-interface/spec/blob/master/spec.md target=) 中有相应的定义。 该映射通过​`ControllerPublishVolumeRequest`​、​`NodeStageVolumeRequest`​、和 ​`NodePublishVolumeRequest` ​中的 ​`volume_attributes` ​字段传递给 CSI 驱动。
*   ​`controllerPublishSecretRef`​：对包含敏感信息的 Secret 对象的引用； 该敏感信息会被传递给 CSI 驱动来完成 CSI ​`ControllerPublishVolume` ​和 ​`ControllerUnpublishVolume` ​调用。 此字段是可选的；在不需要 Secret 时可以是空的。 如果 Secret 对象包含多个 Secret 条目，则所有的 Secret 条目都会被传递。
*   ​`nodeStageSecretRef`​：对包含敏感信息的 Secret 对象的引用。 该信息会传递给 CSI 驱动来完成 CSI ​`NodeStageVolume` ​调用。 此字段是可选的，如果不需要 Secret，则可能是空的。 如果 Secret 对象包含多个 Secret 条目，则传递所有 Secret 条目。
*   ​`nodePublishSecretRef`​：对包含敏感信息的 Secret 对象的引用。 该信息传递给 CSI 驱动来完成 CSI ​`NodePublishVolume` ​调用。 此字段是可选的，如果不需要 Secret，则可能是空的。 如果 Secret 对象包含多个 Secret 条目，则传递所有 Secret 条目。

#### CSI 原始块卷支持 

FEATURE STATE: Kubernetes v1.18 \[stable\]

具有外部 CSI 驱动程序的供应商能够在 Kubernetes 工作负载中实现原始块卷支持。

你可以和以前一样，安装自己的 带有原始块卷支持的 PV/PVC， 采用 CSI 对此过程没有影响。

#### CSI 临时卷 

FEATURE STATE: Kubernetes v1.16 \[beta\]

你可以直接在 Pod 规约中配置 CSI 卷。采用这种方式配置的卷都是临时卷， 无法在 Pod 重新启动后继续存在。 进一步的信息可参阅临时卷。

有关如何开发 CSI 驱动的更多信息，请参考 [kubernetes-csi 文档](https://kubernetes-csi.github.io/docs/)。

#### 从树内插件迁移到 CSI 驱动程序 

FEATURE STATE: Kubernetes v1.17 \[beta\]

启用 ​`CSIMigration` ​功能后，针对现有树内插件的操作会被重定向到相应的 CSI 插件（应已安装和配置）。 因此，操作员在过渡到取代树内插件的 CSI 驱动时，无需对现有存储类、PV 或 PVC（指树内插件）进行任何配置更改。

所支持的操作和功能包括：配备（Provisioning）/删除、挂接（Attach）/解挂（Detach）、 挂载（Mount）/卸载（Unmount）和调整卷大小。

上面的卷类型节列出了支持 ​`CSIMigration` ​并已实现相应 CSI 驱动程序的树内插件。

### flexVolume

FEATURE STATE: Kubernetes v1.23 \[deprecated\]

FlexVolume 是一个使用基于 exec 的模型来与驱动程序对接的树外插件接口。 用户必须在每个节点上的预定义卷插件路径中安装 FlexVolume 驱动程序可执行文件，在某些情况下，控制平面节点中也要安装。

Pod 通过 ​`flexvolume` ​树内插件与 FlexVolume 驱动程序交互。 更多详情请参考 FlexVolume [README](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md target=) 文档。

> FlexVolume 已弃用。推荐使用树外 CSI 驱动来将外部存储整合进 Kubernetes。  
> FlexVolume 驱动的维护者应开发一个 CSI 驱动并帮助用户从 FlexVolume 驱动迁移到 CSI。 FlexVolume 用户应迁移工作负载以使用对等的 CSI 驱动。

挂载卷的传播 
-------

挂载卷的传播能力允许将容器安装的卷共享到同一 Pod 中的其他容器，甚至共享到同一节点上的其他 Pod。

卷的挂载传播特性由 ​`Container.volumeMounts`​ 中的 ​`mountPropagation` ​字段控制。 它的值包括：

*   ​`None` ​- 此卷挂载将不会感知到主机后续在此卷或其任何子目录上执行的挂载变化。 类似的，容器所创建的卷挂载在主机上是不可见的。这是默认模式。
该模式等同于 [Linux 内核文档](https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt) 中描述的 ​`private` ​挂载传播选项。*   ​`HostToContainer` ​- 此卷挂载将会感知到主机后续针对此卷或其任何子目录的挂载操作。
换句话说，如果主机在此挂载卷中挂载任何内容，容器将能看到它被挂载在那里。 类似的，配置了 ​`Bidirectional` ​挂载传播选项的 Pod 如果在同一卷上挂载了内容，挂载传播设置为 ​`HostToContainer` ​的容器都将能看到这一变化。 该模式等同于 [Linux 内核文档](https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt) 中描述的 rslave 挂载传播选项。*   ​`Bidirectional` ​- 这种卷挂载和 ​`HostToContainer` ​挂载表现相同。 另外，容器创建的卷挂载将被传播回至主机和使用同一卷的所有 Pod 的所有容器。
该模式等同于 [Linux 内核文档](https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt) 中描述的 ​`rshared` ​挂载传播选项。

> ​`Bidirectional` ​形式的挂载传播可能比较危险。 它可以破坏主机操作系统，因此它只被允许在特权容器中使用。 强烈建议你熟悉 Linux 内核行为。 此外，由 Pod 中的容器创建的任何卷挂载必须在终止时由容器销毁（卸载）。

### 配置 

在某些部署环境中，挂载传播正常工作前，必须在 Docker 中正确配置挂载共享（mount share），如下所示。

编辑你的 Docker ​`systemd` ​服务文件，按下面的方法设置 ​`MountFlags`​：

`MountFlags=shared`

或者，如果存在 ​`MountFlags=slave`​ 就删除掉。然后重启 Docker 守护进程：

`sudo systemctl daemon-reload sudo systemctl restart docker`

##  2.  Kubernetes 持久卷

##  3.  Kubernetes 投射卷

##  4.  Kubernetes 临时卷

##  5.  Kubernetes 存储类

#  11.  Kubernetes 配置

##  1.  Kubernetes 配置最佳实践

##  2.  Kubernetes ConfigMap

##  3.  Kubernetes Secret

##  4.  Kubernetes 为 Pod 和容器管理资源

##  5.  Kubernetes 使用 kubeconfig 文件组织集群访问

##  6.  Kubernetes Windows 节点的资源管理

#  12.  Kubernetes 安全

##  1.  Kubernetes 云原生安全概述

##  2.  Kubernetes Pod安全性标准

##  3.  Kubernetes Pod安全性准入

##  4.  Kubernetes Pod安全策略

##  5.  Kubernetes Windows节点的安全性

##  6.  Kubernetes API访问控制

##  7.  Kubernetes 基于角色的访问控制良好实践

#  13.  Kubernetes 策略

##  1.  Kubernetes 限制范围

##  2.  Kubernetes 资源配额

##  3.  Kubernetes 进程ID约束与预留

##  4.  Kubernetes 节点资源管理器

#  14.  Kubernetes 调度，抢占和驱逐

##  1.  Kubernetes 调度器

##  2.  Kubernetes 将Pod指派给节点

##  3.  Kubernetes Pod开销

##  4.  Kubernetes 污点和容忍度

##  5.  Kubernetes Pod优先级和抢占

##  6.  Kubernetes 节点压力驱逐

##  7.  Kubernetes API发起的驱逐

##  8.  Kubernetes 扩展资源的资源装箱

##  9.  Kubernetes 调度框架

##  10.  Kubernetes 调度器性能调优

#  15.  Kubernetes 集群管理

##  1.  Kubernetes 管理资源

##  2.  Kubernetes 集群网络系统

##  3.  Kubernetes 系统组件指标

##  4.  Kubernetes 日志架构

##  5.  Kubernetes 系统日志

##  6.  Kubernetes 追踪系统组件

##  7.  Kubernetes 代理

##  8.  Kubernetes API优先级和公平性

##  9.  Kubernetes 安装扩展（Addons）

#  16.  Kubernetes 扩展

##  1.  Kubernetes 扩展API

###  1.  Kubernetes 定制资源

###  2.  Kubernetes 通过聚合层扩展API

##  2.  Kubernetes Operator模式

##  3.  Kubernetes 计算、存储和网络扩展

###  1.  Kubernetes 网络插件

###  2.  Kubernetes 设备插件

##  4.  Kubernetes 服务目录

#  17.  Kubernetes 应用故障排除

##  1.  Kubernetes 调试Pod

##  2.  Kubernetes 调试Service

##  3.  Kubernetes 调试StatefulSet

##  4.  Kubernetes 调试Init容器

##  5.  Kubernetes 确定Pod失败的原因

##  6.  Kubernetes 获取正在运行容器的Shell

##  7.  Kubernetes 调试运行中的Pod

#  18.  Kubernetes 集群故障排查

##  1.  Kubernetes 资源指标管道

##  2.  Kubernetes 节点健康监测

##  3.  Kubernetes 使用crictl对Kubernetes节点进行调试

##  4.  Kubernetes Windows调试提示

##  5.  Kubernetes 使用telepresence在本地开发和调试服务

##  6.  Kubernetes 审计

##  7.  Kubernetes 资源监控工具

#  19.  Kubernetes 管理集群

##  1.  Kubernetes 从dockershim迁移

###  1.  Kubernetes 将节点上的容器运行时从Docker Engine改为containerd

###  2.  Kubernetes 将Docker Engine节点从dockershim迁移到cri-dockerd

###  3.  Kubernetes CNI插件相关错误故障排除

###  4.  Kubernetes 查明节点上所使用的容器运行时

###  5.  Kubernetes 检查弃用Dockershim是否对你有影响

###  6.  Kubernetes 从dockershim迁移遥测和安全代理

##  2.  Kubernetes 用kubeadm进行管理

###  1.  Kubernetes 使用kubeadm进行证书管理

###  2.  Kubernetes 配置cgroup驱动

###  3.  Kubernetes 重新配置kubeadm集群

###  4.  Kubernetes 升级kubeadm集群

###  5.  Kubernetes 添加Windows节点

###  6.  Kubernetes 升级Windows节点

##  3.  Kubernetes 手动生成证书

##  4.  Kubernetes 管理内存，CPU和API资源

###  1.  Kubernetes 为命名空间配置默认的内存请求和限制

###  2.  Kubernetes 为命名空间配置默认的CPU请求和限制

###  3.  Kubernetes 配置命名空间的最小和最大内存约束

###  4.  Kubernetes 为命名空间配置CPU最小和最大约束

###  5.  Kubernetes 为命名空间配置内存和CPU配额

###  6.  Kubernetes 配置命名空间下Pod配额

##  5.  Kubernetes 安装网络策略驱动

###  1.  Kubernetes 使用Antrea提供NetworkPolicy

###  2.  Kubernetes 使用Calico提供NetworkPolicy

###  3.  Kubernetes 使用Cilium提供NetworkPolicy

###  4.  Kubernetes 使用kube-router提供NetworkPolicy

###  5.  Kubernetes 使用Romana提供NetworkPolicy

###  6.  Kubernetes 使用Weave Net提供NetworkPolicy

##  6.  Kubernetes IP Masquerade Agent用户指南

##  7.  Kubernetes 云管理控制器

##  8.  Kubernetes 验证签名的容器镜像

##  9.  Kubernetes 运行 etcd 集群

##  10.  Kubernetes 为系统守护进程预留计算资源

##  11.  Kubernetes 为节点发布扩展资源

##  12.  Kubernetes 以非root用户身份运行Kubernetes节点组件

##  13.  Kubernetes 使用CoreDNS进行服务发现

##  14.  Kubernetes 使用KMS驱动进行数据加密

##  15.  Kubernetes 使用Kubernetes API访问集群

##  16.  Kubernetes 使用NUMA感知的内存管理器

##  17.  Kubernetes 保护集群

##  18.  Kubernetes 关键插件Pod的调度保证

##  19.  Kubernetes 升级集群

##  20.  Kubernetes 名字空间演练

##  21.  Kubernetes 启用/禁用Kubernetes API

##  22.  Kubernetes 在Kubernetes集群中使用NodeLocal DNSCache

##  23.  Kubernetes 在Kubernetes集群中使用sysctl

##  24.  Kubernetes 在运行中的集群上重新配置节点的kubelet

##  25.  Kubernetes 在集群中使用级联删除
